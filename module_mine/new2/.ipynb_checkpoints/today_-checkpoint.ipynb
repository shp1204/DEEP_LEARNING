{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acsr   (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (1, 2)\t3\n",
      "  (2, 0)\t4\n",
      "  (2, 2)\t5\n",
      "Acoo   (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (1, 2)\t3\n",
      "  (2, 0)\t4\n",
      "  (2, 2)\t5\n",
      "[0, 0, 1, 2, 2]\n",
      "[0, 1, 2, 0, 2]\n",
      "[[0, 0, 1, 2, 2], [0, 1, 2, 0, 2]]\n",
      "[1 2 3 4 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 5 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "\n",
    "__author__ = 'Andrea Esuli'\n",
    "\n",
    "Acsr = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
    "print('Acsr',Acsr)\n",
    "\n",
    "Acoo = Acsr.tocoo()\n",
    "print('Acoo',Acoo)\n",
    "\n",
    "Apt = torch.sparse.LongTensor(torch.LongTensor([Acoo.row.tolist(), Acoo.col.tolist()]),\n",
    "                              torch.LongTensor(Acoo.data.astype(np.int32)))\n",
    "\n",
    "print(Acoo.row.tolist())\n",
    "print(Acoo.col.tolist())\n",
    "print([Acoo.row.tolist(), Acoo.col.tolist()])\n",
    "print(Acoo.data)\n",
    "\n",
    "Apt\n",
    "Acoo.data.astype\n",
    "Acsr.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    def __init__(self, AF, labels, weight_channel):\n",
    "        super(mymodule, self).__init__()\n",
    "        self.AF = AF\n",
    "        self.labels = labels\n",
    "\n",
    "        # weight 크기 설정에 따라 node_feature 가 업데이트 될 수 있게\n",
    "        self.weight_channel = weight_channel\n",
    "\n",
    "        # csr_matrix to tensor\n",
    "        self.temp1 = self.AF.tocoo()\n",
    "        self.temp2 = torch.sparse.Tensor([self.temp1.row.tolist(),\n",
    "                                        self.temp1.col.tolist()])\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(np.random.rand(self.AF.size,\n",
    "                                                            self.weight_channel)))\n",
    "        self.bias = Parameter(torch.Tensor(np.random.rand(self.weight_channel,\n",
    "                                                          1)))\n",
    "\n",
    "        self.bias = self.bias.unsqueeze(1)\n",
    "\n",
    "        # AF X Weight + bias\n",
    "        self.cal = torch.matmul(self.temp2, self.weight)\n",
    "        self.cal2 = torch.add(self.cal, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 네트워크 초기화\n",
    "# custom weights initialization called on netG and netD\n",
    "b\n",
    "def weights_init(m) :\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 :\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1 :\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # kaiming_normalizaion : 어떤 \n",
    "                nn.init.kaiming_normal_(m.weight, \n",
    "                                        mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                    \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "#노드 연결 정보\n",
    "edges = np.array([[0 ,1],[2 ,3],[1 ,4],[3 ,4],[4 ,5],[4 ,6]])\n",
    "# 각 노드 특성 정보(H) = 7 X 4\n",
    "features = sp.csr_matrix([[1, 0, 0, 0],[0, 1, 0, 0],[1, 0, 0, 0],[0, 1, 0, 0],[0, 0, 1, 0],[0, 0, 0, 1],[0, 0, 0, 1]])\n",
    "# edge 특성 정보\n",
    "edge_features = [[3],[5],[1],[10],[6],[8]]\n",
    "# labels \n",
    "labels = np.array([1,4,5,2,6,3,0])\n",
    "\n",
    "# 단위 행렬 더해주기\n",
    "# direction matrix 생성\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])),\n",
    "             shape = (labels.shape[0], labels.shape[0]),\n",
    "             dtype = np.float32)\n",
    "adj = adj + sp.eye(adj.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "def normalize(mx) :\n",
    "    rowsum = np.array(mx.sum(1)) # 각 노드 정보 개수\n",
    "    print('=====row별 feature 특성 합=====')\n",
    "    #print(rowsum)\n",
    "\n",
    "    # r_inv\n",
    "    # 역행렬로 np.power 수행\n",
    "    r_inv = np.power(rowsum, 0).flatten() # 0, 1, # power : 0, 1, 8, 27, ,,, / 0, 1, 4, 9, ,,, / 0, 1, 0.5, 0.333, 0.25\n",
    "    print('===== 역행렬로 np.power 수행 =====')\n",
    "    r_inv[np.isinf(r_inv)] = 0 \n",
    "    #print(r_inv)\n",
    "    \n",
    "    # r_mat_inv\n",
    "    r_mat_inv = sp.diags(r_inv) # 행렬로 만들어줌\n",
    "    #print(r_mat_inv.toarray())\n",
    "    \n",
    "    # 노드 adj 와 노드 feature 정보 행렬연산\n",
    "    print('=====adj, feature 행렬곱=====') \n",
    "    mx = r_mat_inv.dot(mx) \n",
    "\n",
    "    print(mx.toarray())\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====row별 feature 특성 합=====\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "=====row별 feature 특성 합=====\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "======1번째======\n",
      "[[1. 1. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 1. 2.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "======2번째======\n",
      "[[1. 2. 1. 0.]\n",
      " [0. 1. 2. 2.]\n",
      " [1. 2. 1. 0.]\n",
      " [0. 1. 2. 2.]\n",
      " [0. 0. 1. 4.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "features = normalize(features)\n",
    "adj = normalize(adj) # 대각행렬\n",
    "\n",
    "# 행렬곱을 위한 array \n",
    "adj_arr = adj.toarray()\n",
    "features_arr = features.toarray()\n",
    "\n",
    "# hop 에 따라 정보 전달\n",
    "hop = 2\n",
    "for idx in range(hop) :\n",
    "    features_arr = np.matmul(adj_arr, features_arr) # 여기에 weight 곱하기, bias 더하기\n",
    "    print('======{0}번째======'.format(idx+1))\n",
    "    print('{}'.format(features_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 5, 2, 6, 3, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
       "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
       "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "       size=(7, 7), nnz=13, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train = range(5)\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(labels) # 원핫인코딩 된 label 중 해당하는 label이 몇 번 째인지\n",
    "print(labels)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "\n",
    "    # 노드\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)) # vstack : 행 추가\n",
    "    # 노드 간 edge의 정보\n",
    "    values = torch.from_numpy(sparse_mx.data) # numpy.ndarray를 tensor로 올려줌\n",
    "    # 노드 개수, 특성 개수\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) # sparse : 크기에 맞게 값을 뿌려주는 것 같은데 규칙 잘 모르겠다\n",
    "\n",
    "t = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 확인\n",
    "# print('===단위행렬 더하기 전===')\n",
    "# print('{}'.format(adj))\n",
    "# print(adj.toarray())\n",
    "# print()\n",
    "\n",
    "## 고려할 것 : 단위행렬(자기자신의 정보) 전달\n",
    "#adj = adj + sp.eye(adj.shape[0])\n",
    "#print('===단위행렬 더한 후===')\n",
    "#print('{}'.format(adj))\n",
    "#print(adj.toarray())\n",
    "\n",
    "# num_nodes = max + 1 = 7\n",
    "# num_edges = len(edges) = 6\n",
    "\n",
    "# edge정보를 어떻게 맞춰줄것인가_dimension\n",
    "\n",
    "# feature 정보 학습, label regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Input 값 확인 =============\n",
      "AF.size : 13\n",
      "weight_channel : 2\n",
      "==================csr_matrix to tensor==============================\n",
      "temp1.row : 13\n",
      "temp1.col : 13\n",
      "temp2.size : torch.Size([2, 13])\n",
      "======================weight, bias 설정==============================\n",
      "Parameter containing:\n",
      "tensor([[0.1906],\n",
      "        [0.1774]], requires_grad=True)\n",
      "tensor([[[0.1906]],\n",
      "\n",
      "        [[0.1774]]], grad_fn=<UnsqueezeBackward0>)\n",
      "========================= calculate ======================\n",
      "tensor([[13.9117, 22.6414],\n",
      "        [18.1916, 29.0733]], grad_fn=<MmBackward>)\n",
      "tensor([[[14.1024, 22.8321],\n",
      "         [18.3823, 29.2639]],\n",
      "\n",
      "        [[14.0891, 22.8188],\n",
      "         [18.3690, 29.2507]]], grad_fn=<AddBackward0>)\n",
      "========================= process =========================\n",
      "  (0, 1)\t1.0\n",
      "  (0, 0)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "  (4, 5)\t1.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 6)\t1.0 x Parameter containing:\n",
      "tensor([[0.0156, 0.5864],\n",
      "        [0.7959, 0.7861],\n",
      "        [0.7657, 0.9142],\n",
      "        [0.7186, 0.2027],\n",
      "        [0.7854, 0.8161],\n",
      "        [0.4622, 0.8813],\n",
      "        [0.4557, 0.0415],\n",
      "        [0.9633, 0.7437],\n",
      "        [0.0443, 0.6864],\n",
      "        [0.6374, 0.8722],\n",
      "        [0.2232, 0.8942],\n",
      "        [0.0545, 0.8776],\n",
      "        [0.2971, 0.2624]], requires_grad=True) = tensor([[13.9117, 22.6414],\n",
      "        [18.1916, 29.0733]], grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<today_third.mymodule at 0x24faf677880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from today_third import mymodule\n",
    "mymodule(adj, labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어디로가나?\n",
      "end\n",
      "1end\n",
      "2end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mymodel()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from today_fourth import *\n",
    "mymodel(adj, labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===단위행렬 더하기 전===\n",
      "  (0, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "  (4, 5)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "======1번째======\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "======2번째======\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "tensor([1, 4, 5, 2, 6, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "#노드 연결 정보\n",
    "edges = np.array([[0 ,1], [2 ,3], [1 ,4], [3 ,4],[4 ,5],[4 ,6]])\n",
    "\n",
    "# 각 노드 특성 정보(H) = 7 X 4\n",
    "features = sp.csr_matrix([[1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [0, 0, 1, 0],\n",
    "                          [0, 0, 0, 1],\n",
    "                          [0, 0, 0, 1]])\n",
    "\n",
    "# edge 특성 정보\n",
    "edge_features = [[3],[5], [1],[10], [6],[8]]\n",
    "\n",
    "# labels \n",
    "labels = np.array([1, 4, 5, 2, 6,3,0])\n",
    "\n",
    "# 단위 행렬 더해주기\n",
    "# direction matrix 생성\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])),\n",
    "             shape = (labels.shape[0], labels.shape[0]),\n",
    "             dtype = np.float32)\n",
    "## 확인\n",
    "print('===단위행렬 더하기 전===')\n",
    "print('{}'.format(adj))\n",
    "print(adj.toarray())\n",
    "print()\n",
    "\n",
    "## 고려할 것 : 단위행렬(자기자신의 정보) 전달\n",
    "#adj = adj + sp.eye(adj.shape[0])\n",
    "#print('===단위행렬 더한 후===')\n",
    "#print('{}'.format(adj))\n",
    "#print(adj.toarray())\n",
    "def normalize(mx) :\n",
    "    rowsum = np.array(mx.sum(1)) # 각 노드 정보 개수\n",
    "    #print('=====row별 feature 특성 합=====')\n",
    "    #print(rowsum)\n",
    "    \n",
    "    # r_inv\n",
    "    # 역행렬로 np.power 수행\n",
    "    r_inv = np.power(rowsum, 0).flatten() # 0, 1, # power : 0, 1, 8, 27, ,,, / 0, 1, 4, 9, ,,, / 0, 1, 0.5, 0.333, 0.25\n",
    "    #print('===== 역행렬로 np.power 수행 =====')\n",
    "    r_inv[np.isinf(r_inv)] = 0 \n",
    "    #print(r_inv)\n",
    "\n",
    "    # r_mat_inv\n",
    "    r_mat_inv = sp.diags(r_inv) # 행렬로 만들어줌\n",
    "    #print(r_mat_inv.toarray())\n",
    "    \n",
    "    # 노드 adj 와 노드 feature 정보 행렬연산\n",
    "    print('=====adj, feature 행렬곱=====')\n",
    "    mx = r_mat_inv.dot(mx) \n",
    "    print(mx.toarray())\n",
    "    return mx\n",
    "\n",
    "# num_nodes = max + 1 = 7\n",
    "# num_edges = len(edges) = 6\n",
    "\n",
    "num_nodes = 7\n",
    "num_edges = len(edges)\n",
    "\n",
    "# direction matrix 생성\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labels.shape[0], labels.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "\n",
    "## 고려할 것 : 단위행렬(자기자신의 정보) 전달\n",
    "# adj = adj + sp.eye(adj.shape[0])\n",
    "# print('===단위행렬 더한 후===')\n",
    "# print(adj.toarray())\n",
    "\n",
    "# edge정보를 어떻게 맞춰줄것인가_dimension\n",
    "\n",
    "# 행렬곱을 위한 array \n",
    "adj_arr = adj.toarray()\n",
    "features_arr = features.toarray()\n",
    "\n",
    "# hop 에 따라 정보 전달\n",
    "hop = 2\n",
    "for idx in range(hop) :\n",
    "    features_arr = np.matmul(adj_arr, features_arr) # 여기에 weight 곱하기, bias 더하기\n",
    "    print('======{0}번째======'.format(idx+1))\n",
    "    print('{}'.format(features_arr))\n",
    "    \n",
    "    \n",
    "# feature 정보 학습, label regression\n",
    "\n",
    "\n",
    "# features : 현재 노드 + 연결된 노드 정보\n",
    "features = normalize(features)\n",
    "\n",
    "adj = normalize(adj + sp.eye(adj.shape[0])) # 대각행렬\n",
    "\n",
    "idx_train = range(5)\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(labels) # 원핫인코딩 된 label 중 해당하는 label이 몇 번 째인지\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
       "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
       "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "       size=(7, 7), nnz=13, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "\n",
    "    # 노드\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)) # vstack : 행 추가\n",
    "\n",
    "    # 노드 간 edge의 정보\n",
    "    values = torch.from_numpy(sparse_mx.data) # numpy.ndarray를 tensor로 올려줌\n",
    "\n",
    "    # 노드 개수, 특성 개수\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) # sparse : 크기에 맞게 값을 뿌려주는 것 같은데 규칙 잘 모르겠다.\n",
    "\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
