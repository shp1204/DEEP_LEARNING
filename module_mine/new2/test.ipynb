{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# graphsage 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import graphsage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGEConv(10, 4)\n"
     ]
    }
   ],
   "source": [
    "t = graphsage.SAGEConv(10, 4)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# make_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 더하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_game\n",
      "do_sum : 1 + 2 = 3\n",
      "temp_sum : 1 + 2 = 3\n",
      "check : True\n",
      "end_game\n"
     ]
    }
   ],
   "source": [
    "from make_test import my_class\n",
    "p = my_class(1, 2)\n",
    "p.check_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------1번째-----------\n",
      "start_game\n",
      "do_sum : 1 + 2 = 3\n",
      "temp_sum : 1 + 2 = 3\n",
      "check : True\n",
      "end_game\n",
      "---------------2번째-----------\n",
      "start_game\n",
      "do_sum : 3 + -1 = 2\n",
      "temp_sum : 3 + -1 = 2\n",
      "check : True\n",
      "end_game\n",
      "---------------3번째-----------\n",
      "start_game\n",
      "do_sum : 100 + 20 = 120\n",
      "temp_sum : 100 + 20 = 120\n",
      "check : True\n",
      "end_game\n",
      "---------------4번째-----------\n",
      "start_game\n",
      "do_sum : 10 + -70 = -60\n",
      "temp_sum : 10 + -70 = -60\n",
      "check : True\n",
      "end_game\n"
     ]
    }
   ],
   "source": [
    "sum_list = [\n",
    "    [1, 2],\n",
    "    [3, -1],\n",
    "    [100, 20],\n",
    "    [10, -70] ]\n",
    "\n",
    "for x, y in enumerate(sum_list) :\n",
    "    print('---------------{}번째-----------'.format(x+1))\n",
    "    temp = my_class(y[0], y[1])\n",
    "    temp.check_sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 주소록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 저는 서울시 서초구에 사는 20살 마리아입니다\n"
     ]
    }
   ],
   "source": [
    "from make_test import Person\n",
    "\n",
    "maria = Person('마리아', 20, '서울시 서초구')\n",
    "maria.greeting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>손흥민</td>\n",
       "      <td>28</td>\n",
       "      <td>영국 런던</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>송중기</td>\n",
       "      <td>36</td>\n",
       "      <td>서울 어딘가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>꾸러기</td>\n",
       "      <td>10</td>\n",
       "      <td>양평</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1       2\n",
       "0  손흥민  28   영국 런던\n",
       "1  송중기  36  서울 어딘가\n",
       "2  꾸러기  10      양평"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "people_info = [\n",
    "    \n",
    "    ['손흥민', 28, '영국 런던'],\n",
    "    ['송중기', 36, '서울 어딘가'],\n",
    "    ['꾸러기', 10, '양평']\n",
    "]\n",
    "\n",
    "people_info = pd.DataFrame(people_info)\n",
    "people_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------1번째 사람---------\n",
      "\n",
      "안녕하세요 저는 영국 런던에 사는 28살 손흥민입니다\n",
      "\n",
      "----------2번째 사람---------\n",
      "\n",
      "안녕하세요 저는 서울 어딘가에 사는 36살 송중기입니다\n",
      "\n",
      "----------3번째 사람---------\n",
      "\n",
      "안녕하세요 저는 양평에 사는 10살 꾸러기입니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(people_info)) :\n",
    "    \n",
    "    print('----------{}번째 사람---------'.format(x+1))\n",
    "    print()\n",
    "    \n",
    "    person_info = list(people_info.iloc[x,:])\n",
    "    temp = Person(person_info[0], person_info[1], person_info[2])\n",
    "    temp.greeting()\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## matrix _ linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1.shape : (2, 2), matrix2.shape : (2,), matrix3.shape : (2,)\n",
      "[5 5]\n",
      "[3 6]\n"
     ]
    }
   ],
   "source": [
    "from make_test import my_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "a = [[1, 2],\n",
    "     [1, 2]]\n",
    "\n",
    "# b = [[3],\n",
    "#     [1]]\n",
    "\n",
    "# c = [[-2],\n",
    "#     [1]]\n",
    "\n",
    "b = [3, 1]\n",
    "\n",
    "c = [-2, 1]\n",
    "\n",
    "a = np.array(a)\n",
    "b = np.array(b)\n",
    "c = np.array(c)\n",
    "\n",
    "mat = my_matrix(a, b, c)\n",
    "mat.shape()\n",
    "\n",
    "mat.Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 다중 상속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아빠를 닮아 잘 생겼다.\n",
      "눈 부분을 닮았네\n",
      "엄마를 닮아 참 착하다.\n"
     ]
    }
   ],
   "source": [
    "from make_test import father, mother, sister\n",
    "\n",
    "girl = sister('아빠', '눈','엄마')\n",
    "girl.handsome()\n",
    "girl.kind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습_GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABCDEFG\n",
    "# 0123456"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드 연결 정보\n",
    "edges = np.array([[0 ,1],\n",
    "                 [2 ,3],\n",
    "                 [1 ,4],\n",
    "                 [3 ,4],\n",
    "                 [4 ,5],\n",
    "                 [4 ,6]])\n",
    "\n",
    "# 각 노드 특성 정보(H) = 7 X 4\n",
    "features = sp.csr_matrix([[1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [0, 0, 1, 0],\n",
    "                          [0, 0, 0, 1],\n",
    "                          [0, 0, 0, 1]])\n",
    "\n",
    "# edge 특성 정보\n",
    "edge_features = [[3],\n",
    "                 [5],\n",
    "                 [1],\n",
    "                 [10],\n",
    "                 [6],\n",
    "                 [8]]\n",
    "\n",
    "# labels \n",
    "labels = np.array([1,\n",
    "                  4,\n",
    "                  5,\n",
    "                  2,\n",
    "                  6,\n",
    "                  8,\n",
    "                  7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===단위행렬 더하기 전===\n",
      "  (0, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "  (4, 5)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# direction matrix 생성\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])),\n",
    "             shape = (labels.shape[0], labels.shape[0]),\n",
    "             dtype = np.float32)\n",
    "\n",
    "\n",
    "## 확인\n",
    "print('===단위행렬 더하기 전===')\n",
    "print('{}'.format(adj))\n",
    "print(adj.toarray())\n",
    "print()\n",
    "\n",
    "## 고려할 것 : 단위행렬(자기자신의 정보) 전달\n",
    "#adj = adj + sp.eye(adj.shape[0])\n",
    "#print('===단위행렬 더한 후===')\n",
    "#print('{}'.format(adj))\n",
    "#print(adj.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(mx) :\n",
    "    rowsum = np.array(mx.sum(1)) # 각 노드 정보 개수\n",
    "    print('=====row별 feature 특성 합=====')\n",
    "    print(rowsum)\n",
    "\n",
    "    # r_inv\n",
    "    # 역행렬로 np.power 수행\n",
    "    r_inv = np.power(rowsum, 0).flatten() # 0, 1, # power : 0, 1, 8, 27, ,,, / 0, 1, 4, 9, ,,, / 0, 1, 0.5, 0.333, 0.25\n",
    "    print('===== 역행렬로 np.power 수행 =====')\n",
    "    r_inv[np.isinf(r_inv)] = 0 \n",
    "    print(r_inv)\n",
    "\n",
    "\n",
    "    # r_mat_inv\n",
    "    r_mat_inv = sp.diags(r_inv) # 행렬로 만들어줌\n",
    "    print(r_mat_inv.toarray())\n",
    "    \n",
    "    # 노드 adj 와 노드 feature 정보 행렬연산\n",
    "    print('=====adj, feature 행렬곱=====')\n",
    "    mx = r_mat_inv.dot(mx) \n",
    "\n",
    "    print(mx.toarray())\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====row별 feature 특성 합=====\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "[1 1 1 1 1 1 1]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# features : 현재 노드 + 연결된 노드 정보\n",
    "features = normalize(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====row별 feature 특성 합=====\n",
      "[[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [1.]]\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "adj = normalize(adj + sp.eye(adj.shape[0])) # 대각행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 5, 2, 6, 8, 7])\n"
     ]
    }
   ],
   "source": [
    "idx_train = range(5)\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(labels) # 원핫인코딩 된 label 중 해당하는 label이 몇 번 째인지\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "\n",
    "    # 노드\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)) # vstack : 행 추가\n",
    "\n",
    "    # 노드 간 edge의 정보\n",
    "    values = torch.from_numpy(sparse_mx.data) # numpy.ndarray를 tensor로 올려줌\n",
    "\n",
    "    # 노드 개수, 특성 개수\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) # sparse : 크기에 맞게 값을 뿌려주는 것 같은데 규칙 잘 모르겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
       "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
       "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "       size=(7, 7), nnz=13, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(model, Loss, optimizer, num_epochs) :\n",
    "    \n",
    "    train_loss_arr = []\n",
    "    test_loss_arr = []\n",
    "\n",
    "    best_test_loss = 99999999\n",
    "    # early stopping 걸어줌\n",
    "    early_stop, early_stop_max = 0., 20.\n",
    "\n",
    "    best_ACC = 0\n",
    "    final_ACC = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1) :\n",
    "\n",
    "        # forward pass\n",
    "        model.train()\n",
    "        output = model(features)\n",
    "        train_loss = criterion(output[idx_train], labels[idx_train])\n",
    "\n",
    "        # backward, optimize\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss 따로 저장해두고\n",
    "        train_loss_arr.append(train_loss.data)\n",
    "        if epoch % 100 == 1 :\n",
    "            print('epoch : ', epoch, 'train_loss : ', train_loss.item())\n",
    "            \n",
    "        \n",
    "\n",
    "        print('End training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GCN_layer(nn.Module) :\n",
    "    def __init__(self, in_features, out_features, A) :\n",
    "        super(GCN_layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.A = A\n",
    "        self.fc = nn.Linear(in_features,\n",
    "                            out_features)\n",
    "\n",
    "    def forward(self, X) :\n",
    "        print('------------X------------')\n",
    "        print(X)\n",
    "\n",
    "        print('--------self.A-----------') # torch.spmm이 알아서 self.A를 numpy array로 바꾸고  X랑 행렬곱을 진행함\n",
    "        print(A)\n",
    "\n",
    "        print('--------이웃정보종합-------')\n",
    "        print(torch.spmm(self.A, X)) # A X X 행렬곱 한 값\n",
    "\n",
    "        print('--------END_GCN_layer--------')\n",
    "        \n",
    "        return self.fc(torch.spmm(self.A, X)) # 이웃 정보를 종합한다.\n",
    "    \n",
    "\n",
    "\n",
    "class GCN(nn.Module) :\n",
    "    def __init__(self, num_feature, num_class, A) : # feature개수, label개수\n",
    "        super(GCN, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential( GCN_layer(num_feature, 16, A), # A : \n",
    "                                       nn.ReLU(),\n",
    "                                       GCN_layer(16, num_class, A))\n",
    "\n",
    "    def forward(self, X) :\n",
    "        return self.feature_extractor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique().size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8a6a744c3102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# GCN 학습 후 epoch에 따른 loss 확인\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGCN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "# GCN 학습 후 epoch에 따른 loss 확인\n",
    "model = GCN(features.size(1), labels.unique().size(0), A)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "train(model, criterion, optimizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------X------------\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n",
      "--------self.A-----------\n",
      "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
      "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(7, 7), nnz=13, layout=torch.sparse_coo)\n",
      "--------이웃정보종합-------\n",
      "tensor([[1., 1., 0., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [0., 0., 1., 2.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n",
      "--------END_GCN_layer--------\n",
      "------------X------------\n",
      "tensor([[0.0000, 0.2218, 1.1766, 0.0000, 1.2408, 0.0000, 0.2497, 2.3071, 0.0000,\n",
      "         0.0680, 0.0400, 0.1231, 0.5789, 0.6298, 0.6331, 1.6679],\n",
      "        [0.0000, 0.6639, 0.6106, 0.0000, 0.5333, 0.2435, 0.0606, 1.6713, 0.0000,\n",
      "         1.0808, 0.5769, 1.1676, 1.7645, 0.0000, 1.5517, 1.3781],\n",
      "        [0.0000, 0.2218, 1.1766, 0.0000, 1.2408, 0.0000, 0.2497, 2.3071, 0.0000,\n",
      "         0.0680, 0.0400, 0.1231, 0.5789, 0.6298, 0.6331, 1.6679],\n",
      "        [0.0000, 0.6639, 0.6106, 0.0000, 0.5333, 0.2435, 0.0606, 1.6713, 0.0000,\n",
      "         1.0808, 0.5769, 1.1676, 1.7645, 0.0000, 1.5517, 1.3781],\n",
      "        [0.0000, 2.6168, 1.4902, 0.0000, 0.0000, 2.9360, 1.9609, 0.0000, 0.0000,\n",
      "         2.8784, 1.6903, 0.0000, 0.6417, 1.7886, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2605, 1.3436, 0.0000, 0.0000, 1.6698, 1.6787, 0.0000, 0.0000,\n",
      "         1.2302, 0.5925, 0.0000, 0.0434, 1.3607, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2605, 1.3436, 0.0000, 0.0000, 1.6698, 1.6787, 0.0000, 0.0000,\n",
      "         1.2302, 0.5925, 0.0000, 0.0434, 1.3607, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "--------self.A-----------\n",
      "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
      "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(7, 7), nnz=13, layout=torch.sparse_coo)\n",
      "--------이웃정보종합-------\n",
      "tensor([[0.0000, 0.8857, 1.7872, 0.0000, 1.7741, 0.2435, 0.3103, 3.9784, 0.0000,\n",
      "         1.1487, 0.6169, 1.2906, 2.3433, 0.6298, 2.1848, 3.0460],\n",
      "        [0.0000, 3.2807, 2.1007, 0.0000, 0.5333, 3.1795, 2.0215, 1.6713, 0.0000,\n",
      "         3.9591, 2.2672, 1.1676, 2.4062, 1.7886, 1.5517, 1.3781],\n",
      "        [0.0000, 0.8857, 1.7872, 0.0000, 1.7741, 0.2435, 0.3103, 3.9784, 0.0000,\n",
      "         1.1487, 0.6169, 1.2906, 2.3433, 0.6298, 2.1848, 3.0460],\n",
      "        [0.0000, 3.2807, 2.1007, 0.0000, 0.5333, 3.1795, 2.0215, 1.6713, 0.0000,\n",
      "         3.9591, 2.2672, 1.1676, 2.4062, 1.7886, 1.5517, 1.3781],\n",
      "        [0.0000, 5.1379, 4.1774, 0.0000, 0.0000, 6.2756, 5.3184, 0.0000, 0.0000,\n",
      "         5.3387, 2.8752, 0.0000, 0.7284, 4.5100, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2605, 1.3436, 0.0000, 0.0000, 1.6698, 1.6787, 0.0000, 0.0000,\n",
      "         1.2302, 0.5925, 0.0000, 0.0434, 1.3607, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2605, 1.3436, 0.0000, 0.0000, 1.6698, 1.6787, 0.0000, 0.0000,\n",
      "         1.2302, 0.5925, 0.0000, 0.0434, 1.3607, 0.0000, 0.0000]],\n",
      "       grad_fn=<MmBackward>)\n",
      "--------END_GCN_layer--------\n",
      "예측값 : tensor([1, 2, 1, 2, 6, 6, 6])\n",
      "실제값 : tensor([1, 4, 5, 2, 6, 8, 7])\n",
      "tensor(42.8571, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "output = model(features)\n",
    "\n",
    "preds = output.max(1)[1].type_as(labels)\n",
    "print('예측값 : {}'.format(preds))\n",
    "print('실제값 : {}'.format(labels))\n",
    "\n",
    "correct = preds.eq(labels).double()\n",
    "correct = correct.sum()\n",
    "print(correct / len(labels) * 100) \n",
    "# 10 , 100: 42.8571\n",
    "# 1000 : 14.2857 ( 4로 수렴한다. )\n",
    "# 500 : 28. 57~\n",
    "# 10000 : 28.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 노드 연결 정보\n",
    "edges = np.array([[0 ,1],\n",
    "                 [2 ,3],\n",
    "                 [1 ,4],\n",
    "                 [3 ,4],\n",
    "                 [4 ,5],\n",
    "                 [4 ,6]])\n",
    "\n",
    "# 각 노드 특성 정보(H) = 7 X 4\n",
    "features = sp.csr_matrix([[1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [0, 0, 1, 0],\n",
    "                          [0, 0, 0, 1],\n",
    "                          [0, 0, 0, 1]])\n",
    "\n",
    "# edge 특성 정보\n",
    "edge_features = [[3],\n",
    "                 [5],\n",
    "                 [1],\n",
    "                 [10],\n",
    "                 [6],\n",
    "                 [8]]\n",
    "\n",
    "# labels \n",
    "labels = np.array([1,\n",
    "                  4,\n",
    "                  5,\n",
    "                  2,\n",
    "                  6,\n",
    "                  8,\n",
    "                  7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===단위행렬 더하기 전===\n",
      "  (0, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "  (4, 5)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# direction matrix 생성\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])),\n",
    "             shape = (labels.shape[0], labels.shape[0]),\n",
    "             dtype = np.float32)\n",
    "\n",
    "\n",
    "## 확인\n",
    "print('===단위행렬 더하기 전===')\n",
    "print('{}'.format(adj))\n",
    "print(adj.toarray())\n",
    "print()\n",
    "\n",
    "## 고려할 것 : 단위행렬(자기자신의 정보) 전달\n",
    "#adj = adj + sp.eye(adj.shape[0])\n",
    "#print('===단위행렬 더한 후===')\n",
    "#print('{}'.format(adj))\n",
    "#print(adj.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def normalize(mx) :\n",
    "    rowsum = np.array(mx.sum(1)) # 각 노드 정보 개수\n",
    "    print('=====row별 feature 특성 합=====')\n",
    "    print(rowsum)\n",
    "\n",
    "    # r_inv\n",
    "    # 역행렬로 np.power 수행\n",
    "    r_inv = np.power(rowsum, 0).flatten() # 0, 1, # power : 0, 1, 8, 27, ,,, / 0, 1, 4, 9, ,,, / 0, 1, 0.5, 0.333, 0.25\n",
    "    print('===== 역행렬로 np.power 수행 =====')\n",
    "    r_inv[np.isinf(r_inv)] = 0 \n",
    "    print(r_inv)\n",
    "\n",
    "\n",
    "    # r_mat_inv\n",
    "    r_mat_inv = sp.diags(r_inv) # 행렬로 만들어줌\n",
    "    print(r_mat_inv.toarray())\n",
    "    \n",
    "    # 노드 adj 와 노드 feature 정보 행렬연산\n",
    "    print('=====adj, feature 행렬곱=====')\n",
    "    mx = r_mat_inv.dot(mx) \n",
    "\n",
    "    print(mx.toarray())\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====row별 feature 특성 합=====\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "[1 1 1 1 1 1 1]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# features : 현재 노드 + 연결된 노드 정보\n",
    "features = normalize(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====row별 feature 특성 합=====\n",
      "[[2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [1.]]\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "adj = normalize(adj + sp.eye(adj.shape[0])) # 대각행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 5, 2, 6, 8, 7])\n"
     ]
    }
   ],
   "source": [
    "idx_train = range(5)\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(labels) # 원핫인코딩 된 label 중 해당하는 label이 몇 번 째인지\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "\n",
    "    # 노드\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)) # vstack : 행 추가\n",
    "\n",
    "    # 노드 간 edge의 정보\n",
    "    values = torch.from_numpy(sparse_mx.data) # numpy.ndarray를 tensor로 올려줌\n",
    "\n",
    "    # 노드 개수, 특성 개수\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "\n",
    "    return torch.sparse.LongTensor(indices, values, shape) # sparse : 크기에 맞게 값을 뿌려주는 것 같은데 규칙 잘 모르겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
       "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
       "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "       size=(7, 7), nnz=13, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "A = adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels : (4, 4)\n",
      "lin_l : Linear(in_features=4, out_features=7, bias=True)\n",
      "lin_r : Linear(in_features=4, out_features=7, bias=False)\n",
      "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
      "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(7, 7), nnz=13, layout=torch.sparse_coo)\n",
      "(tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]]), tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]]))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a8d23f261e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSAGEConv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\연구\\module_mine\\new2\\raw_graphsage.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# edge_index는 propagate 오류남\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# lin_l에 변수 두개 들어가야하는데 하나만 들어감 어떻게 계산?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0maggregate\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mto\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mnode\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \"\"\"\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__check_input__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# Run \"fused\" message and aggregation (if applicable).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__check_input__\u001b[1;34m(self, edge_index, size)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from raw_graphsage import SAGEConv\n",
    "\n",
    "model = SAGEConv(4, 7)\n",
    "model(features, edge_index = A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-253064b5f801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSAGEConv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0maggregate\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mto\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mnode\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \"\"\"\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__check_input__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# Run \"fused\" message and aggregation (if applicable).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__check_input__\u001b[1;34m(self, edge_index, size)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "model = SAGEConv(4, 7)\n",
    "model(features, edge_index = A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch_geometric.nn.conv.sage_conv.SAGEConv.forward(self, x: Union[torch.Tensor, Tuple[torch.Tensor, Union[torch.Tensor, NoneType]]], edge_index: Union[torch.Tensor, torch_sparse.tensor.SparseTensor], size: Union[Tuple[int, int], NoneType] = None) -> torch.Tensor>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "\n",
    "SAGEConv(4, 7)\n",
    "SAGEConv.forward\n",
    "#SAGEConv.message \n",
    "#SAGEConv.message_and_aggregate # self, adj_t, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(model, Loss, optimizer, num_epochs) :\n",
    "    \n",
    "    train_loss_arr = []\n",
    "    test_loss_arr = []\n",
    "\n",
    "    best_test_loss = 99999999\n",
    "    # early stopping 걸어줌\n",
    "    early_stop, early_stop_max = 0., 20.\n",
    "\n",
    "    best_ACC = 0\n",
    "    final_ACC = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1) :\n",
    "\n",
    "        # forward pass\n",
    "        model.train()\n",
    "        output = model(features) #input\n",
    "        train_loss = criterion(output[idx_train], labels[idx_train])\n",
    "\n",
    "        # backward, optimize\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss 따로 저장해두고\n",
    "        train_loss_arr.append(train_loss.data)\n",
    "        \n",
    "        if epoch % 100 == 1 :\n",
    "            print('epoch : ', epoch, 'train_loss : ', train_loss.item())\n",
    "\n",
    "        print('End training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 쌓을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGENet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): SAGEConv(4, 16)\n",
      "    (1): ReLU()\n",
      "    (2): SAGEConv(16, 7)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class SAGENet(torch.nn.Module) :\n",
    "    def __init__(self, num_feature, num_class, A) :\n",
    "        super(SAGENet, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(SAGEConv(num_feature, 16, A),\n",
    "                                              nn.ReLU(),\n",
    "                                              SAGEConv(16, num_class, A))\n",
    "        \n",
    "#        mlp1 = ###\n",
    "#        self.conv1 = SAGEConv(###)\n",
    "#        mlp2 = ###\n",
    "#        self.conv2 = SAGEConv(###)\n",
    "#        self.classifier = Linear(32, )###)\n",
    "                                 \n",
    "    def forward(self, X) :\n",
    "        \n",
    "        return self.feature_extractor(X)\n",
    "    \n",
    "#        edge_index = knn_graph(pos, k=16, batch = batch, loop = False)\n",
    "                                 \n",
    "#        x = self.conv1(x = None, edge_index = edge_index)\n",
    "#        x = x.relu()\n",
    "#        x = self.conv2(x = x, edge_index = edge_index)\n",
    "#        x = x.relu()\n",
    "            \n",
    "#        x = global_max_pool(x, batch)\n",
    "        \n",
    "            \n",
    "        # return self.classifier(x)\n",
    "            \n",
    "model = SAGENet(4, 7, adj)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
       "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
       "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "       size=(7, 7), nnz=13, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-e2400572a824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-32f84cb0bd0e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, Loss, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'edge_index'"
     ]
    }
   ],
   "source": [
    "model = SAGEConv(features.size(1), labels.unique().size(0))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "train(model, criterion, optimizer, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 대기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------reset_parameters----------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SAGE(10, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from new_graphsage import SAGE\n",
    "\n",
    "SAGE(10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "#노드 연결 정보\n",
    "edges = np.array([[0 ,1],\n",
    "                 [2 ,3],\n",
    "                 [1 ,4],\n",
    "                 [3 ,4],\n",
    "                 [4 ,5],\n",
    "                 [4 ,6]])\n",
    "\n",
    "# 각 노드 특성 정보(H) = 7 X 4\n",
    "features = sp.csr_matrix([[1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [0, 0, 1, 0],\n",
    "                          [0, 0, 0, 1],\n",
    "                          [0, 0, 0, 1]])\n",
    "\n",
    "# edge 특성 정보\n",
    "edge_features = [[3],\n",
    "                 [5],\n",
    "                 [1],\n",
    "                 [10],\n",
    "                 [6],\n",
    "                 [8]]\n",
    "\n",
    "# labels \n",
    "labels = np.array([1,\n",
    "                  4,\n",
    "                  5,\n",
    "                  2,\n",
    "                  6,\n",
    "                  3,\n",
    "                  0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[6, 2], x=[7, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "edge_index = torch.tensor([[0 ,1],\n",
    "                             [2 ,3],\n",
    "                             [1 ,4],\n",
    "                             [3 ,4],\n",
    "                             [4 ,5],\n",
    "                             [4 ,6]], dtype=torch.long)\n",
    "x = torch.tensor([[0], [1], [2], [3], [4], [5], [6]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "data\n",
    "\n",
    "# edge 개수 : 6\n",
    "# node 개수 : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===단위행렬 더하기 전===\n",
      "  (0, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "  (4, 5)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 단위 행렬 더해주기\n",
    "# direction matrix 생성\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])),\n",
    "             shape = (labels.shape[0], labels.shape[0]),\n",
    "             dtype = np.float32)\n",
    "\n",
    "\n",
    "## 확인\n",
    "print('===단위행렬 더하기 전===')\n",
    "print('{}'.format(adj))\n",
    "print(adj.toarray())\n",
    "print()\n",
    "\n",
    "## 고려할 것 : 단위행렬(자기자신의 정보) 전달\n",
    "#adj = adj + sp.eye(adj.shape[0])\n",
    "#print('===단위행렬 더한 후===')\n",
    "#print('{}'.format(adj))\n",
    "#print(adj.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def normalize(mx) :\n",
    "    rowsum = np.array(mx.sum(1)) # 각 노드 정보 개수\n",
    "    print('=====row별 feature 특성 합=====')\n",
    "    print(rowsum)\n",
    "\n",
    "    # r_inv\n",
    "    # 역행렬로 np.power 수행\n",
    "    r_inv = np.power(rowsum, 0).flatten() # 0, 1, # power : 0, 1, 8, 27, ,,, / 0, 1, 4, 9, ,,, / 0, 1, 0.5, 0.333, 0.25\n",
    "    print('===== 역행렬로 np.power 수행 =====')\n",
    "    r_inv[np.isinf(r_inv)] = 0 \n",
    "    print(r_inv)\n",
    "\n",
    "\n",
    "    # r_mat_inv\n",
    "    r_mat_inv = sp.diags(r_inv) # 행렬로 만들어줌\n",
    "    print(r_mat_inv.toarray())\n",
    "    \n",
    "    # 노드 adj 와 노드 feature 정보 행렬연산\n",
    "    print('=====adj, feature 행렬곱=====')\n",
    "    mx = r_mat_inv.dot(mx) \n",
    "\n",
    "    print(mx.toarray())\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===단위행렬 더한 후===\n",
      "[[1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "======1번째======\n",
      "[[1. 1. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 1. 2.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "======2번째======\n",
      "[[1. 2. 1. 0.]\n",
      " [0. 1. 2. 2.]\n",
      " [1. 2. 1. 0.]\n",
      " [0. 1. 2. 2.]\n",
      " [0. 0. 1. 4.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# num_nodes = max + 1 = 7\n",
    "# num_edges = len(edges) = 6\n",
    "\n",
    "num_nodes = 7\n",
    "num_edges = len(edges)\n",
    "\n",
    "# direction matrix 생성\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labels.shape[0], labels.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "\n",
    "## 고려할 것 : 단위행렬(자기자신의 정보) 전달\n",
    "adj = adj + sp.eye(adj.shape[0])\n",
    "print('===단위행렬 더한 후===')\n",
    "print(adj.toarray())\n",
    "\n",
    "# edge정보를 어떻게 맞춰줄것인가_dimension\n",
    "\n",
    "# 행렬곱을 위한 array \n",
    "adj_arr = adj.toarray()\n",
    "features_arr = features.toarray()\n",
    "\n",
    "# hop 에 따라 정보 전달\n",
    "hop = 2\n",
    "for idx in range(hop) :\n",
    "    features_arr = np.matmul(adj_arr, features_arr) # 여기에 weight 곱하기, bias 더하기\n",
    "    print('======{0}번째======'.format(idx+1))\n",
    "    print('{}'.format(features_arr))\n",
    "    \n",
    "    \n",
    "# feature 정보 학습, label regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====row별 feature 특성 합=====\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "[1 1 1 1 1 1 1]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "=====row별 feature 특성 합=====\n",
      "[[3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]]\n",
      "===== 역행렬로 np.power 수행 =====\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "=====adj, feature 행렬곱=====\n",
      "[[2. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 2. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 2. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 2.]]\n",
      "tensor([1, 4, 5, 2, 6, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# features : 현재 노드 + 연결된 노드 정보\n",
    "features = normalize(features)\n",
    "\n",
    "adj = normalize(adj + sp.eye(adj.shape[0])) # 대각행렬\n",
    "\n",
    "idx_train = range(5)\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(labels) # 원핫인코딩 된 label 중 해당하는 label이 몇 번 째인지\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "\n",
    "    # 노드\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)) # vstack : 행 추가\n",
    "\n",
    "    # 노드 간 edge의 정보\n",
    "    values = torch.from_numpy(sparse_mx.data) # numpy.ndarray를 tensor로 올려줌\n",
    "\n",
    "    # 노드 개수, 특성 개수\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) # sparse : 크기에 맞게 값을 뿌려주는 것 같은데 규칙 잘 모르겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 2, 2, 3, 1, 3, 4, 4, 5, 4, 6],\n",
       "                       [0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6]]),\n",
       "       values=tensor([2., 1., 2., 2., 1., 2., 1., 1., 2., 1., 2., 1., 2.]),\n",
       "       size=(7, 7), nnz=13, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 연결\n",
    "import torch\n",
    "torch.manual_seed(777)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, tensor([[0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 0.]]), cost : 2.0560359954833984\n",
      "epoch = 1, tensor([[0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0.]]), cost : 1.1990779638290405\n",
      "epoch = 2, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8610450625419617\n",
      "epoch = 3, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.84809410572052\n",
      "epoch = 4, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8398990631103516\n",
      "epoch = 5, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8378618359565735\n",
      "epoch = 6, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8366185426712036\n",
      "epoch = 7, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8358139991760254\n",
      "epoch = 8, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8352808952331543\n",
      "epoch = 9, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8349215388298035\n",
      "epoch = 10, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8346753716468811\n",
      "epoch = 11, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8345035314559937\n",
      "epoch = 12, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8343814611434937\n",
      "epoch = 13, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8342931866645813\n",
      "epoch = 14, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8342281579971313\n",
      "epoch = 15, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8341795802116394\n",
      "epoch = 16, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8341427445411682\n",
      "epoch = 17, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8341141939163208\n",
      "epoch = 18, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340919613838196\n",
      "epoch = 19, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340743780136108\n",
      "epoch = 20, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340601325035095\n",
      "epoch = 21, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340487480163574\n",
      "epoch = 22, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340393304824829\n",
      "epoch = 23, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340316414833069\n",
      "epoch = 24, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340252637863159\n",
      "epoch = 25, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340198397636414\n",
      "epoch = 26, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340152502059937\n",
      "epoch = 27, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340113759040833\n",
      "epoch = 28, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340078592300415\n",
      "epoch = 29, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340049982070923\n",
      "epoch = 30, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340024948120117\n",
      "epoch = 31, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8340004086494446\n",
      "epoch = 32, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339983820915222\n",
      "epoch = 33, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339967131614685\n",
      "epoch = 34, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339952826499939\n",
      "epoch = 35, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339939117431641\n",
      "epoch = 36, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339927792549133\n",
      "epoch = 37, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339917063713074\n",
      "epoch = 38, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833990752696991\n",
      "epoch = 39, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339899778366089\n",
      "epoch = 40, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833989143371582\n",
      "epoch = 41, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339884877204895\n",
      "epoch = 42, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833987832069397\n",
      "epoch = 43, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833987295627594\n",
      "epoch = 44, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339866995811462\n",
      "epoch = 45, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339862823486328\n",
      "epoch = 46, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339858651161194\n",
      "epoch = 47, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833985447883606\n",
      "epoch = 48, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339849710464478\n",
      "epoch = 49, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339846730232239\n",
      "epoch = 50, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833984375\n",
      "epoch = 51, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339840769767761\n",
      "epoch = 52, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833983838558197\n",
      "epoch = 53, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339835405349731\n",
      "epoch = 54, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833983302116394\n",
      "epoch = 55, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339830636978149\n",
      "epoch = 56, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339828848838806\n",
      "epoch = 57, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339826464653015\n",
      "epoch = 58, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339824676513672\n",
      "epoch = 59, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339823484420776\n",
      "epoch = 60, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339821100234985\n",
      "epoch = 61, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833981990814209\n",
      "epoch = 62, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339818120002747\n",
      "epoch = 63, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339816331863403\n",
      "epoch = 64, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833981454372406\n",
      "epoch = 65, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339813351631165\n",
      "epoch = 66, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339811563491821\n",
      "epoch = 67, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339809775352478\n",
      "epoch = 68, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833980917930603\n",
      "epoch = 69, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339807391166687\n",
      "epoch = 70, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339806199073792\n",
      "epoch = 71, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339805603027344\n",
      "epoch = 72, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339804410934448\n",
      "epoch = 73, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339802622795105\n",
      "epoch = 74, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833980143070221\n",
      "epoch = 75, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339800834655762\n",
      "epoch = 76, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339799642562866\n",
      "epoch = 77, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339799046516418\n",
      "epoch = 78, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339797258377075\n",
      "epoch = 79, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339796662330627\n",
      "epoch = 80, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833979606628418\n",
      "epoch = 81, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339794874191284\n",
      "epoch = 82, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339794278144836\n",
      "epoch = 83, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339793086051941\n",
      "epoch = 84, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339791893959045\n",
      "epoch = 85, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339791297912598\n",
      "epoch = 86, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833979070186615\n",
      "epoch = 87, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339789509773254\n",
      "epoch = 88, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339788317680359\n",
      "epoch = 89, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339787721633911\n",
      "epoch = 90, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339787125587463\n",
      "epoch = 91, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339785933494568\n",
      "epoch = 92, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833978533744812\n",
      "epoch = 93, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339784741401672\n",
      "epoch = 94, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339782953262329\n",
      "epoch = 95, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339782357215881\n",
      "epoch = 96, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339781761169434\n",
      "epoch = 97, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339781165122986\n",
      "epoch = 98, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.833977997303009\n",
      "epoch = 99, tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]]), cost : 0.8339778780937195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "linear1 = torch.nn.Linear(7, 7, bias=True)\n",
    "activate = torch.nn.ReLU()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, activate)\n",
    "\n",
    "# define cost / loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(adj)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, labels)\n",
    "    \n",
    "    print('epoch = {}, {}, cost : {}'.format(step, (hypothesis > 0.5).float(), cost.item()))\n",
    "    \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#     if step % 1000 == 0:\n",
    "#         print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Hypothesis:  [[ 0.        0.        0.        0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.       10.997732  0.        0.      ]\n",
      " [ 0.        0.        0.        0.        0.        0.        0.      ]\n",
      " [ 0.        0.       12.962054  0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.       17.623104  0.        0.        0.      ]\n",
      " [21.348667  0.        0.        0.        0.        0.        0.      ]] \n",
      "Correct:  [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]] \n",
      "Accuracy:  16.326530277729034 %\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(adj)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    print(predicted)\n",
    "    accuracy = (predicted == labels).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), \n",
    "          '\\nCorrect: ', predicted.detach().cpu().numpy(), \n",
    "          '\\nAccuracy: ', accuracy.item() * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## modeling_Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.619379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.857290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.628194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.725426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.498926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.867042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.934539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.864829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.241539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.823689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     x         y\n",
       "0           0   1.0  1.619379\n",
       "1           1   2.0  1.857290\n",
       "2           2   3.0  2.628194\n",
       "3           3   4.0  2.725426\n",
       "4           4   5.0  3.498926\n",
       "5           5   6.0  3.867042\n",
       "6           6   7.0  4.934539\n",
       "7           7   8.0  4.864829\n",
       "8           8   9.0  5.241539\n",
       "9           9  10.0  5.823689"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('02_Linear_Regression_Model_Data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_data = torch.FloatTensor(data.iloc[:,1])\n",
    "y_data = torch.FloatTensor(data.iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<AddBackward0>)\n",
      "Epoch :  0 cost :  15.700907707214355\n",
      "tensor([0.5615, 1.0489, 1.5363, 2.0236, 2.5110, 2.9984, 3.4858, 3.9731, 4.4605,\n",
      "        4.9479], grad_fn=<AddBackward0>)\n",
      "tensor([0.6845, 1.2758, 1.8671, 2.4584, 3.0497, 3.6411, 4.2324, 4.8237, 5.4150,\n",
      "        6.0063], grad_fn=<AddBackward0>)\n",
      "tensor([0.7135, 1.3266, 1.9398, 2.5529, 3.1660, 3.7792, 4.3923, 5.0054, 5.6186,\n",
      "        6.2317], grad_fn=<AddBackward0>)\n",
      "tensor([0.7224, 1.3397, 1.9571, 2.5745, 3.1918, 3.8092, 4.4265, 5.0439, 5.6612,\n",
      "        6.2786], grad_fn=<AddBackward0>)\n",
      "tensor([0.7270, 1.3448, 1.9626, 2.5804, 3.1982, 3.8160, 4.4339, 5.0517, 5.6695,\n",
      "        6.2873], grad_fn=<AddBackward0>)\n",
      "tensor([0.7306, 1.3481, 1.9655, 2.5830, 3.2005, 3.8179, 4.4354, 5.0529, 5.6703,\n",
      "        6.2878], grad_fn=<AddBackward0>)\n",
      "tensor([0.7340, 1.3510, 1.9679, 2.5849, 3.2018, 3.8188, 4.4357, 5.0527, 5.6696,\n",
      "        6.2866], grad_fn=<AddBackward0>)\n",
      "tensor([0.7374, 1.3538, 1.9702, 2.5866, 3.2030, 3.8194, 4.4358, 5.0522, 5.6686,\n",
      "        6.2850], grad_fn=<AddBackward0>)\n",
      "tensor([0.7407, 1.3566, 1.9724, 2.5882, 3.2041, 3.8199, 4.4358, 5.0516, 5.6674,\n",
      "        6.2833], grad_fn=<AddBackward0>)\n",
      "tensor([0.7440, 1.3593, 1.9746, 2.5899, 3.2052, 3.8205, 4.4357, 5.0510, 5.6663,\n",
      "        6.2816], grad_fn=<AddBackward0>)\n",
      "tensor([0.7474, 1.3621, 1.9768, 2.5915, 3.2063, 3.8210, 4.4357, 5.0505, 5.6652,\n",
      "        6.2799], grad_fn=<AddBackward0>)\n",
      "tensor([0.7506, 1.3648, 1.9790, 2.5932, 3.2074, 3.8215, 4.4357, 5.0499, 5.6641,\n",
      "        6.2782], grad_fn=<AddBackward0>)\n",
      "tensor([0.7539, 1.3676, 1.9812, 2.5948, 3.2084, 3.8221, 4.4357, 5.0493, 5.6629,\n",
      "        6.2766], grad_fn=<AddBackward0>)\n",
      "tensor([0.7572, 1.3703, 1.9834, 2.5964, 3.2095, 3.8226, 4.4357, 5.0487, 5.6618,\n",
      "        6.2749], grad_fn=<AddBackward0>)\n",
      "tensor([0.7605, 1.3730, 1.9855, 2.5980, 3.2106, 3.8231, 4.4356, 5.0482, 5.6607,\n",
      "        6.2732], grad_fn=<AddBackward0>)\n",
      "tensor([0.7637, 1.3757, 1.9877, 2.5997, 3.2116, 3.8236, 4.4356, 5.0476, 5.6596,\n",
      "        6.2716], grad_fn=<AddBackward0>)\n",
      "tensor([0.7669, 1.3784, 1.9898, 2.6013, 3.2127, 3.8242, 4.4356, 5.0470, 5.6585,\n",
      "        6.2699], grad_fn=<AddBackward0>)\n",
      "tensor([0.7701, 1.3810, 1.9919, 2.6029, 3.2138, 3.8247, 4.4356, 5.0465, 5.6574,\n",
      "        6.2683], grad_fn=<AddBackward0>)\n",
      "tensor([0.7733, 1.3837, 1.9941, 2.6044, 3.2148, 3.8252, 4.4356, 5.0459, 5.6563,\n",
      "        6.2667], grad_fn=<AddBackward0>)\n",
      "tensor([0.7765, 1.3864, 1.9962, 2.6060, 3.2159, 3.8257, 4.4355, 5.0454, 5.6552,\n",
      "        6.2650], grad_fn=<AddBackward0>)\n",
      "tensor([0.7797, 1.3890, 1.9983, 2.6076, 3.2169, 3.8262, 4.4355, 5.0448, 5.6541,\n",
      "        6.2634], grad_fn=<AddBackward0>)\n",
      "tensor([0.7829, 1.3916, 2.0004, 2.6092, 3.2179, 3.8267, 4.4355, 5.0443, 5.6530,\n",
      "        6.2618], grad_fn=<AddBackward0>)\n",
      "tensor([0.7860, 1.3943, 2.0025, 2.6107, 3.2190, 3.8272, 4.4355, 5.0437, 5.6520,\n",
      "        6.2602], grad_fn=<AddBackward0>)\n",
      "tensor([0.7891, 1.3969, 2.0046, 2.6123, 3.2200, 3.8277, 4.4355, 5.0432, 5.6509,\n",
      "        6.2586], grad_fn=<AddBackward0>)\n",
      "tensor([0.7923, 1.3995, 2.0067, 2.6138, 3.2210, 3.8282, 4.4354, 5.0426, 5.6498,\n",
      "        6.2570], grad_fn=<AddBackward0>)\n",
      "tensor([0.7954, 1.4020, 2.0087, 2.6154, 3.2221, 3.8287, 4.4354, 5.0421, 5.6488,\n",
      "        6.2554], grad_fn=<AddBackward0>)\n",
      "tensor([0.7985, 1.4046, 2.0108, 2.6169, 3.2231, 3.8292, 4.4354, 5.0416, 5.6477,\n",
      "        6.2539], grad_fn=<AddBackward0>)\n",
      "tensor([0.8015, 1.4072, 2.0128, 2.6185, 3.2241, 3.8297, 4.4354, 5.0410, 5.6467,\n",
      "        6.2523], grad_fn=<AddBackward0>)\n",
      "tensor([0.8046, 1.4097, 2.0149, 2.6200, 3.2251, 3.8302, 4.4354, 5.0405, 5.6456,\n",
      "        6.2507], grad_fn=<AddBackward0>)\n",
      "tensor([0.8077, 1.4123, 2.0169, 2.6215, 3.2261, 3.8307, 4.4353, 5.0399, 5.6446,\n",
      "        6.2492], grad_fn=<AddBackward0>)\n",
      "tensor([0.8107, 1.4148, 2.0189, 2.6230, 3.2271, 3.8312, 4.4353, 5.0394, 5.6435,\n",
      "        6.2476], grad_fn=<AddBackward0>)\n",
      "tensor([0.8137, 1.4173, 2.0209, 2.6245, 3.2281, 3.8317, 4.4353, 5.0389, 5.6425,\n",
      "        6.2461], grad_fn=<AddBackward0>)\n",
      "tensor([0.8168, 1.4198, 2.0229, 2.6260, 3.2291, 3.8322, 4.4353, 5.0384, 5.6415,\n",
      "        6.2445], grad_fn=<AddBackward0>)\n",
      "tensor([0.8198, 1.4223, 2.0249, 2.6275, 3.2301, 3.8327, 4.4353, 5.0378, 5.6404,\n",
      "        6.2430], grad_fn=<AddBackward0>)\n",
      "tensor([0.8228, 1.4248, 2.0269, 2.6290, 3.2311, 3.8332, 4.4352, 5.0373, 5.6394,\n",
      "        6.2415], grad_fn=<AddBackward0>)\n",
      "tensor([0.8257, 1.4273, 2.0289, 2.6305, 3.2321, 3.8336, 4.4352, 5.0368, 5.6384,\n",
      "        6.2400], grad_fn=<AddBackward0>)\n",
      "tensor([0.8287, 1.4298, 2.0309, 2.6320, 3.2330, 3.8341, 4.4352, 5.0363, 5.6374,\n",
      "        6.2385], grad_fn=<AddBackward0>)\n",
      "tensor([0.8317, 1.4322, 2.0328, 2.6334, 3.2340, 3.8346, 4.4352, 5.0358, 5.6364,\n",
      "        6.2369], grad_fn=<AddBackward0>)\n",
      "tensor([0.8346, 1.4347, 2.0348, 2.6349, 3.2350, 3.8351, 4.4352, 5.0353, 5.6354,\n",
      "        6.2354], grad_fn=<AddBackward0>)\n",
      "tensor([0.8375, 1.4371, 2.0367, 2.6363, 3.2359, 3.8355, 4.4351, 5.0347, 5.6344,\n",
      "        6.2340], grad_fn=<AddBackward0>)\n",
      "tensor([0.8404, 1.4396, 2.0387, 2.6378, 3.2369, 3.8360, 4.4351, 5.0342, 5.6334,\n",
      "        6.2325], grad_fn=<AddBackward0>)\n",
      "tensor([0.8434, 1.4420, 2.0406, 2.6392, 3.2379, 3.8365, 4.4351, 5.0337, 5.6324,\n",
      "        6.2310], grad_fn=<AddBackward0>)\n",
      "tensor([0.8462, 1.4444, 2.0425, 2.6407, 3.2388, 3.8369, 4.4351, 5.0332, 5.6314,\n",
      "        6.2295], grad_fn=<AddBackward0>)\n",
      "tensor([0.8491, 1.4468, 2.0444, 2.6421, 3.2398, 3.8374, 4.4351, 5.0327, 5.6304,\n",
      "        6.2280], grad_fn=<AddBackward0>)\n",
      "tensor([0.8520, 1.4492, 2.0463, 2.6435, 3.2407, 3.8379, 4.4351, 5.0322, 5.6294,\n",
      "        6.2266], grad_fn=<AddBackward0>)\n",
      "tensor([0.8549, 1.4515, 2.0482, 2.6449, 3.2416, 3.8383, 4.4350, 5.0317, 5.6284,\n",
      "        6.2251], grad_fn=<AddBackward0>)\n",
      "tensor([0.8577, 1.4539, 2.0501, 2.6464, 3.2426, 3.8388, 4.4350, 5.0312, 5.6275,\n",
      "        6.2237], grad_fn=<AddBackward0>)\n",
      "tensor([0.8605, 1.4563, 2.0520, 2.6478, 3.2435, 3.8393, 4.4350, 5.0307, 5.6265,\n",
      "        6.2222], grad_fn=<AddBackward0>)\n",
      "tensor([0.8633, 1.4586, 2.0539, 2.6492, 3.2444, 3.8397, 4.4350, 5.0303, 5.6255,\n",
      "        6.2208], grad_fn=<AddBackward0>)\n",
      "tensor([0.8662, 1.4610, 2.0558, 2.6506, 3.2454, 3.8402, 4.4350, 5.0298, 5.6246,\n",
      "        6.2194], grad_fn=<AddBackward0>)\n",
      "tensor([0.8690, 1.4633, 2.0576, 2.6519, 3.2463, 3.8406, 4.4349, 5.0293, 5.6236,\n",
      "        6.2179], grad_fn=<AddBackward0>)\n",
      "tensor([0.8717, 1.4656, 2.0595, 2.6533, 3.2472, 3.8411, 4.4349, 5.0288, 5.6227,\n",
      "        6.2165], grad_fn=<AddBackward0>)\n",
      "tensor([0.8745, 1.4679, 2.0613, 2.6547, 3.2481, 3.8415, 4.4349, 5.0283, 5.6217,\n",
      "        6.2151], grad_fn=<AddBackward0>)\n",
      "tensor([0.8773, 1.4702, 2.0631, 2.6561, 3.2490, 3.8420, 4.4349, 5.0278, 5.6208,\n",
      "        6.2137], grad_fn=<AddBackward0>)\n",
      "tensor([0.8800, 1.4725, 2.0650, 2.6575, 3.2499, 3.8424, 4.4349, 5.0273, 5.6198,\n",
      "        6.2123], grad_fn=<AddBackward0>)\n",
      "tensor([0.8828, 1.4748, 2.0668, 2.6588, 3.2508, 3.8428, 4.4349, 5.0269, 5.6189,\n",
      "        6.2109], grad_fn=<AddBackward0>)\n",
      "tensor([0.8855, 1.4771, 2.0686, 2.6602, 3.2517, 3.8433, 4.4348, 5.0264, 5.6180,\n",
      "        6.2095], grad_fn=<AddBackward0>)\n",
      "tensor([0.8882, 1.4793, 2.0704, 2.6615, 3.2526, 3.8437, 4.4348, 5.0259, 5.6170,\n",
      "        6.2081], grad_fn=<AddBackward0>)\n",
      "tensor([0.8909, 1.4816, 2.0722, 2.6629, 3.2535, 3.8442, 4.4348, 5.0255, 5.6161,\n",
      "        6.2067], grad_fn=<AddBackward0>)\n",
      "tensor([0.8936, 1.4838, 2.0740, 2.6642, 3.2544, 3.8446, 4.4348, 5.0250, 5.6152,\n",
      "        6.2054], grad_fn=<AddBackward0>)\n",
      "tensor([0.8963, 1.4860, 2.0758, 2.6655, 3.2553, 3.8450, 4.4348, 5.0245, 5.6143,\n",
      "        6.2040], grad_fn=<AddBackward0>)\n",
      "tensor([0.8990, 1.4883, 2.0776, 2.6669, 3.2562, 3.8455, 4.4348, 5.0240, 5.6133,\n",
      "        6.2026], grad_fn=<AddBackward0>)\n",
      "tensor([0.9016, 1.4905, 2.0793, 2.6682, 3.2570, 3.8459, 4.4347, 5.0236, 5.6124,\n",
      "        6.2013], grad_fn=<AddBackward0>)\n",
      "tensor([0.9043, 1.4927, 2.0811, 2.6695, 3.2579, 3.8463, 4.4347, 5.0231, 5.6115,\n",
      "        6.1999], grad_fn=<AddBackward0>)\n",
      "tensor([0.9069, 1.4949, 2.0828, 2.6708, 3.2588, 3.8467, 4.4347, 5.0227, 5.6106,\n",
      "        6.1986], grad_fn=<AddBackward0>)\n",
      "tensor([0.9095, 1.4971, 2.0846, 2.6721, 3.2596, 3.8472, 4.4347, 5.0222, 5.6097,\n",
      "        6.1973], grad_fn=<AddBackward0>)\n",
      "tensor([0.9121, 1.4992, 2.0863, 2.6734, 3.2605, 3.8476, 4.4347, 5.0218, 5.6088,\n",
      "        6.1959], grad_fn=<AddBackward0>)\n",
      "tensor([0.9148, 1.5014, 2.0881, 2.6747, 3.2614, 3.8480, 4.4347, 5.0213, 5.6080,\n",
      "        6.1946], grad_fn=<AddBackward0>)\n",
      "tensor([0.9173, 1.5036, 2.0898, 2.6760, 3.2622, 3.8484, 4.4346, 5.0208, 5.6071,\n",
      "        6.1933], grad_fn=<AddBackward0>)\n",
      "tensor([0.9199, 1.5057, 2.0915, 2.6773, 3.2631, 3.8488, 4.4346, 5.0204, 5.6062,\n",
      "        6.1920], grad_fn=<AddBackward0>)\n",
      "tensor([0.9225, 1.5078, 2.0932, 2.6785, 3.2639, 3.8493, 4.4346, 5.0200, 5.6053,\n",
      "        6.1907], grad_fn=<AddBackward0>)\n",
      "tensor([0.9251, 1.5100, 2.0949, 2.6798, 3.2647, 3.8497, 4.4346, 5.0195, 5.6044,\n",
      "        6.1893], grad_fn=<AddBackward0>)\n",
      "tensor([0.9276, 1.5121, 2.0966, 2.6811, 3.2656, 3.8501, 4.4346, 5.0191, 5.6036,\n",
      "        6.1880], grad_fn=<AddBackward0>)\n",
      "tensor([0.9301, 1.5142, 2.0983, 2.6824, 3.2664, 3.8505, 4.4346, 5.0186, 5.6027,\n",
      "        6.1868], grad_fn=<AddBackward0>)\n",
      "tensor([0.9327, 1.5163, 2.1000, 2.6836, 3.2672, 3.8509, 4.4345, 5.0182, 5.6018,\n",
      "        6.1855], grad_fn=<AddBackward0>)\n",
      "tensor([0.9352, 1.5184, 2.1016, 2.6849, 3.2681, 3.8513, 4.4345, 5.0177, 5.6010,\n",
      "        6.1842], grad_fn=<AddBackward0>)\n",
      "tensor([0.9377, 1.5205, 2.1033, 2.6861, 3.2689, 3.8517, 4.4345, 5.0173, 5.6001,\n",
      "        6.1829], grad_fn=<AddBackward0>)\n",
      "tensor([0.9402, 1.5226, 2.1050, 2.6873, 3.2697, 3.8521, 4.4345, 5.0169, 5.5992,\n",
      "        6.1816], grad_fn=<AddBackward0>)\n",
      "tensor([0.9427, 1.5247, 2.1066, 2.6886, 3.2705, 3.8525, 4.4345, 5.0164, 5.5984,\n",
      "        6.1804], grad_fn=<AddBackward0>)\n",
      "tensor([0.9452, 1.5267, 2.1083, 2.6898, 3.2714, 3.8529, 4.4345, 5.0160, 5.5976,\n",
      "        6.1791], grad_fn=<AddBackward0>)\n",
      "tensor([0.9476, 1.5288, 2.1099, 2.6910, 3.2722, 3.8533, 4.4344, 5.0156, 5.5967,\n",
      "        6.1778], grad_fn=<AddBackward0>)\n",
      "tensor([0.9501, 1.5308, 2.1115, 2.6923, 3.2730, 3.8537, 4.4344, 5.0151, 5.5959,\n",
      "        6.1766], grad_fn=<AddBackward0>)\n",
      "tensor([0.9525, 1.5328, 2.1132, 2.6935, 3.2738, 3.8541, 4.4344, 5.0147, 5.5950,\n",
      "        6.1753], grad_fn=<AddBackward0>)\n",
      "tensor([0.9550, 1.5349, 2.1148, 2.6947, 3.2746, 3.8545, 4.4344, 5.0143, 5.5942,\n",
      "        6.1741], grad_fn=<AddBackward0>)\n",
      "tensor([0.9574, 1.5369, 2.1164, 2.6959, 3.2754, 3.8549, 4.4344, 5.0139, 5.5934,\n",
      "        6.1729], grad_fn=<AddBackward0>)\n",
      "tensor([0.9598, 1.5389, 2.1180, 2.6971, 3.2762, 3.8553, 4.4344, 5.0135, 5.5925,\n",
      "        6.1716], grad_fn=<AddBackward0>)\n",
      "tensor([0.9622, 1.5409, 2.1196, 2.6983, 3.2770, 3.8557, 4.4343, 5.0130, 5.5917,\n",
      "        6.1704], grad_fn=<AddBackward0>)\n",
      "tensor([0.9646, 1.5429, 2.1212, 2.6995, 3.2778, 3.8560, 4.4343, 5.0126, 5.5909,\n",
      "        6.1692], grad_fn=<AddBackward0>)\n",
      "tensor([0.9670, 1.5449, 2.1228, 2.7006, 3.2785, 3.8564, 4.4343, 5.0122, 5.5901,\n",
      "        6.1680], grad_fn=<AddBackward0>)\n",
      "tensor([0.9694, 1.5468, 2.1243, 2.7018, 3.2793, 3.8568, 4.4343, 5.0118, 5.5893,\n",
      "        6.1668], grad_fn=<AddBackward0>)\n",
      "tensor([0.9717, 1.5488, 2.1259, 2.7030, 3.2801, 3.8572, 4.4343, 5.0114, 5.5885,\n",
      "        6.1656], grad_fn=<AddBackward0>)\n",
      "tensor([0.9741, 1.5508, 2.1275, 2.7042, 3.2809, 3.8576, 4.4343, 5.0110, 5.5877,\n",
      "        6.1644], grad_fn=<AddBackward0>)\n",
      "tensor([0.9764, 1.5527, 2.1290, 2.7053, 3.2816, 3.8579, 4.4343, 5.0106, 5.5869,\n",
      "        6.1632], grad_fn=<AddBackward0>)\n",
      "tensor([0.9788, 1.5547, 2.1306, 2.7065, 3.2824, 3.8583, 4.4342, 5.0102, 5.5861,\n",
      "        6.1620], grad_fn=<AddBackward0>)\n",
      "tensor([0.9811, 1.5566, 2.1321, 2.7077, 3.2832, 3.8587, 4.4342, 5.0098, 5.5853,\n",
      "        6.1608], grad_fn=<AddBackward0>)\n",
      "tensor([0.9834, 1.5585, 2.1337, 2.7088, 3.2839, 3.8591, 4.4342, 5.0093, 5.5845,\n",
      "        6.1596], grad_fn=<AddBackward0>)\n",
      "tensor([0.9857, 1.5604, 2.1352, 2.7099, 3.2847, 3.8594, 4.4342, 5.0089, 5.5837,\n",
      "        6.1584], grad_fn=<AddBackward0>)\n",
      "tensor([0.9880, 1.5624, 2.1367, 2.7111, 3.2855, 3.8598, 4.4342, 5.0085, 5.5829,\n",
      "        6.1573], grad_fn=<AddBackward0>)\n",
      "tensor([0.9903, 1.5643, 2.1382, 2.7122, 3.2862, 3.8602, 4.4342, 5.0081, 5.5821,\n",
      "        6.1561], grad_fn=<AddBackward0>)\n",
      "tensor([0.9926, 1.5662, 2.1398, 2.7134, 3.2870, 3.8606, 4.4342, 5.0078, 5.5814,\n",
      "        6.1550], grad_fn=<AddBackward0>)\n",
      "Epoch :  100 cost :  0.12573865056037903\n",
      "tensor([0.9948, 1.5680, 2.1413, 2.7145, 3.2877, 3.8609, 4.4341, 5.0074, 5.5806,\n",
      "        6.1538], grad_fn=<AddBackward0>)\n",
      "tensor([0.9971, 1.5699, 2.1428, 2.7156, 3.2884, 3.8613, 4.4341, 5.0070, 5.5798,\n",
      "        6.1526], grad_fn=<AddBackward0>)\n",
      "tensor([0.9993, 1.5718, 2.1443, 2.7167, 3.2892, 3.8616, 4.4341, 5.0066, 5.5790,\n",
      "        6.1515], grad_fn=<AddBackward0>)\n",
      "tensor([1.0016, 1.5737, 2.1457, 2.7178, 3.2899, 3.8620, 4.4341, 5.0062, 5.5783,\n",
      "        6.1504], grad_fn=<AddBackward0>)\n",
      "tensor([1.0038, 1.5755, 2.1472, 2.7189, 3.2907, 3.8624, 4.4341, 5.0058, 5.5775,\n",
      "        6.1492], grad_fn=<AddBackward0>)\n",
      "tensor([1.0060, 1.5774, 2.1487, 2.7200, 3.2914, 3.8627, 4.4341, 5.0054, 5.5768,\n",
      "        6.1481], grad_fn=<AddBackward0>)\n",
      "tensor([1.0082, 1.5792, 2.1502, 2.7211, 3.2921, 3.8631, 4.4341, 5.0050, 5.5760,\n",
      "        6.1470], grad_fn=<AddBackward0>)\n",
      "tensor([1.0104, 1.5810, 2.1516, 2.7222, 3.2928, 3.8634, 4.4340, 5.0046, 5.5752,\n",
      "        6.1458], grad_fn=<AddBackward0>)\n",
      "tensor([1.0126, 1.5828, 2.1531, 2.7233, 3.2936, 3.8638, 4.4340, 5.0043, 5.5745,\n",
      "        6.1447], grad_fn=<AddBackward0>)\n",
      "tensor([1.0148, 1.5847, 2.1545, 2.7244, 3.2943, 3.8641, 4.4340, 5.0039, 5.5737,\n",
      "        6.1436], grad_fn=<AddBackward0>)\n",
      "tensor([1.0170, 1.5865, 2.1560, 2.7255, 3.2950, 3.8645, 4.4340, 5.0035, 5.5730,\n",
      "        6.1425], grad_fn=<AddBackward0>)\n",
      "tensor([1.0191, 1.5883, 2.1574, 2.7266, 3.2957, 3.8648, 4.4340, 5.0031, 5.5723,\n",
      "        6.1414], grad_fn=<AddBackward0>)\n",
      "tensor([1.0213, 1.5901, 2.1588, 2.7276, 3.2964, 3.8652, 4.4340, 5.0027, 5.5715,\n",
      "        6.1403], grad_fn=<AddBackward0>)\n",
      "tensor([1.0234, 1.5919, 2.1603, 2.7287, 3.2971, 3.8655, 4.4340, 5.0024, 5.5708,\n",
      "        6.1392], grad_fn=<AddBackward0>)\n",
      "tensor([1.0256, 1.5936, 2.1617, 2.7298, 3.2978, 3.8659, 4.4339, 5.0020, 5.5701,\n",
      "        6.1381], grad_fn=<AddBackward0>)\n",
      "tensor([1.0277, 1.5954, 2.1631, 2.7308, 3.2985, 3.8662, 4.4339, 5.0016, 5.5693,\n",
      "        6.1370], grad_fn=<AddBackward0>)\n",
      "tensor([1.0298, 1.5972, 2.1645, 2.7319, 3.2992, 3.8666, 4.4339, 5.0013, 5.5686,\n",
      "        6.1360], grad_fn=<AddBackward0>)\n",
      "tensor([1.0319, 1.5989, 2.1659, 2.7329, 3.2999, 3.8669, 4.4339, 5.0009, 5.5679,\n",
      "        6.1349], grad_fn=<AddBackward0>)\n",
      "tensor([1.0340, 1.6007, 2.1673, 2.7340, 3.3006, 3.8672, 4.4339, 5.0005, 5.5672,\n",
      "        6.1338], grad_fn=<AddBackward0>)\n",
      "tensor([1.0361, 1.6024, 2.1687, 2.7350, 3.3013, 3.8676, 4.4339, 5.0002, 5.5665,\n",
      "        6.1327], grad_fn=<AddBackward0>)\n",
      "tensor([1.0382, 1.6041, 2.1701, 2.7360, 3.3020, 3.8679, 4.4339, 4.9998, 5.5657,\n",
      "        6.1317], grad_fn=<AddBackward0>)\n",
      "tensor([1.0403, 1.6059, 2.1715, 2.7371, 3.3027, 3.8683, 4.4338, 4.9994, 5.5650,\n",
      "        6.1306], grad_fn=<AddBackward0>)\n",
      "tensor([1.0423, 1.6076, 2.1728, 2.7381, 3.3033, 3.8686, 4.4338, 4.9991, 5.5643,\n",
      "        6.1296], grad_fn=<AddBackward0>)\n",
      "tensor([1.0444, 1.6093, 2.1742, 2.7391, 3.3040, 3.8689, 4.4338, 4.9987, 5.5636,\n",
      "        6.1285], grad_fn=<AddBackward0>)\n",
      "tensor([1.0465, 1.6110, 2.1756, 2.7401, 3.3047, 3.8692, 4.4338, 4.9984, 5.5629,\n",
      "        6.1275], grad_fn=<AddBackward0>)\n",
      "tensor([1.0485, 1.6127, 2.1769, 2.7411, 3.3054, 3.8696, 4.4338, 4.9980, 5.5622,\n",
      "        6.1264], grad_fn=<AddBackward0>)\n",
      "tensor([1.0505, 1.6144, 2.1783, 2.7422, 3.3060, 3.8699, 4.4338, 4.9977, 5.5615,\n",
      "        6.1254], grad_fn=<AddBackward0>)\n",
      "tensor([1.0525, 1.6161, 2.1796, 2.7432, 3.3067, 3.8702, 4.4338, 4.9973, 5.5608,\n",
      "        6.1244], grad_fn=<AddBackward0>)\n",
      "tensor([1.0546, 1.6178, 2.1810, 2.7442, 3.3074, 3.8706, 4.4338, 4.9970, 5.5602,\n",
      "        6.1234], grad_fn=<AddBackward0>)\n",
      "tensor([1.0566, 1.6194, 2.1823, 2.7452, 3.3080, 3.8709, 4.4337, 4.9966, 5.5595,\n",
      "        6.1223], grad_fn=<AddBackward0>)\n",
      "tensor([1.0586, 1.6211, 2.1836, 2.7461, 3.3087, 3.8712, 4.4337, 4.9963, 5.5588,\n",
      "        6.1213], grad_fn=<AddBackward0>)\n",
      "tensor([1.0606, 1.6227, 2.1849, 2.7471, 3.3093, 3.8715, 4.4337, 4.9959, 5.5581,\n",
      "        6.1203], grad_fn=<AddBackward0>)\n",
      "tensor([1.0625, 1.6244, 2.1863, 2.7481, 3.3100, 3.8718, 4.4337, 4.9956, 5.5574,\n",
      "        6.1193], grad_fn=<AddBackward0>)\n",
      "tensor([1.0645, 1.6260, 2.1876, 2.7491, 3.3106, 3.8722, 4.4337, 4.9952, 5.5568,\n",
      "        6.1183], grad_fn=<AddBackward0>)\n",
      "tensor([1.0665, 1.6277, 2.1889, 2.7501, 3.3113, 3.8725, 4.4337, 4.9949, 5.5561,\n",
      "        6.1173], grad_fn=<AddBackward0>)\n",
      "tensor([1.0684, 1.6293, 2.1902, 2.7510, 3.3119, 3.8728, 4.4337, 4.9945, 5.5554,\n",
      "        6.1163], grad_fn=<AddBackward0>)\n",
      "tensor([1.0704, 1.6309, 2.1915, 2.7520, 3.3126, 3.8731, 4.4337, 4.9942, 5.5547,\n",
      "        6.1153], grad_fn=<AddBackward0>)\n",
      "tensor([1.0723, 1.6325, 2.1928, 2.7530, 3.3132, 3.8734, 4.4336, 4.9939, 5.5541,\n",
      "        6.1143], grad_fn=<AddBackward0>)\n",
      "tensor([1.0742, 1.6341, 2.1940, 2.7539, 3.3138, 3.8737, 4.4336, 4.9935, 5.5534,\n",
      "        6.1133], grad_fn=<AddBackward0>)\n",
      "tensor([1.0762, 1.6357, 2.1953, 2.7549, 3.3145, 3.8740, 4.4336, 4.9932, 5.5528,\n",
      "        6.1123], grad_fn=<AddBackward0>)\n",
      "tensor([1.0781, 1.6373, 2.1966, 2.7558, 3.3151, 3.8744, 4.4336, 4.9929, 5.5521,\n",
      "        6.1114], grad_fn=<AddBackward0>)\n",
      "tensor([1.0800, 1.6389, 2.1979, 2.7568, 3.3157, 3.8747, 4.4336, 4.9925, 5.5515,\n",
      "        6.1104], grad_fn=<AddBackward0>)\n",
      "tensor([1.0819, 1.6405, 2.1991, 2.7577, 3.3164, 3.8750, 4.4336, 4.9922, 5.5508,\n",
      "        6.1094], grad_fn=<AddBackward0>)\n",
      "tensor([1.0838, 1.6421, 2.2004, 2.7587, 3.3170, 3.8753, 4.4336, 4.9919, 5.5502,\n",
      "        6.1085], grad_fn=<AddBackward0>)\n",
      "tensor([1.0857, 1.6436, 2.2016, 2.7596, 3.3176, 3.8756, 4.4336, 4.9915, 5.5495,\n",
      "        6.1075], grad_fn=<AddBackward0>)\n",
      "tensor([1.0875, 1.6452, 2.2029, 2.7605, 3.3182, 3.8759, 4.4335, 4.9912, 5.5489,\n",
      "        6.1065], grad_fn=<AddBackward0>)\n",
      "tensor([1.0894, 1.6468, 2.2041, 2.7615, 3.3188, 3.8762, 4.4335, 4.9909, 5.5482,\n",
      "        6.1056], grad_fn=<AddBackward0>)\n",
      "tensor([1.0913, 1.6483, 2.2054, 2.7624, 3.3194, 3.8765, 4.4335, 4.9906, 5.5476,\n",
      "        6.1046], grad_fn=<AddBackward0>)\n",
      "tensor([1.0931, 1.6499, 2.2066, 2.7633, 3.3200, 3.8768, 4.4335, 4.9902, 5.5470,\n",
      "        6.1037], grad_fn=<AddBackward0>)\n",
      "tensor([1.0950, 1.6514, 2.2078, 2.7642, 3.3207, 3.8771, 4.4335, 4.9899, 5.5463,\n",
      "        6.1028], grad_fn=<AddBackward0>)\n",
      "tensor([1.0968, 1.6529, 2.2090, 2.7651, 3.3213, 3.8774, 4.4335, 4.9896, 5.5457,\n",
      "        6.1018], grad_fn=<AddBackward0>)\n",
      "tensor([1.0986, 1.6544, 2.2102, 2.7661, 3.3219, 3.8777, 4.4335, 4.9893, 5.5451,\n",
      "        6.1009], grad_fn=<AddBackward0>)\n",
      "tensor([1.1005, 1.6560, 2.2115, 2.7670, 3.3225, 3.8780, 4.4335, 4.9890, 5.5445,\n",
      "        6.1000], grad_fn=<AddBackward0>)\n",
      "tensor([1.1023, 1.6575, 2.2127, 2.7679, 3.3231, 3.8783, 4.4334, 4.9886, 5.5438,\n",
      "        6.0990], grad_fn=<AddBackward0>)\n",
      "tensor([1.1041, 1.6590, 2.2139, 2.7688, 3.3236, 3.8785, 4.4334, 4.9883, 5.5432,\n",
      "        6.0981], grad_fn=<AddBackward0>)\n",
      "tensor([1.1059, 1.6605, 2.2151, 2.7696, 3.3242, 3.8788, 4.4334, 4.9880, 5.5426,\n",
      "        6.0972], grad_fn=<AddBackward0>)\n",
      "tensor([1.1077, 1.6620, 2.2162, 2.7705, 3.3248, 3.8791, 4.4334, 4.9877, 5.5420,\n",
      "        6.0963], grad_fn=<AddBackward0>)\n",
      "tensor([1.1094, 1.6634, 2.2174, 2.7714, 3.3254, 3.8794, 4.4334, 4.9874, 5.5414,\n",
      "        6.0954], grad_fn=<AddBackward0>)\n",
      "tensor([1.1112, 1.6649, 2.2186, 2.7723, 3.3260, 3.8797, 4.4334, 4.9871, 5.5408,\n",
      "        6.0945], grad_fn=<AddBackward0>)\n",
      "tensor([1.1130, 1.6664, 2.2198, 2.7732, 3.3266, 3.8800, 4.4334, 4.9868, 5.5402,\n",
      "        6.0936], grad_fn=<AddBackward0>)\n",
      "tensor([1.1147, 1.6679, 2.2210, 2.7741, 3.3272, 3.8803, 4.4334, 4.9865, 5.5396,\n",
      "        6.0927], grad_fn=<AddBackward0>)\n",
      "tensor([1.1165, 1.6693, 2.2221, 2.7749, 3.3277, 3.8805, 4.4334, 4.9862, 5.5390,\n",
      "        6.0918], grad_fn=<AddBackward0>)\n",
      "tensor([1.1182, 1.6708, 2.2233, 2.7758, 3.3283, 3.8808, 4.4333, 4.9859, 5.5384,\n",
      "        6.0909], grad_fn=<AddBackward0>)\n",
      "tensor([1.1200, 1.6722, 2.2244, 2.7767, 3.3289, 3.8811, 4.4333, 4.9856, 5.5378,\n",
      "        6.0900], grad_fn=<AddBackward0>)\n",
      "tensor([1.1217, 1.6737, 2.2256, 2.7775, 3.3295, 3.8814, 4.4333, 4.9853, 5.5372,\n",
      "        6.0891], grad_fn=<AddBackward0>)\n",
      "tensor([1.1234, 1.6751, 2.2267, 2.7784, 3.3300, 3.8817, 4.4333, 4.9850, 5.5366,\n",
      "        6.0883], grad_fn=<AddBackward0>)\n",
      "tensor([1.1252, 1.6765, 2.2279, 2.7792, 3.3306, 3.8819, 4.4333, 4.9847, 5.5360,\n",
      "        6.0874], grad_fn=<AddBackward0>)\n",
      "tensor([1.1269, 1.6779, 2.2290, 2.7801, 3.3312, 3.8822, 4.4333, 4.9844, 5.5354,\n",
      "        6.0865], grad_fn=<AddBackward0>)\n",
      "tensor([1.1286, 1.6794, 2.2301, 2.7809, 3.3317, 3.8825, 4.4333, 4.9841, 5.5349,\n",
      "        6.0856], grad_fn=<AddBackward0>)\n",
      "tensor([1.1303, 1.6808, 2.2313, 2.7818, 3.3323, 3.8828, 4.4333, 4.9838, 5.5343,\n",
      "        6.0848], grad_fn=<AddBackward0>)\n",
      "tensor([1.1320, 1.6822, 2.2324, 2.7826, 3.3328, 3.8830, 4.4333, 4.9835, 5.5337,\n",
      "        6.0839], grad_fn=<AddBackward0>)\n",
      "tensor([1.1336, 1.6836, 2.2335, 2.7834, 3.3334, 3.8833, 4.4332, 4.9832, 5.5331,\n",
      "        6.0831], grad_fn=<AddBackward0>)\n",
      "tensor([1.1353, 1.6850, 2.2346, 2.7843, 3.3339, 3.8836, 4.4332, 4.9829, 5.5325,\n",
      "        6.0822], grad_fn=<AddBackward0>)\n",
      "tensor([1.1370, 1.6863, 2.2357, 2.7851, 3.3345, 3.8839, 4.4332, 4.9826, 5.5320,\n",
      "        6.0814], grad_fn=<AddBackward0>)\n",
      "tensor([1.1386, 1.6877, 2.2368, 2.7859, 3.3350, 3.8841, 4.4332, 4.9823, 5.5314,\n",
      "        6.0805], grad_fn=<AddBackward0>)\n",
      "tensor([1.1403, 1.6891, 2.2379, 2.7867, 3.3356, 3.8844, 4.4332, 4.9820, 5.5308,\n",
      "        6.0797], grad_fn=<AddBackward0>)\n",
      "tensor([1.1419, 1.6905, 2.2390, 2.7876, 3.3361, 3.8847, 4.4332, 4.9817, 5.5303,\n",
      "        6.0788], grad_fn=<AddBackward0>)\n",
      "tensor([1.1436, 1.6918, 2.2401, 2.7884, 3.3366, 3.8849, 4.4332, 4.9815, 5.5297,\n",
      "        6.0780], grad_fn=<AddBackward0>)\n",
      "tensor([1.1452, 1.6932, 2.2412, 2.7892, 3.3372, 3.8852, 4.4332, 4.9812, 5.5292,\n",
      "        6.0772], grad_fn=<AddBackward0>)\n",
      "tensor([1.1468, 1.6946, 2.2423, 2.7900, 3.3377, 3.8854, 4.4332, 4.9809, 5.5286,\n",
      "        6.0763], grad_fn=<AddBackward0>)\n",
      "tensor([1.1484, 1.6959, 2.2434, 2.7908, 3.3383, 3.8857, 4.4332, 4.9806, 5.5281,\n",
      "        6.0755], grad_fn=<AddBackward0>)\n",
      "tensor([1.1501, 1.6972, 2.2444, 2.7916, 3.3388, 3.8860, 4.4331, 4.9803, 5.5275,\n",
      "        6.0747], grad_fn=<AddBackward0>)\n",
      "tensor([1.1517, 1.6986, 2.2455, 2.7924, 3.3393, 3.8862, 4.4331, 4.9800, 5.5270,\n",
      "        6.0739], grad_fn=<AddBackward0>)\n",
      "tensor([1.1533, 1.6999, 2.2466, 2.7932, 3.3398, 3.8865, 4.4331, 4.9798, 5.5264,\n",
      "        6.0731], grad_fn=<AddBackward0>)\n",
      "tensor([1.1549, 1.7012, 2.2476, 2.7940, 3.3404, 3.8867, 4.4331, 4.9795, 5.5259,\n",
      "        6.0722], grad_fn=<AddBackward0>)\n",
      "tensor([1.1564, 1.7026, 2.2487, 2.7948, 3.3409, 3.8870, 4.4331, 4.9792, 5.5253,\n",
      "        6.0714], grad_fn=<AddBackward0>)\n",
      "tensor([1.1580, 1.7039, 2.2497, 2.7956, 3.3414, 3.8872, 4.4331, 4.9789, 5.5248,\n",
      "        6.0706], grad_fn=<AddBackward0>)\n",
      "tensor([1.1596, 1.7052, 2.2508, 2.7963, 3.3419, 3.8875, 4.4331, 4.9787, 5.5242,\n",
      "        6.0698], grad_fn=<AddBackward0>)\n",
      "tensor([1.1612, 1.7065, 2.2518, 2.7971, 3.3424, 3.8878, 4.4331, 4.9784, 5.5237,\n",
      "        6.0690], grad_fn=<AddBackward0>)\n",
      "tensor([1.1627, 1.7078, 2.2528, 2.7979, 3.3429, 3.8880, 4.4331, 4.9781, 5.5232,\n",
      "        6.0682], grad_fn=<AddBackward0>)\n",
      "tensor([1.1643, 1.7091, 2.2539, 2.7987, 3.3435, 3.8883, 4.4331, 4.9779, 5.5226,\n",
      "        6.0674], grad_fn=<AddBackward0>)\n",
      "tensor([1.1658, 1.7103, 2.2549, 2.7994, 3.3440, 3.8885, 4.4330, 4.9776, 5.5221,\n",
      "        6.0667], grad_fn=<AddBackward0>)\n",
      "tensor([1.1673, 1.7116, 2.2559, 2.8002, 3.3445, 3.8888, 4.4330, 4.9773, 5.5216,\n",
      "        6.0659], grad_fn=<AddBackward0>)\n",
      "tensor([1.1689, 1.7129, 2.2569, 2.8010, 3.3450, 3.8890, 4.4330, 4.9770, 5.5211,\n",
      "        6.0651], grad_fn=<AddBackward0>)\n",
      "tensor([1.1704, 1.7142, 2.2579, 2.8017, 3.3455, 3.8892, 4.4330, 4.9768, 5.5205,\n",
      "        6.0643], grad_fn=<AddBackward0>)\n",
      "tensor([1.1719, 1.7154, 2.2590, 2.8025, 3.3460, 3.8895, 4.4330, 4.9765, 5.5200,\n",
      "        6.0635], grad_fn=<AddBackward0>)\n",
      "tensor([1.1734, 1.7167, 2.2600, 2.8032, 3.3465, 3.8897, 4.4330, 4.9763, 5.5195,\n",
      "        6.0628], grad_fn=<AddBackward0>)\n",
      "tensor([1.1749, 1.7180, 2.2610, 2.8040, 3.3470, 3.8900, 4.4330, 4.9760, 5.5190,\n",
      "        6.0620], grad_fn=<AddBackward0>)\n",
      "tensor([1.1764, 1.7192, 2.2620, 2.8047, 3.3475, 3.8902, 4.4330, 4.9757, 5.5185,\n",
      "        6.0612], grad_fn=<AddBackward0>)\n",
      "tensor([1.1779, 1.7204, 2.2629, 2.8055, 3.3480, 3.8905, 4.4330, 4.9755, 5.5180,\n",
      "        6.0605], grad_fn=<AddBackward0>)\n",
      "Epoch :  200 cost :  0.07730412483215332\n",
      "tensor([1.1794, 1.7217, 2.2639, 2.8062, 3.3484, 3.8907, 4.4330, 4.9752, 5.5175,\n",
      "        6.0597], grad_fn=<AddBackward0>)\n",
      "tensor([1.1809, 1.7229, 2.2649, 2.8069, 3.3489, 3.8909, 4.4329, 4.9750, 5.5170,\n",
      "        6.0590], grad_fn=<AddBackward0>)\n",
      "tensor([1.1824, 1.7241, 2.2659, 2.8077, 3.3494, 3.8912, 4.4329, 4.9747, 5.5165,\n",
      "        6.0582], grad_fn=<AddBackward0>)\n",
      "tensor([1.1839, 1.7254, 2.2669, 2.8084, 3.3499, 3.8914, 4.4329, 4.9744, 5.5160,\n",
      "        6.0575], grad_fn=<AddBackward0>)\n",
      "tensor([1.1853, 1.7266, 2.2679, 2.8091, 3.3504, 3.8917, 4.4329, 4.9742, 5.5155,\n",
      "        6.0567], grad_fn=<AddBackward0>)\n",
      "tensor([1.1868, 1.7278, 2.2688, 2.8098, 3.3509, 3.8919, 4.4329, 4.9739, 5.5150,\n",
      "        6.0560], grad_fn=<AddBackward0>)\n",
      "tensor([1.1882, 1.7290, 2.2698, 2.8106, 3.3513, 3.8921, 4.4329, 4.9737, 5.5145,\n",
      "        6.0552], grad_fn=<AddBackward0>)\n",
      "tensor([1.1897, 1.7302, 2.2707, 2.8113, 3.3518, 3.8924, 4.4329, 4.9734, 5.5140,\n",
      "        6.0545], grad_fn=<AddBackward0>)\n",
      "tensor([1.1911, 1.7314, 2.2717, 2.8120, 3.3523, 3.8926, 4.4329, 4.9732, 5.5135,\n",
      "        6.0538], grad_fn=<AddBackward0>)\n",
      "tensor([1.1925, 1.7326, 2.2727, 2.8127, 3.3528, 3.8928, 4.4329, 4.9729, 5.5130,\n",
      "        6.0530], grad_fn=<AddBackward0>)\n",
      "tensor([1.1940, 1.7338, 2.2736, 2.8134, 3.3532, 3.8930, 4.4329, 4.9727, 5.5125,\n",
      "        6.0523], grad_fn=<AddBackward0>)\n",
      "tensor([1.1954, 1.7350, 2.2745, 2.8141, 3.3537, 3.8933, 4.4329, 4.9724, 5.5120,\n",
      "        6.0516], grad_fn=<AddBackward0>)\n",
      "tensor([1.1968, 1.7361, 2.2755, 2.8148, 3.3542, 3.8935, 4.4328, 4.9722, 5.5115,\n",
      "        6.0509], grad_fn=<AddBackward0>)\n",
      "tensor([1.1982, 1.7373, 2.2764, 2.8155, 3.3546, 3.8937, 4.4328, 4.9719, 5.5110,\n",
      "        6.0501], grad_fn=<AddBackward0>)\n",
      "tensor([1.1996, 1.7385, 2.2774, 2.8162, 3.3551, 3.8940, 4.4328, 4.9717, 5.5106,\n",
      "        6.0494], grad_fn=<AddBackward0>)\n",
      "tensor([1.2010, 1.7396, 2.2783, 2.8169, 3.3555, 3.8942, 4.4328, 4.9715, 5.5101,\n",
      "        6.0487], grad_fn=<AddBackward0>)\n",
      "tensor([1.2024, 1.7408, 2.2792, 2.8176, 3.3560, 3.8944, 4.4328, 4.9712, 5.5096,\n",
      "        6.0480], grad_fn=<AddBackward0>)\n",
      "tensor([1.2038, 1.7420, 2.2801, 2.8183, 3.3565, 3.8946, 4.4328, 4.9710, 5.5091,\n",
      "        6.0473], grad_fn=<AddBackward0>)\n",
      "tensor([1.2052, 1.7431, 2.2810, 2.8190, 3.3569, 3.8949, 4.4328, 4.9707, 5.5087,\n",
      "        6.0466], grad_fn=<AddBackward0>)\n",
      "tensor([1.2065, 1.7442, 2.2820, 2.8197, 3.3574, 3.8951, 4.4328, 4.9705, 5.5082,\n",
      "        6.0459], grad_fn=<AddBackward0>)\n",
      "tensor([1.2079, 1.7454, 2.2829, 2.8203, 3.3578, 3.8953, 4.4328, 4.9703, 5.5077,\n",
      "        6.0452], grad_fn=<AddBackward0>)\n",
      "tensor([1.2093, 1.7465, 2.2838, 2.8210, 3.3583, 3.8955, 4.4328, 4.9700, 5.5073,\n",
      "        6.0445], grad_fn=<AddBackward0>)\n",
      "tensor([1.2106, 1.7477, 2.2847, 2.8217, 3.3587, 3.8957, 4.4328, 4.9698, 5.5068,\n",
      "        6.0438], grad_fn=<AddBackward0>)\n",
      "tensor([1.2120, 1.7488, 2.2856, 2.8224, 3.3592, 3.8960, 4.4327, 4.9695, 5.5063,\n",
      "        6.0431], grad_fn=<AddBackward0>)\n",
      "tensor([1.2133, 1.7499, 2.2865, 2.8230, 3.3596, 3.8962, 4.4327, 4.9693, 5.5059,\n",
      "        6.0424], grad_fn=<AddBackward0>)\n",
      "tensor([1.2147, 1.7510, 2.2874, 2.8237, 3.3600, 3.8964, 4.4327, 4.9691, 5.5054,\n",
      "        6.0418], grad_fn=<AddBackward0>)\n",
      "tensor([1.2160, 1.7521, 2.2882, 2.8244, 3.3605, 3.8966, 4.4327, 4.9688, 5.5050,\n",
      "        6.0411], grad_fn=<AddBackward0>)\n",
      "tensor([1.2173, 1.7532, 2.2891, 2.8250, 3.3609, 3.8968, 4.4327, 4.9686, 5.5045,\n",
      "        6.0404], grad_fn=<AddBackward0>)\n",
      "tensor([1.2186, 1.7543, 2.2900, 2.8257, 3.3614, 3.8970, 4.4327, 4.9684, 5.5041,\n",
      "        6.0397], grad_fn=<AddBackward0>)\n",
      "tensor([1.2200, 1.7554, 2.2909, 2.8263, 3.3618, 3.8972, 4.4327, 4.9682, 5.5036,\n",
      "        6.0391], grad_fn=<AddBackward0>)\n",
      "tensor([1.2213, 1.7565, 2.2917, 2.8270, 3.3622, 3.8975, 4.4327, 4.9679, 5.5032,\n",
      "        6.0384], grad_fn=<AddBackward0>)\n",
      "tensor([1.2226, 1.7576, 2.2926, 2.8276, 3.3626, 3.8977, 4.4327, 4.9677, 5.5027,\n",
      "        6.0377], grad_fn=<AddBackward0>)\n",
      "tensor([1.2239, 1.7587, 2.2935, 2.8283, 3.3631, 3.8979, 4.4327, 4.9675, 5.5023,\n",
      "        6.0371], grad_fn=<AddBackward0>)\n",
      "tensor([1.2252, 1.7598, 2.2943, 2.8289, 3.3635, 3.8981, 4.4327, 4.9672, 5.5018,\n",
      "        6.0364], grad_fn=<AddBackward0>)\n",
      "tensor([1.2265, 1.7608, 2.2952, 2.8296, 3.3639, 3.8983, 4.4327, 4.9670, 5.5014,\n",
      "        6.0357], grad_fn=<AddBackward0>)\n",
      "tensor([1.2278, 1.7619, 2.2961, 2.8302, 3.3643, 3.8985, 4.4326, 4.9668, 5.5009,\n",
      "        6.0351], grad_fn=<AddBackward0>)\n",
      "tensor([1.2290, 1.7630, 2.2969, 2.8308, 3.3648, 3.8987, 4.4326, 4.9666, 5.5005,\n",
      "        6.0344], grad_fn=<AddBackward0>)\n",
      "tensor([1.2303, 1.7640, 2.2977, 2.8315, 3.3652, 3.8989, 4.4326, 4.9664, 5.5001,\n",
      "        6.0338], grad_fn=<AddBackward0>)\n",
      "tensor([1.2316, 1.7651, 2.2986, 2.8321, 3.3656, 3.8991, 4.4326, 4.9661, 5.4996,\n",
      "        6.0331], grad_fn=<AddBackward0>)\n",
      "tensor([1.2328, 1.7661, 2.2994, 2.8327, 3.3660, 3.8993, 4.4326, 4.9659, 5.4992,\n",
      "        6.0325], grad_fn=<AddBackward0>)\n",
      "tensor([1.2341, 1.7672, 2.3003, 2.8333, 3.3664, 3.8995, 4.4326, 4.9657, 5.4988,\n",
      "        6.0319], grad_fn=<AddBackward0>)\n",
      "tensor([1.2353, 1.7682, 2.3011, 2.8340, 3.3668, 3.8997, 4.4326, 4.9655, 5.4983,\n",
      "        6.0312], grad_fn=<AddBackward0>)\n",
      "tensor([1.2366, 1.7693, 2.3019, 2.8346, 3.3673, 3.8999, 4.4326, 4.9653, 5.4979,\n",
      "        6.0306], grad_fn=<AddBackward0>)\n",
      "tensor([1.2378, 1.7703, 2.3027, 2.8352, 3.3677, 3.9001, 4.4326, 4.9650, 5.4975,\n",
      "        6.0300], grad_fn=<AddBackward0>)\n",
      "tensor([1.2391, 1.7713, 2.3036, 2.8358, 3.3681, 3.9003, 4.4326, 4.9648, 5.4971,\n",
      "        6.0293], grad_fn=<AddBackward0>)\n",
      "tensor([1.2403, 1.7723, 2.3044, 2.8364, 3.3685, 3.9005, 4.4326, 4.9646, 5.4967,\n",
      "        6.0287], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2415, 1.7734, 2.3052, 2.8370, 3.3689, 3.9007, 4.4326, 4.9644, 5.4962,\n",
      "        6.0281], grad_fn=<AddBackward0>)\n",
      "tensor([1.2427, 1.7744, 2.3060, 2.8376, 3.3693, 3.9009, 4.4325, 4.9642, 5.4958,\n",
      "        6.0275], grad_fn=<AddBackward0>)\n",
      "tensor([1.2440, 1.7754, 2.3068, 2.8383, 3.3697, 3.9011, 4.4325, 4.9640, 5.4954,\n",
      "        6.0268], grad_fn=<AddBackward0>)\n",
      "tensor([1.2452, 1.7764, 2.3076, 2.8389, 3.3701, 3.9013, 4.4325, 4.9638, 5.4950,\n",
      "        6.0262], grad_fn=<AddBackward0>)\n",
      "tensor([1.2464, 1.7774, 2.3084, 2.8395, 3.3705, 3.9015, 4.4325, 4.9636, 5.4946,\n",
      "        6.0256], grad_fn=<AddBackward0>)\n",
      "tensor([1.2476, 1.7784, 2.3092, 2.8400, 3.3709, 3.9017, 4.4325, 4.9633, 5.4942,\n",
      "        6.0250], grad_fn=<AddBackward0>)\n",
      "tensor([1.2488, 1.7794, 2.3100, 2.8406, 3.3713, 3.9019, 4.4325, 4.9631, 5.4938,\n",
      "        6.0244], grad_fn=<AddBackward0>)\n",
      "tensor([1.2500, 1.7804, 2.3108, 2.8412, 3.3717, 3.9021, 4.4325, 4.9629, 5.4933,\n",
      "        6.0238], grad_fn=<AddBackward0>)\n",
      "tensor([1.2512, 1.7814, 2.3116, 2.8418, 3.3720, 3.9023, 4.4325, 4.9627, 5.4929,\n",
      "        6.0232], grad_fn=<AddBackward0>)\n",
      "tensor([1.2523, 1.7824, 2.3124, 2.8424, 3.3724, 3.9025, 4.4325, 4.9625, 5.4925,\n",
      "        6.0226], grad_fn=<AddBackward0>)\n",
      "tensor([1.2535, 1.7833, 2.3132, 2.8430, 3.3728, 3.9027, 4.4325, 4.9623, 5.4921,\n",
      "        6.0220], grad_fn=<AddBackward0>)\n",
      "tensor([1.2547, 1.7843, 2.3139, 2.8436, 3.3732, 3.9028, 4.4325, 4.9621, 5.4917,\n",
      "        6.0214], grad_fn=<AddBackward0>)\n",
      "tensor([1.2558, 1.7853, 2.3147, 2.8442, 3.3736, 3.9030, 4.4325, 4.9619, 5.4913,\n",
      "        6.0208], grad_fn=<AddBackward0>)\n",
      "tensor([1.2570, 1.7862, 2.3155, 2.8447, 3.3740, 3.9032, 4.4325, 4.9617, 5.4909,\n",
      "        6.0202], grad_fn=<AddBackward0>)\n",
      "tensor([1.2582, 1.7872, 2.3163, 2.8453, 3.3744, 3.9034, 4.4325, 4.9615, 5.4905,\n",
      "        6.0196], grad_fn=<AddBackward0>)\n",
      "tensor([1.2593, 1.7882, 2.3170, 2.8459, 3.3747, 3.9036, 4.4324, 4.9613, 5.4902,\n",
      "        6.0190], grad_fn=<AddBackward0>)\n",
      "tensor([1.2605, 1.7891, 2.3178, 2.8464, 3.3751, 3.9038, 4.4324, 4.9611, 5.4898,\n",
      "        6.0184], grad_fn=<AddBackward0>)\n",
      "tensor([1.2616, 1.7901, 2.3185, 2.8470, 3.3755, 3.9040, 4.4324, 4.9609, 5.4894,\n",
      "        6.0178], grad_fn=<AddBackward0>)\n",
      "tensor([1.2627, 1.7910, 2.3193, 2.8476, 3.3759, 3.9041, 4.4324, 4.9607, 5.4890,\n",
      "        6.0173], grad_fn=<AddBackward0>)\n",
      "tensor([1.2639, 1.7920, 2.3201, 2.8481, 3.3762, 3.9043, 4.4324, 4.9605, 5.4886,\n",
      "        6.0167], grad_fn=<AddBackward0>)\n",
      "tensor([1.2650, 1.7929, 2.3208, 2.8487, 3.3766, 3.9045, 4.4324, 4.9603, 5.4882,\n",
      "        6.0161], grad_fn=<AddBackward0>)\n",
      "tensor([1.2661, 1.7938, 2.3215, 2.8493, 3.3770, 3.9047, 4.4324, 4.9601, 5.4878,\n",
      "        6.0155], grad_fn=<AddBackward0>)\n",
      "tensor([1.2672, 1.7948, 2.3223, 2.8498, 3.3773, 3.9049, 4.4324, 4.9599, 5.4874,\n",
      "        6.0150], grad_fn=<AddBackward0>)\n",
      "tensor([1.2683, 1.7957, 2.3230, 2.8504, 3.3777, 3.9050, 4.4324, 4.9597, 5.4871,\n",
      "        6.0144], grad_fn=<AddBackward0>)\n",
      "tensor([1.2695, 1.7966, 2.3238, 2.8509, 3.3781, 3.9052, 4.4324, 4.9595, 5.4867,\n",
      "        6.0138], grad_fn=<AddBackward0>)\n",
      "tensor([1.2706, 1.7975, 2.3245, 2.8515, 3.3784, 3.9054, 4.4324, 4.9593, 5.4863,\n",
      "        6.0133], grad_fn=<AddBackward0>)\n",
      "tensor([1.2717, 1.7984, 2.3252, 2.8520, 3.3788, 3.9056, 4.4324, 4.9591, 5.4859,\n",
      "        6.0127], grad_fn=<AddBackward0>)\n",
      "tensor([1.2728, 1.7994, 2.3260, 2.8526, 3.3792, 3.9058, 4.4324, 4.9590, 5.4856,\n",
      "        6.0122], grad_fn=<AddBackward0>)\n",
      "tensor([1.2738, 1.8003, 2.3267, 2.8531, 3.3795, 3.9059, 4.4324, 4.9588, 5.4852,\n",
      "        6.0116], grad_fn=<AddBackward0>)\n",
      "tensor([1.2749, 1.8012, 2.3274, 2.8536, 3.3799, 3.9061, 4.4323, 4.9586, 5.4848,\n",
      "        6.0111], grad_fn=<AddBackward0>)\n",
      "tensor([1.2760, 1.8021, 2.3281, 2.8542, 3.3802, 3.9063, 4.4323, 4.9584, 5.4844,\n",
      "        6.0105], grad_fn=<AddBackward0>)\n",
      "tensor([1.2771, 1.8030, 2.3288, 2.8547, 3.3806, 3.9065, 4.4323, 4.9582, 5.4841,\n",
      "        6.0100], grad_fn=<AddBackward0>)\n",
      "tensor([1.2782, 1.8039, 2.3295, 2.8552, 3.3809, 3.9066, 4.4323, 4.9580, 5.4837,\n",
      "        6.0094], grad_fn=<AddBackward0>)\n",
      "tensor([1.2792, 1.8047, 2.3303, 2.8558, 3.3813, 3.9068, 4.4323, 4.9578, 5.4833,\n",
      "        6.0089], grad_fn=<AddBackward0>)\n",
      "tensor([1.2803, 1.8056, 2.3310, 2.8563, 3.3816, 3.9070, 4.4323, 4.9576, 5.4830,\n",
      "        6.0083], grad_fn=<AddBackward0>)\n",
      "tensor([1.2813, 1.8065, 2.3317, 2.8568, 3.3820, 3.9071, 4.4323, 4.9575, 5.4826,\n",
      "        6.0078], grad_fn=<AddBackward0>)\n",
      "tensor([1.2824, 1.8074, 2.3324, 2.8573, 3.3823, 3.9073, 4.4323, 4.9573, 5.4823,\n",
      "        6.0072], grad_fn=<AddBackward0>)\n",
      "tensor([1.2834, 1.8083, 2.3331, 2.8579, 3.3827, 3.9075, 4.4323, 4.9571, 5.4819,\n",
      "        6.0067], grad_fn=<AddBackward0>)\n",
      "tensor([1.2845, 1.8091, 2.3338, 2.8584, 3.3830, 3.9077, 4.4323, 4.9569, 5.4815,\n",
      "        6.0062], grad_fn=<AddBackward0>)\n",
      "tensor([1.2855, 1.8100, 2.3344, 2.8589, 3.3834, 3.9078, 4.4323, 4.9567, 5.4812,\n",
      "        6.0056], grad_fn=<AddBackward0>)\n",
      "tensor([1.2866, 1.8109, 2.3351, 2.8594, 3.3837, 3.9080, 4.4323, 4.9566, 5.4808,\n",
      "        6.0051], grad_fn=<AddBackward0>)\n",
      "tensor([1.2876, 1.8117, 2.3358, 2.8599, 3.3840, 3.9082, 4.4323, 4.9564, 5.4805,\n",
      "        6.0046], grad_fn=<AddBackward0>)\n",
      "tensor([1.2886, 1.8126, 2.3365, 2.8604, 3.3844, 3.9083, 4.4323, 4.9562, 5.4801,\n",
      "        6.0041], grad_fn=<AddBackward0>)\n",
      "tensor([1.2897, 1.8134, 2.3372, 2.8610, 3.3847, 3.9085, 4.4322, 4.9560, 5.4798,\n",
      "        6.0035], grad_fn=<AddBackward0>)\n",
      "tensor([1.2907, 1.8143, 2.3379, 2.8615, 3.3851, 3.9086, 4.4322, 4.9558, 5.4794,\n",
      "        6.0030], grad_fn=<AddBackward0>)\n",
      "tensor([1.2917, 1.8151, 2.3385, 2.8620, 3.3854, 3.9088, 4.4322, 4.9557, 5.4791,\n",
      "        6.0025], grad_fn=<AddBackward0>)\n",
      "tensor([1.2927, 1.8160, 2.3392, 2.8625, 3.3857, 3.9090, 4.4322, 4.9555, 5.4787,\n",
      "        6.0020], grad_fn=<AddBackward0>)\n",
      "tensor([1.2937, 1.8168, 2.3399, 2.8630, 3.3860, 3.9091, 4.4322, 4.9553, 5.4784,\n",
      "        6.0015], grad_fn=<AddBackward0>)\n",
      "tensor([1.2947, 1.8176, 2.3405, 2.8635, 3.3864, 3.9093, 4.4322, 4.9551, 5.4781,\n",
      "        6.0010], grad_fn=<AddBackward0>)\n",
      "tensor([1.2957, 1.8185, 2.3412, 2.8640, 3.3867, 3.9095, 4.4322, 4.9550, 5.4777,\n",
      "        6.0005], grad_fn=<AddBackward0>)\n",
      "tensor([1.2967, 1.8193, 2.3419, 2.8644, 3.3870, 3.9096, 4.4322, 4.9548, 5.4774,\n",
      "        6.0000], grad_fn=<AddBackward0>)\n",
      "tensor([1.2977, 1.8201, 2.3425, 2.8649, 3.3874, 3.9098, 4.4322, 4.9546, 5.4770,\n",
      "        5.9995], grad_fn=<AddBackward0>)\n",
      "tensor([1.2987, 1.8209, 2.3432, 2.8654, 3.3877, 3.9099, 4.4322, 4.9544, 5.4767,\n",
      "        5.9990], grad_fn=<AddBackward0>)\n",
      "tensor([1.2996, 1.8217, 2.3438, 2.8659, 3.3880, 3.9101, 4.4322, 4.9543, 5.4764,\n",
      "        5.9985], grad_fn=<AddBackward0>)\n",
      "Epoch :  300 cost :  0.056429438292980194\n",
      "tensor([1.3006, 1.8226, 2.3445, 2.8664, 3.3883, 3.9103, 4.4322, 4.9541, 5.4760,\n",
      "        5.9980], grad_fn=<AddBackward0>)\n",
      "tensor([1.3016, 1.8234, 2.3451, 2.8669, 3.3886, 3.9104, 4.4322, 4.9539, 5.4757,\n",
      "        5.9975], grad_fn=<AddBackward0>)\n",
      "tensor([1.3026, 1.8242, 2.3458, 2.8674, 3.3890, 3.9106, 4.4322, 4.9538, 5.4754,\n",
      "        5.9970], grad_fn=<AddBackward0>)\n",
      "tensor([1.3035, 1.8250, 2.3464, 2.8678, 3.3893, 3.9107, 4.4322, 4.9536, 5.4750,\n",
      "        5.9965], grad_fn=<AddBackward0>)\n",
      "tensor([1.3045, 1.8258, 2.3470, 2.8683, 3.3896, 3.9109, 4.4322, 4.9534, 5.4747,\n",
      "        5.9960], grad_fn=<AddBackward0>)\n",
      "tensor([1.3054, 1.8266, 2.3477, 2.8688, 3.3899, 3.9110, 4.4321, 4.9533, 5.4744,\n",
      "        5.9955], grad_fn=<AddBackward0>)\n",
      "tensor([1.3064, 1.8274, 2.3483, 2.8693, 3.3902, 3.9112, 4.4321, 4.9531, 5.4741,\n",
      "        5.9950], grad_fn=<AddBackward0>)\n",
      "tensor([1.3074, 1.8281, 2.3489, 2.8697, 3.3905, 3.9113, 4.4321, 4.9529, 5.4737,\n",
      "        5.9945], grad_fn=<AddBackward0>)\n",
      "tensor([1.3083, 1.8289, 2.3496, 2.8702, 3.3909, 3.9115, 4.4321, 4.9528, 5.4734,\n",
      "        5.9940], grad_fn=<AddBackward0>)\n",
      "tensor([1.3092, 1.8297, 2.3502, 2.8707, 3.3912, 3.9116, 4.4321, 4.9526, 5.4731,\n",
      "        5.9936], grad_fn=<AddBackward0>)\n",
      "tensor([1.3102, 1.8305, 2.3508, 2.8711, 3.3915, 3.9118, 4.4321, 4.9524, 5.4728,\n",
      "        5.9931], grad_fn=<AddBackward0>)\n",
      "tensor([1.3111, 1.8313, 2.3514, 2.8716, 3.3918, 3.9119, 4.4321, 4.9523, 5.4724,\n",
      "        5.9926], grad_fn=<AddBackward0>)\n",
      "tensor([1.3120, 1.8320, 2.3521, 2.8721, 3.3921, 3.9121, 4.4321, 4.9521, 5.4721,\n",
      "        5.9921], grad_fn=<AddBackward0>)\n",
      "tensor([1.3130, 1.8328, 2.3527, 2.8725, 3.3924, 3.9122, 4.4321, 4.9520, 5.4718,\n",
      "        5.9917], grad_fn=<AddBackward0>)\n",
      "tensor([1.3139, 1.8336, 2.3533, 2.8730, 3.3927, 3.9124, 4.4321, 4.9518, 5.4715,\n",
      "        5.9912], grad_fn=<AddBackward0>)\n",
      "tensor([1.3148, 1.8343, 2.3539, 2.8734, 3.3930, 3.9125, 4.4321, 4.9516, 5.4712,\n",
      "        5.9907], grad_fn=<AddBackward0>)\n",
      "tensor([1.3157, 1.8351, 2.3545, 2.8739, 3.3933, 3.9127, 4.4321, 4.9515, 5.4709,\n",
      "        5.9903], grad_fn=<AddBackward0>)\n",
      "tensor([1.3166, 1.8359, 2.3551, 2.8743, 3.3936, 3.9128, 4.4321, 4.9513, 5.4706,\n",
      "        5.9898], grad_fn=<AddBackward0>)\n",
      "tensor([1.3175, 1.8366, 2.3557, 2.8748, 3.3939, 3.9130, 4.4321, 4.9512, 5.4703,\n",
      "        5.9893], grad_fn=<AddBackward0>)\n",
      "tensor([1.3184, 1.8374, 2.3563, 2.8752, 3.3942, 3.9131, 4.4321, 4.9510, 5.4699,\n",
      "        5.9889], grad_fn=<AddBackward0>)\n",
      "tensor([1.3193, 1.8381, 2.3569, 2.8757, 3.3945, 3.9133, 4.4321, 4.9508, 5.4696,\n",
      "        5.9884], grad_fn=<AddBackward0>)\n",
      "tensor([1.3202, 1.8389, 2.3575, 2.8761, 3.3948, 3.9134, 4.4321, 4.9507, 5.4693,\n",
      "        5.9880], grad_fn=<AddBackward0>)\n",
      "tensor([1.3211, 1.8396, 2.3581, 2.8766, 3.3951, 3.9136, 4.4320, 4.9505, 5.4690,\n",
      "        5.9875], grad_fn=<AddBackward0>)\n",
      "tensor([1.3220, 1.8403, 2.3587, 2.8770, 3.3954, 3.9137, 4.4320, 4.9504, 5.4687,\n",
      "        5.9871], grad_fn=<AddBackward0>)\n",
      "tensor([1.3229, 1.8411, 2.3593, 2.8775, 3.3957, 3.9138, 4.4320, 4.9502, 5.4684,\n",
      "        5.9866], grad_fn=<AddBackward0>)\n",
      "tensor([1.3238, 1.8418, 2.3598, 2.8779, 3.3959, 3.9140, 4.4320, 4.9501, 5.4681,\n",
      "        5.9862], grad_fn=<AddBackward0>)\n",
      "tensor([1.3246, 1.8425, 2.3604, 2.8783, 3.3962, 3.9141, 4.4320, 4.9499, 5.4678,\n",
      "        5.9857], grad_fn=<AddBackward0>)\n",
      "tensor([1.3255, 1.8433, 2.3610, 2.8788, 3.3965, 3.9143, 4.4320, 4.9498, 5.4675,\n",
      "        5.9853], grad_fn=<AddBackward0>)\n",
      "tensor([1.3264, 1.8440, 2.3616, 2.8792, 3.3968, 3.9144, 4.4320, 4.9496, 5.4672,\n",
      "        5.9848], grad_fn=<AddBackward0>)\n",
      "tensor([1.3272, 1.8447, 2.3622, 2.8796, 3.3971, 3.9145, 4.4320, 4.9495, 5.4669,\n",
      "        5.9844], grad_fn=<AddBackward0>)\n",
      "tensor([1.3281, 1.8454, 2.3627, 2.8801, 3.3974, 3.9147, 4.4320, 4.9493, 5.4666,\n",
      "        5.9840], grad_fn=<AddBackward0>)\n",
      "tensor([1.3290, 1.8461, 2.3633, 2.8805, 3.3977, 3.9148, 4.4320, 4.9492, 5.4663,\n",
      "        5.9835], grad_fn=<AddBackward0>)\n",
      "tensor([1.3298, 1.8468, 2.3639, 2.8809, 3.3979, 3.9150, 4.4320, 4.9490, 5.4661,\n",
      "        5.9831], grad_fn=<AddBackward0>)\n",
      "tensor([1.3307, 1.8475, 2.3644, 2.8813, 3.3982, 3.9151, 4.4320, 4.9489, 5.4658,\n",
      "        5.9826], grad_fn=<AddBackward0>)\n",
      "tensor([1.3315, 1.8483, 2.3650, 2.8817, 3.3985, 3.9152, 4.4320, 4.9487, 5.4655,\n",
      "        5.9822], grad_fn=<AddBackward0>)\n",
      "tensor([1.3323, 1.8490, 2.3656, 2.8822, 3.3988, 3.9154, 4.4320, 4.9486, 5.4652,\n",
      "        5.9818], grad_fn=<AddBackward0>)\n",
      "tensor([1.3332, 1.8497, 2.3661, 2.8826, 3.3990, 3.9155, 4.4320, 4.9484, 5.4649,\n",
      "        5.9814], grad_fn=<AddBackward0>)\n",
      "tensor([1.3340, 1.8503, 2.3667, 2.8830, 3.3993, 3.9156, 4.4320, 4.9483, 5.4646,\n",
      "        5.9809], grad_fn=<AddBackward0>)\n",
      "tensor([1.3349, 1.8510, 2.3672, 2.8834, 3.3996, 3.9158, 4.4320, 4.9481, 5.4643,\n",
      "        5.9805], grad_fn=<AddBackward0>)\n",
      "tensor([1.3357, 1.8517, 2.3678, 2.8838, 3.3999, 3.9159, 4.4320, 4.9480, 5.4640,\n",
      "        5.9801], grad_fn=<AddBackward0>)\n",
      "tensor([1.3365, 1.8524, 2.3683, 2.8842, 3.4001, 3.9160, 4.4319, 4.9479, 5.4638,\n",
      "        5.9797], grad_fn=<AddBackward0>)\n",
      "tensor([1.3373, 1.8531, 2.3689, 2.8846, 3.4004, 3.9162, 4.4319, 4.9477, 5.4635,\n",
      "        5.9792], grad_fn=<AddBackward0>)\n",
      "tensor([1.3382, 1.8538, 2.3694, 2.8850, 3.4007, 3.9163, 4.4319, 4.9476, 5.4632,\n",
      "        5.9788], grad_fn=<AddBackward0>)\n",
      "tensor([1.3390, 1.8545, 2.3700, 2.8855, 3.4009, 3.9164, 4.4319, 4.9474, 5.4629,\n",
      "        5.9784], grad_fn=<AddBackward0>)\n",
      "tensor([1.3398, 1.8551, 2.3705, 2.8859, 3.4012, 3.9166, 4.4319, 4.9473, 5.4626,\n",
      "        5.9780], grad_fn=<AddBackward0>)\n",
      "tensor([1.3406, 1.8558, 2.3710, 2.8863, 3.4015, 3.9167, 4.4319, 4.9471, 5.4624,\n",
      "        5.9776], grad_fn=<AddBackward0>)\n",
      "tensor([1.3414, 1.8565, 2.3716, 2.8867, 3.4017, 3.9168, 4.4319, 4.9470, 5.4621,\n",
      "        5.9772], grad_fn=<AddBackward0>)\n",
      "tensor([1.3422, 1.8571, 2.3721, 2.8871, 3.4020, 3.9170, 4.4319, 4.9469, 5.4618,\n",
      "        5.9768], grad_fn=<AddBackward0>)\n",
      "tensor([1.3430, 1.8578, 2.3726, 2.8875, 3.4023, 3.9171, 4.4319, 4.9467, 5.4615,\n",
      "        5.9764], grad_fn=<AddBackward0>)\n",
      "tensor([1.3438, 1.8585, 2.3732, 2.8878, 3.4025, 3.9172, 4.4319, 4.9466, 5.4613,\n",
      "        5.9760], grad_fn=<AddBackward0>)\n",
      "tensor([1.3446, 1.8591, 2.3737, 2.8882, 3.4028, 3.9173, 4.4319, 4.9465, 5.4610,\n",
      "        5.9756], grad_fn=<AddBackward0>)\n",
      "tensor([1.3454, 1.8598, 2.3742, 2.8886, 3.4031, 3.9175, 4.4319, 4.9463, 5.4607,\n",
      "        5.9752], grad_fn=<AddBackward0>)\n",
      "tensor([1.3462, 1.8604, 2.3747, 2.8890, 3.4033, 3.9176, 4.4319, 4.9462, 5.4605,\n",
      "        5.9748], grad_fn=<AddBackward0>)\n",
      "tensor([1.3469, 1.8611, 2.3752, 2.8894, 3.4036, 3.9177, 4.4319, 4.9460, 5.4602,\n",
      "        5.9744], grad_fn=<AddBackward0>)\n",
      "tensor([1.3477, 1.8617, 2.3758, 2.8898, 3.4038, 3.9179, 4.4319, 4.9459, 5.4599,\n",
      "        5.9740], grad_fn=<AddBackward0>)\n",
      "tensor([1.3485, 1.8624, 2.3763, 2.8902, 3.4041, 3.9180, 4.4319, 4.9458, 5.4597,\n",
      "        5.9736], grad_fn=<AddBackward0>)\n",
      "tensor([1.3493, 1.8630, 2.3768, 2.8906, 3.4043, 3.9181, 4.4319, 4.9456, 5.4594,\n",
      "        5.9732], grad_fn=<AddBackward0>)\n",
      "tensor([1.3500, 1.8637, 2.3773, 2.8909, 3.4046, 3.9182, 4.4319, 4.9455, 5.4591,\n",
      "        5.9728], grad_fn=<AddBackward0>)\n",
      "tensor([1.3508, 1.8643, 2.3778, 2.8913, 3.4048, 3.9183, 4.4319, 4.9454, 5.4589,\n",
      "        5.9724], grad_fn=<AddBackward0>)\n",
      "tensor([1.3516, 1.8649, 2.3783, 2.8917, 3.4051, 3.9185, 4.4319, 4.9452, 5.4586,\n",
      "        5.9720], grad_fn=<AddBackward0>)\n",
      "tensor([1.3523, 1.8656, 2.3788, 2.8921, 3.4053, 3.9186, 4.4318, 4.9451, 5.4584,\n",
      "        5.9716], grad_fn=<AddBackward0>)\n",
      "tensor([1.3531, 1.8662, 2.3793, 2.8925, 3.4056, 3.9187, 4.4318, 4.9450, 5.4581,\n",
      "        5.9712], grad_fn=<AddBackward0>)\n",
      "tensor([1.3538, 1.8668, 2.3798, 2.8928, 3.4058, 3.9188, 4.4318, 4.9448, 5.4578,\n",
      "        5.9708], grad_fn=<AddBackward0>)\n",
      "tensor([1.3546, 1.8674, 2.3803, 2.8932, 3.4061, 3.9190, 4.4318, 4.9447, 5.4576,\n",
      "        5.9705], grad_fn=<AddBackward0>)\n",
      "tensor([1.3553, 1.8681, 2.3808, 2.8936, 3.4063, 3.9191, 4.4318, 4.9446, 5.4573,\n",
      "        5.9701], grad_fn=<AddBackward0>)\n",
      "tensor([1.3561, 1.8687, 2.3813, 2.8939, 3.4066, 3.9192, 4.4318, 4.9445, 5.4571,\n",
      "        5.9697], grad_fn=<AddBackward0>)\n",
      "tensor([1.3568, 1.8693, 2.3818, 2.8943, 3.4068, 3.9193, 4.4318, 4.9443, 5.4568,\n",
      "        5.9693], grad_fn=<AddBackward0>)\n",
      "tensor([1.3575, 1.8699, 2.3823, 2.8947, 3.4071, 3.9194, 4.4318, 4.9442, 5.4566,\n",
      "        5.9690], grad_fn=<AddBackward0>)\n",
      "tensor([1.3583, 1.8705, 2.3828, 2.8950, 3.4073, 3.9196, 4.4318, 4.9441, 5.4563,\n",
      "        5.9686], grad_fn=<AddBackward0>)\n",
      "tensor([1.3590, 1.8711, 2.3833, 2.8954, 3.4075, 3.9197, 4.4318, 4.9439, 5.4561,\n",
      "        5.9682], grad_fn=<AddBackward0>)\n",
      "tensor([1.3597, 1.8717, 2.3838, 2.8958, 3.4078, 3.9198, 4.4318, 4.9438, 5.4558,\n",
      "        5.9678], grad_fn=<AddBackward0>)\n",
      "tensor([1.3605, 1.8723, 2.3842, 2.8961, 3.4080, 3.9199, 4.4318, 4.9437, 5.4556,\n",
      "        5.9675], grad_fn=<AddBackward0>)\n",
      "tensor([1.3612, 1.8729, 2.3847, 2.8965, 3.4083, 3.9200, 4.4318, 4.9436, 5.4553,\n",
      "        5.9671], grad_fn=<AddBackward0>)\n",
      "tensor([1.3619, 1.8735, 2.3852, 2.8968, 3.4085, 3.9201, 4.4318, 4.9434, 5.4551,\n",
      "        5.9667], grad_fn=<AddBackward0>)\n",
      "tensor([1.3626, 1.8741, 2.3857, 2.8972, 3.4087, 3.9203, 4.4318, 4.9433, 5.4548,\n",
      "        5.9664], grad_fn=<AddBackward0>)\n",
      "tensor([1.3633, 1.8747, 2.3861, 2.8975, 3.4090, 3.9204, 4.4318, 4.9432, 5.4546,\n",
      "        5.9660], grad_fn=<AddBackward0>)\n",
      "tensor([1.3640, 1.8753, 2.3866, 2.8979, 3.4092, 3.9205, 4.4318, 4.9431, 5.4544,\n",
      "        5.9656], grad_fn=<AddBackward0>)\n",
      "tensor([1.3647, 1.8759, 2.3871, 2.8983, 3.4094, 3.9206, 4.4318, 4.9429, 5.4541,\n",
      "        5.9653], grad_fn=<AddBackward0>)\n",
      "tensor([1.3654, 1.8765, 2.3875, 2.8986, 3.4097, 3.9207, 4.4318, 4.9428, 5.4539,\n",
      "        5.9649], grad_fn=<AddBackward0>)\n",
      "tensor([1.3661, 1.8771, 2.3880, 2.8990, 3.4099, 3.9208, 4.4318, 4.9427, 5.4536,\n",
      "        5.9646], grad_fn=<AddBackward0>)\n",
      "tensor([1.3668, 1.8777, 2.3885, 2.8993, 3.4101, 3.9209, 4.4318, 4.9426, 5.4534,\n",
      "        5.9642], grad_fn=<AddBackward0>)\n",
      "tensor([1.3675, 1.8782, 2.3889, 2.8996, 3.4103, 3.9210, 4.4318, 4.9425, 5.4532,\n",
      "        5.9639], grad_fn=<AddBackward0>)\n",
      "tensor([1.3682, 1.8788, 2.3894, 2.9000, 3.4106, 3.9212, 4.4317, 4.9423, 5.4529,\n",
      "        5.9635], grad_fn=<AddBackward0>)\n",
      "tensor([1.3689, 1.8794, 2.3899, 2.9003, 3.4108, 3.9213, 4.4317, 4.9422, 5.4527,\n",
      "        5.9632], grad_fn=<AddBackward0>)\n",
      "tensor([1.3696, 1.8800, 2.3903, 2.9007, 3.4110, 3.9214, 4.4317, 4.9421, 5.4525,\n",
      "        5.9628], grad_fn=<AddBackward0>)\n",
      "tensor([1.3703, 1.8805, 2.3908, 2.9010, 3.4112, 3.9215, 4.4317, 4.9420, 5.4522,\n",
      "        5.9625], grad_fn=<AddBackward0>)\n",
      "tensor([1.3710, 1.8811, 2.3912, 2.9013, 3.4115, 3.9216, 4.4317, 4.9419, 5.4520,\n",
      "        5.9621], grad_fn=<AddBackward0>)\n",
      "tensor([1.3716, 1.8817, 2.3917, 2.9017, 3.4117, 3.9217, 4.4317, 4.9417, 5.4518,\n",
      "        5.9618], grad_fn=<AddBackward0>)\n",
      "tensor([1.3723, 1.8822, 2.3921, 2.9020, 3.4119, 3.9218, 4.4317, 4.9416, 5.4515,\n",
      "        5.9614], grad_fn=<AddBackward0>)\n",
      "tensor([1.3730, 1.8828, 2.3926, 2.9024, 3.4121, 3.9219, 4.4317, 4.9415, 5.4513,\n",
      "        5.9611], grad_fn=<AddBackward0>)\n",
      "tensor([1.3737, 1.8833, 2.3930, 2.9027, 3.4124, 3.9220, 4.4317, 4.9414, 5.4511,\n",
      "        5.9607], grad_fn=<AddBackward0>)\n",
      "tensor([1.3743, 1.8839, 2.3934, 2.9030, 3.4126, 3.9221, 4.4317, 4.9413, 5.4508,\n",
      "        5.9604], grad_fn=<AddBackward0>)\n",
      "tensor([1.3750, 1.8844, 2.3939, 2.9033, 3.4128, 3.9223, 4.4317, 4.9412, 5.4506,\n",
      "        5.9601], grad_fn=<AddBackward0>)\n",
      "tensor([1.3756, 1.8850, 2.3943, 2.9037, 3.4130, 3.9224, 4.4317, 4.9410, 5.4504,\n",
      "        5.9597], grad_fn=<AddBackward0>)\n",
      "tensor([1.3763, 1.8855, 2.3948, 2.9040, 3.4132, 3.9225, 4.4317, 4.9409, 5.4502,\n",
      "        5.9594], grad_fn=<AddBackward0>)\n",
      "tensor([1.3770, 1.8861, 2.3952, 2.9043, 3.4134, 3.9226, 4.4317, 4.9408, 5.4499,\n",
      "        5.9591], grad_fn=<AddBackward0>)\n",
      "tensor([1.3776, 1.8866, 2.3956, 2.9046, 3.4137, 3.9227, 4.4317, 4.9407, 5.4497,\n",
      "        5.9587], grad_fn=<AddBackward0>)\n",
      "tensor([1.3783, 1.8872, 2.3961, 2.9050, 3.4139, 3.9228, 4.4317, 4.9406, 5.4495,\n",
      "        5.9584], grad_fn=<AddBackward0>)\n",
      "tensor([1.3789, 1.8877, 2.3965, 2.9053, 3.4141, 3.9229, 4.4317, 4.9405, 5.4493,\n",
      "        5.9581], grad_fn=<AddBackward0>)\n",
      "tensor([1.3795, 1.8882, 2.3969, 2.9056, 3.4143, 3.9230, 4.4317, 4.9404, 5.4490,\n",
      "        5.9577], grad_fn=<AddBackward0>)\n",
      "Epoch :  400 cost :  0.04743271693587303\n",
      "tensor([1.3802, 1.8888, 2.3974, 2.9059, 3.4145, 3.9231, 4.4317, 4.9402, 5.4488,\n",
      "        5.9574], grad_fn=<AddBackward0>)\n",
      "tensor([1.3808, 1.8893, 2.3978, 2.9062, 3.4147, 3.9232, 4.4317, 4.9401, 5.4486,\n",
      "        5.9571], grad_fn=<AddBackward0>)\n",
      "tensor([1.3815, 1.8898, 2.3982, 2.9066, 3.4149, 3.9233, 4.4317, 4.9400, 5.4484,\n",
      "        5.9568], grad_fn=<AddBackward0>)\n",
      "tensor([1.3821, 1.8904, 2.3986, 2.9069, 3.4151, 3.9234, 4.4317, 4.9399, 5.4482,\n",
      "        5.9564], grad_fn=<AddBackward0>)\n",
      "tensor([1.3827, 1.8909, 2.3990, 2.9072, 3.4153, 3.9235, 4.4317, 4.9398, 5.4480,\n",
      "        5.9561], grad_fn=<AddBackward0>)\n",
      "tensor([1.3834, 1.8914, 2.3995, 2.9075, 3.4156, 3.9236, 4.4316, 4.9397, 5.4477,\n",
      "        5.9558], grad_fn=<AddBackward0>)\n",
      "tensor([1.3840, 1.8919, 2.3999, 2.9078, 3.4158, 3.9237, 4.4316, 4.9396, 5.4475,\n",
      "        5.9555], grad_fn=<AddBackward0>)\n",
      "tensor([1.3846, 1.8924, 2.4003, 2.9081, 3.4160, 3.9238, 4.4316, 4.9395, 5.4473,\n",
      "        5.9552], grad_fn=<AddBackward0>)\n",
      "tensor([1.3852, 1.8930, 2.4007, 2.9084, 3.4162, 3.9239, 4.4316, 4.9394, 5.4471,\n",
      "        5.9548], grad_fn=<AddBackward0>)\n",
      "tensor([1.3858, 1.8935, 2.4011, 2.9087, 3.4164, 3.9240, 4.4316, 4.9393, 5.4469,\n",
      "        5.9545], grad_fn=<AddBackward0>)\n",
      "tensor([1.3865, 1.8940, 2.4015, 2.9090, 3.4166, 3.9241, 4.4316, 4.9392, 5.4467,\n",
      "        5.9542], grad_fn=<AddBackward0>)\n",
      "tensor([1.3871, 1.8945, 2.4019, 2.9093, 3.4168, 3.9242, 4.4316, 4.9391, 5.4465,\n",
      "        5.9539], grad_fn=<AddBackward0>)\n",
      "tensor([1.3877, 1.8950, 2.4023, 2.9097, 3.4170, 3.9243, 4.4316, 4.9389, 5.4463,\n",
      "        5.9536], grad_fn=<AddBackward0>)\n",
      "tensor([1.3883, 1.8955, 2.4027, 2.9100, 3.4172, 3.9244, 4.4316, 4.9388, 5.4461,\n",
      "        5.9533], grad_fn=<AddBackward0>)\n",
      "tensor([1.3889, 1.8960, 2.4031, 2.9103, 3.4174, 3.9245, 4.4316, 4.9387, 5.4459,\n",
      "        5.9530], grad_fn=<AddBackward0>)\n",
      "tensor([1.3895, 1.8965, 2.4035, 2.9106, 3.4176, 3.9246, 4.4316, 4.9386, 5.4456,\n",
      "        5.9527], grad_fn=<AddBackward0>)\n",
      "tensor([1.3901, 1.8970, 2.4039, 2.9108, 3.4178, 3.9247, 4.4316, 4.9385, 5.4454,\n",
      "        5.9524], grad_fn=<AddBackward0>)\n",
      "tensor([1.3907, 1.8975, 2.4043, 2.9111, 3.4180, 3.9248, 4.4316, 4.9384, 5.4452,\n",
      "        5.9521], grad_fn=<AddBackward0>)\n",
      "tensor([1.3913, 1.8980, 2.4047, 2.9114, 3.4182, 3.9249, 4.4316, 4.9383, 5.4450,\n",
      "        5.9518], grad_fn=<AddBackward0>)\n",
      "tensor([1.3919, 1.8985, 2.4051, 2.9117, 3.4184, 3.9250, 4.4316, 4.9382, 5.4448,\n",
      "        5.9515], grad_fn=<AddBackward0>)\n",
      "tensor([1.3925, 1.8990, 2.4055, 2.9120, 3.4185, 3.9251, 4.4316, 4.9381, 5.4446,\n",
      "        5.9512], grad_fn=<AddBackward0>)\n",
      "tensor([1.3931, 1.8995, 2.4059, 2.9123, 3.4187, 3.9252, 4.4316, 4.9380, 5.4444,\n",
      "        5.9509], grad_fn=<AddBackward0>)\n",
      "tensor([1.3936, 1.9000, 2.4063, 2.9126, 3.4189, 3.9253, 4.4316, 4.9379, 5.4442,\n",
      "        5.9506], grad_fn=<AddBackward0>)\n",
      "tensor([1.3942, 1.9004, 2.4067, 2.9129, 3.4191, 3.9254, 4.4316, 4.9378, 5.4440,\n",
      "        5.9503], grad_fn=<AddBackward0>)\n",
      "tensor([1.3948, 1.9009, 2.4071, 2.9132, 3.4193, 3.9254, 4.4316, 4.9377, 5.4438,\n",
      "        5.9500], grad_fn=<AddBackward0>)\n",
      "tensor([1.3954, 1.9014, 2.4074, 2.9135, 3.4195, 3.9255, 4.4316, 4.9376, 5.4436,\n",
      "        5.9497], grad_fn=<AddBackward0>)\n",
      "tensor([1.3960, 1.9019, 2.4078, 2.9138, 3.4197, 3.9256, 4.4316, 4.9375, 5.4434,\n",
      "        5.9494], grad_fn=<AddBackward0>)\n",
      "tensor([1.3965, 1.9024, 2.4082, 2.9140, 3.4199, 3.9257, 4.4316, 4.9374, 5.4432,\n",
      "        5.9491], grad_fn=<AddBackward0>)\n",
      "tensor([1.3971, 1.9028, 2.4086, 2.9143, 3.4201, 3.9258, 4.4316, 4.9373, 5.4431,\n",
      "        5.9488], grad_fn=<AddBackward0>)\n",
      "tensor([1.3977, 1.9033, 2.4090, 2.9146, 3.4203, 3.9259, 4.4316, 4.9372, 5.4429,\n",
      "        5.9485], grad_fn=<AddBackward0>)\n",
      "tensor([1.3982, 1.9038, 2.4093, 2.9149, 3.4204, 3.9260, 4.4316, 4.9371, 5.4427,\n",
      "        5.9482], grad_fn=<AddBackward0>)\n",
      "tensor([1.3988, 1.9042, 2.4097, 2.9152, 3.4206, 3.9261, 4.4316, 4.9370, 5.4425,\n",
      "        5.9479], grad_fn=<AddBackward0>)\n",
      "tensor([1.3993, 1.9047, 2.4101, 2.9154, 3.4208, 3.9262, 4.4315, 4.9369, 5.4423,\n",
      "        5.9476], grad_fn=<AddBackward0>)\n",
      "tensor([1.3999, 1.9052, 2.4105, 2.9157, 3.4210, 3.9263, 4.4315, 4.9368, 5.4421,\n",
      "        5.9474], grad_fn=<AddBackward0>)\n",
      "tensor([1.4005, 1.9056, 2.4108, 2.9160, 3.4212, 3.9264, 4.4315, 4.9367, 5.4419,\n",
      "        5.9471], grad_fn=<AddBackward0>)\n",
      "tensor([1.4010, 1.9061, 2.4112, 2.9163, 3.4214, 3.9264, 4.4315, 4.9366, 5.4417,\n",
      "        5.9468], grad_fn=<AddBackward0>)\n",
      "tensor([1.4016, 1.9066, 2.4116, 2.9166, 3.4215, 3.9265, 4.4315, 4.9365, 5.4415,\n",
      "        5.9465], grad_fn=<AddBackward0>)\n",
      "tensor([1.4021, 1.9070, 2.4119, 2.9168, 3.4217, 3.9266, 4.4315, 4.9364, 5.4413,\n",
      "        5.9462], grad_fn=<AddBackward0>)\n",
      "tensor([1.4027, 1.9075, 2.4123, 2.9171, 3.4219, 3.9267, 4.4315, 4.9363, 5.4411,\n",
      "        5.9460], grad_fn=<AddBackward0>)\n",
      "tensor([1.4032, 1.9079, 2.4126, 2.9174, 3.4221, 3.9268, 4.4315, 4.9362, 5.4410,\n",
      "        5.9457], grad_fn=<AddBackward0>)\n",
      "tensor([1.4037, 1.9084, 2.4130, 2.9176, 3.4223, 3.9269, 4.4315, 4.9361, 5.4408,\n",
      "        5.9454], grad_fn=<AddBackward0>)\n",
      "tensor([1.4043, 1.9088, 2.4134, 2.9179, 3.4224, 3.9270, 4.4315, 4.9361, 5.4406,\n",
      "        5.9451], grad_fn=<AddBackward0>)\n",
      "tensor([1.4048, 1.9093, 2.4137, 2.9182, 3.4226, 3.9271, 4.4315, 4.9360, 5.4404,\n",
      "        5.9449], grad_fn=<AddBackward0>)\n",
      "tensor([1.4054, 1.9097, 2.4141, 2.9184, 3.4228, 3.9272, 4.4315, 4.9359, 5.4402,\n",
      "        5.9446], grad_fn=<AddBackward0>)\n",
      "tensor([1.4059, 1.9102, 2.4144, 2.9187, 3.4230, 3.9272, 4.4315, 4.9358, 5.4400,\n",
      "        5.9443], grad_fn=<AddBackward0>)\n",
      "tensor([1.4064, 1.9106, 2.4148, 2.9190, 3.4231, 3.9273, 4.4315, 4.9357, 5.4399,\n",
      "        5.9440], grad_fn=<AddBackward0>)\n",
      "tensor([1.4070, 1.9110, 2.4151, 2.9192, 3.4233, 3.9274, 4.4315, 4.9356, 5.4397,\n",
      "        5.9438], grad_fn=<AddBackward0>)\n",
      "tensor([1.4075, 1.9115, 2.4155, 2.9195, 3.4235, 3.9275, 4.4315, 4.9355, 5.4395,\n",
      "        5.9435], grad_fn=<AddBackward0>)\n",
      "tensor([1.4080, 1.9119, 2.4158, 2.9197, 3.4237, 3.9276, 4.4315, 4.9354, 5.4393,\n",
      "        5.9432], grad_fn=<AddBackward0>)\n",
      "tensor([1.4085, 1.9124, 2.4162, 2.9200, 3.4238, 3.9277, 4.4315, 4.9353, 5.4391,\n",
      "        5.9430], grad_fn=<AddBackward0>)\n",
      "tensor([1.4090, 1.9128, 2.4165, 2.9203, 3.4240, 3.9277, 4.4315, 4.9352, 5.4390,\n",
      "        5.9427], grad_fn=<AddBackward0>)\n",
      "tensor([1.4096, 1.9132, 2.4169, 2.9205, 3.4242, 3.9278, 4.4315, 4.9351, 5.4388,\n",
      "        5.9424], grad_fn=<AddBackward0>)\n",
      "tensor([1.4101, 1.9136, 2.4172, 2.9208, 3.4243, 3.9279, 4.4315, 4.9350, 5.4386,\n",
      "        5.9422], grad_fn=<AddBackward0>)\n",
      "tensor([1.4106, 1.9141, 2.4176, 2.9210, 3.4245, 3.9280, 4.4315, 4.9350, 5.4384,\n",
      "        5.9419], grad_fn=<AddBackward0>)\n",
      "tensor([1.4111, 1.9145, 2.4179, 2.9213, 3.4247, 3.9281, 4.4315, 4.9349, 5.4383,\n",
      "        5.9417], grad_fn=<AddBackward0>)\n",
      "tensor([1.4116, 1.9149, 2.4182, 2.9215, 3.4248, 3.9282, 4.4315, 4.9348, 5.4381,\n",
      "        5.9414], grad_fn=<AddBackward0>)\n",
      "tensor([1.4121, 1.9153, 2.4186, 2.9218, 3.4250, 3.9282, 4.4315, 4.9347, 5.4379,\n",
      "        5.9411], grad_fn=<AddBackward0>)\n",
      "tensor([1.4126, 1.9158, 2.4189, 2.9220, 3.4252, 3.9283, 4.4315, 4.9346, 5.4377,\n",
      "        5.9409], grad_fn=<AddBackward0>)\n",
      "tensor([1.4131, 1.9162, 2.4192, 2.9223, 3.4253, 3.9284, 4.4315, 4.9345, 5.4376,\n",
      "        5.9406], grad_fn=<AddBackward0>)\n",
      "tensor([1.4136, 1.9166, 2.4196, 2.9225, 3.4255, 3.9285, 4.4315, 4.9344, 5.4374,\n",
      "        5.9404], grad_fn=<AddBackward0>)\n",
      "tensor([1.4141, 1.9170, 2.4199, 2.9228, 3.4257, 3.9286, 4.4315, 4.9343, 5.4372,\n",
      "        5.9401], grad_fn=<AddBackward0>)\n",
      "tensor([1.4146, 1.9174, 2.4202, 2.9230, 3.4258, 3.9286, 4.4314, 4.9343, 5.4371,\n",
      "        5.9399], grad_fn=<AddBackward0>)\n",
      "tensor([1.4151, 1.9178, 2.4206, 2.9233, 3.4260, 3.9287, 4.4314, 4.9342, 5.4369,\n",
      "        5.9396], grad_fn=<AddBackward0>)\n",
      "tensor([1.4156, 1.9182, 2.4209, 2.9235, 3.4262, 3.9288, 4.4314, 4.9341, 5.4367,\n",
      "        5.9394], grad_fn=<AddBackward0>)\n",
      "tensor([1.4161, 1.9187, 2.4212, 2.9238, 3.4263, 3.9289, 4.4314, 4.9340, 5.4366,\n",
      "        5.9391], grad_fn=<AddBackward0>)\n",
      "tensor([1.4166, 1.9191, 2.4215, 2.9240, 3.4265, 3.9290, 4.4314, 4.9339, 5.4364,\n",
      "        5.9389], grad_fn=<AddBackward0>)\n",
      "tensor([1.4171, 1.9195, 2.4219, 2.9243, 3.4266, 3.9290, 4.4314, 4.9338, 5.4362,\n",
      "        5.9386], grad_fn=<AddBackward0>)\n",
      "tensor([1.4176, 1.9199, 2.4222, 2.9245, 3.4268, 3.9291, 4.4314, 4.9337, 5.4361,\n",
      "        5.9384], grad_fn=<AddBackward0>)\n",
      "tensor([1.4180, 1.9203, 2.4225, 2.9247, 3.4270, 3.9292, 4.4314, 4.9337, 5.4359,\n",
      "        5.9381], grad_fn=<AddBackward0>)\n",
      "tensor([1.4185, 1.9207, 2.4228, 2.9250, 3.4271, 3.9293, 4.4314, 4.9336, 5.4357,\n",
      "        5.9379], grad_fn=<AddBackward0>)\n",
      "tensor([1.4190, 1.9211, 2.4231, 2.9252, 3.4273, 3.9293, 4.4314, 4.9335, 5.4356,\n",
      "        5.9376], grad_fn=<AddBackward0>)\n",
      "tensor([1.4195, 1.9215, 2.4235, 2.9254, 3.4274, 3.9294, 4.4314, 4.9334, 5.4354,\n",
      "        5.9374], grad_fn=<AddBackward0>)\n",
      "tensor([1.4199, 1.9219, 2.4238, 2.9257, 3.4276, 3.9295, 4.4314, 4.9333, 5.4352,\n",
      "        5.9372], grad_fn=<AddBackward0>)\n",
      "tensor([1.4204, 1.9222, 2.4241, 2.9259, 3.4277, 3.9296, 4.4314, 4.9332, 5.4351,\n",
      "        5.9369], grad_fn=<AddBackward0>)\n",
      "tensor([1.4209, 1.9226, 2.4244, 2.9261, 3.4279, 3.9297, 4.4314, 4.9332, 5.4349,\n",
      "        5.9367], grad_fn=<AddBackward0>)\n",
      "tensor([1.4213, 1.9230, 2.4247, 2.9264, 3.4281, 3.9297, 4.4314, 4.9331, 5.4348,\n",
      "        5.9364], grad_fn=<AddBackward0>)\n",
      "tensor([1.4218, 1.9234, 2.4250, 2.9266, 3.4282, 3.9298, 4.4314, 4.9330, 5.4346,\n",
      "        5.9362], grad_fn=<AddBackward0>)\n",
      "tensor([1.4223, 1.9238, 2.4253, 2.9268, 3.4284, 3.9299, 4.4314, 4.9329, 5.4344,\n",
      "        5.9360], grad_fn=<AddBackward0>)\n",
      "tensor([1.4227, 1.9242, 2.4256, 2.9271, 3.4285, 3.9300, 4.4314, 4.9328, 5.4343,\n",
      "        5.9357], grad_fn=<AddBackward0>)\n",
      "tensor([1.4232, 1.9246, 2.4259, 2.9273, 3.4287, 3.9300, 4.4314, 4.9328, 5.4341,\n",
      "        5.9355], grad_fn=<AddBackward0>)\n",
      "tensor([1.4237, 1.9249, 2.4262, 2.9275, 3.4288, 3.9301, 4.4314, 4.9327, 5.4340,\n",
      "        5.9353], grad_fn=<AddBackward0>)\n",
      "tensor([1.4241, 1.9253, 2.4265, 2.9278, 3.4290, 3.9302, 4.4314, 4.9326, 5.4338,\n",
      "        5.9350], grad_fn=<AddBackward0>)\n",
      "tensor([1.4246, 1.9257, 2.4268, 2.9280, 3.4291, 3.9302, 4.4314, 4.9325, 5.4337,\n",
      "        5.9348], grad_fn=<AddBackward0>)\n",
      "tensor([1.4250, 1.9261, 2.4271, 2.9282, 3.4293, 3.9303, 4.4314, 4.9324, 5.4335,\n",
      "        5.9346], grad_fn=<AddBackward0>)\n",
      "tensor([1.4255, 1.9265, 2.4274, 2.9284, 3.4294, 3.9304, 4.4314, 4.9324, 5.4333,\n",
      "        5.9343], grad_fn=<AddBackward0>)\n",
      "tensor([1.4259, 1.9268, 2.4277, 2.9286, 3.4296, 3.9305, 4.4314, 4.9323, 5.4332,\n",
      "        5.9341], grad_fn=<AddBackward0>)\n",
      "tensor([1.4264, 1.9272, 2.4280, 2.9289, 3.4297, 3.9305, 4.4314, 4.9322, 5.4330,\n",
      "        5.9339], grad_fn=<AddBackward0>)\n",
      "tensor([1.4268, 1.9276, 2.4283, 2.9291, 3.4299, 3.9306, 4.4314, 4.9321, 5.4329,\n",
      "        5.9337], grad_fn=<AddBackward0>)\n",
      "tensor([1.4273, 1.9279, 2.4286, 2.9293, 3.4300, 3.9307, 4.4314, 4.9321, 5.4327,\n",
      "        5.9334], grad_fn=<AddBackward0>)\n",
      "tensor([1.4277, 1.9283, 2.4289, 2.9295, 3.4301, 3.9308, 4.4314, 4.9320, 5.4326,\n",
      "        5.9332], grad_fn=<AddBackward0>)\n",
      "tensor([1.4281, 1.9287, 2.4292, 2.9297, 3.4303, 3.9308, 4.4314, 4.9319, 5.4324,\n",
      "        5.9330], grad_fn=<AddBackward0>)\n",
      "tensor([1.4286, 1.9290, 2.4295, 2.9300, 3.4304, 3.9309, 4.4314, 4.9318, 5.4323,\n",
      "        5.9328], grad_fn=<AddBackward0>)\n",
      "tensor([1.4290, 1.9294, 2.4298, 2.9302, 3.4306, 3.9310, 4.4314, 4.9317, 5.4321,\n",
      "        5.9325], grad_fn=<AddBackward0>)\n",
      "tensor([1.4294, 1.9298, 2.4301, 2.9304, 3.4307, 3.9310, 4.4314, 4.9317, 5.4320,\n",
      "        5.9323], grad_fn=<AddBackward0>)\n",
      "tensor([1.4299, 1.9301, 2.4304, 2.9306, 3.4309, 3.9311, 4.4314, 4.9316, 5.4318,\n",
      "        5.9321], grad_fn=<AddBackward0>)\n",
      "tensor([1.4303, 1.9305, 2.4307, 2.9308, 3.4310, 3.9312, 4.4313, 4.9315, 5.4317,\n",
      "        5.9319], grad_fn=<AddBackward0>)\n",
      "tensor([1.4307, 1.9308, 2.4309, 2.9310, 3.4311, 3.9312, 4.4313, 4.9314, 5.4316,\n",
      "        5.9317], grad_fn=<AddBackward0>)\n",
      "tensor([1.4312, 1.9312, 2.4312, 2.9312, 3.4313, 3.9313, 4.4313, 4.9314, 5.4314,\n",
      "        5.9314], grad_fn=<AddBackward0>)\n",
      "tensor([1.4316, 1.9315, 2.4315, 2.9315, 3.4314, 3.9314, 4.4313, 4.9313, 5.4313,\n",
      "        5.9312], grad_fn=<AddBackward0>)\n",
      "tensor([1.4320, 1.9319, 2.4318, 2.9317, 3.4316, 3.9314, 4.4313, 4.9312, 5.4311,\n",
      "        5.9310], grad_fn=<AddBackward0>)\n",
      "Epoch :  500 cost :  0.04355520009994507\n",
      "tensor([1.4324, 1.9322, 2.4321, 2.9319, 3.4317, 3.9315, 4.4313, 4.9312, 5.4310,\n",
      "        5.9308], grad_fn=<AddBackward0>)\n",
      "tensor([1.4328, 1.9326, 2.4323, 2.9321, 3.4318, 3.9316, 4.4313, 4.9311, 5.4308,\n",
      "        5.9306], grad_fn=<AddBackward0>)\n",
      "tensor([1.4333, 1.9329, 2.4326, 2.9323, 3.4320, 3.9317, 4.4313, 4.9310, 5.4307,\n",
      "        5.9304], grad_fn=<AddBackward0>)\n",
      "tensor([1.4337, 1.9333, 2.4329, 2.9325, 3.4321, 3.9317, 4.4313, 4.9309, 5.4305,\n",
      "        5.9302], grad_fn=<AddBackward0>)\n",
      "tensor([1.4341, 1.9336, 2.4332, 2.9327, 3.4322, 3.9318, 4.4313, 4.9309, 5.4304,\n",
      "        5.9299], grad_fn=<AddBackward0>)\n",
      "tensor([1.4345, 1.9340, 2.4334, 2.9329, 3.4324, 3.9319, 4.4313, 4.9308, 5.4303,\n",
      "        5.9297], grad_fn=<AddBackward0>)\n",
      "tensor([1.4349, 1.9343, 2.4337, 2.9331, 3.4325, 3.9319, 4.4313, 4.9307, 5.4301,\n",
      "        5.9295], grad_fn=<AddBackward0>)\n",
      "tensor([1.4353, 1.9347, 2.4340, 2.9333, 3.4327, 3.9320, 4.4313, 4.9306, 5.4300,\n",
      "        5.9293], grad_fn=<AddBackward0>)\n",
      "tensor([1.4357, 1.9350, 2.4343, 2.9335, 3.4328, 3.9320, 4.4313, 4.9306, 5.4298,\n",
      "        5.9291], grad_fn=<AddBackward0>)\n",
      "tensor([1.4361, 1.9353, 2.4345, 2.9337, 3.4329, 3.9321, 4.4313, 4.9305, 5.4297,\n",
      "        5.9289], grad_fn=<AddBackward0>)\n",
      "tensor([1.4365, 1.9357, 2.4348, 2.9339, 3.4331, 3.9322, 4.4313, 4.9304, 5.4296,\n",
      "        5.9287], grad_fn=<AddBackward0>)\n",
      "tensor([1.4369, 1.9360, 2.4351, 2.9341, 3.4332, 3.9322, 4.4313, 4.9304, 5.4294,\n",
      "        5.9285], grad_fn=<AddBackward0>)\n",
      "tensor([1.4373, 1.9363, 2.4353, 2.9343, 3.4333, 3.9323, 4.4313, 4.9303, 5.4293,\n",
      "        5.9283], grad_fn=<AddBackward0>)\n",
      "tensor([1.4377, 1.9367, 2.4356, 2.9345, 3.4334, 3.9324, 4.4313, 4.9302, 5.4292,\n",
      "        5.9281], grad_fn=<AddBackward0>)\n",
      "tensor([1.4381, 1.9370, 2.4359, 2.9347, 3.4336, 3.9324, 4.4313, 4.9302, 5.4290,\n",
      "        5.9279], grad_fn=<AddBackward0>)\n",
      "tensor([1.4385, 1.9373, 2.4361, 2.9349, 3.4337, 3.9325, 4.4313, 4.9301, 5.4289,\n",
      "        5.9277], grad_fn=<AddBackward0>)\n",
      "tensor([1.4389, 1.9377, 2.4364, 2.9351, 3.4338, 3.9326, 4.4313, 4.9300, 5.4287,\n",
      "        5.9275], grad_fn=<AddBackward0>)\n",
      "tensor([1.4393, 1.9380, 2.4366, 2.9353, 3.4340, 3.9326, 4.4313, 4.9300, 5.4286,\n",
      "        5.9273], grad_fn=<AddBackward0>)\n",
      "tensor([1.4397, 1.9383, 2.4369, 2.9355, 3.4341, 3.9327, 4.4313, 4.9299, 5.4285,\n",
      "        5.9271], grad_fn=<AddBackward0>)\n",
      "tensor([1.4401, 1.9386, 2.4372, 2.9357, 3.4342, 3.9328, 4.4313, 4.9298, 5.4283,\n",
      "        5.9269], grad_fn=<AddBackward0>)\n",
      "tensor([1.4405, 1.9390, 2.4374, 2.9359, 3.4344, 3.9328, 4.4313, 4.9298, 5.4282,\n",
      "        5.9267], grad_fn=<AddBackward0>)\n",
      "tensor([1.4409, 1.9393, 2.4377, 2.9361, 3.4345, 3.9329, 4.4313, 4.9297, 5.4281,\n",
      "        5.9265], grad_fn=<AddBackward0>)\n",
      "tensor([1.4413, 1.9396, 2.4379, 2.9363, 3.4346, 3.9329, 4.4313, 4.9296, 5.4280,\n",
      "        5.9263], grad_fn=<AddBackward0>)\n",
      "tensor([1.4416, 1.9399, 2.4382, 2.9365, 3.4347, 3.9330, 4.4313, 4.9295, 5.4278,\n",
      "        5.9261], grad_fn=<AddBackward0>)\n",
      "tensor([1.4420, 1.9402, 2.4384, 2.9366, 3.4349, 3.9331, 4.4313, 4.9295, 5.4277,\n",
      "        5.9259], grad_fn=<AddBackward0>)\n",
      "tensor([1.4424, 1.9405, 2.4387, 2.9368, 3.4350, 3.9331, 4.4313, 4.9294, 5.4276,\n",
      "        5.9257], grad_fn=<AddBackward0>)\n",
      "tensor([1.4428, 1.9409, 2.4389, 2.9370, 3.4351, 3.9332, 4.4313, 4.9294, 5.4274,\n",
      "        5.9255], grad_fn=<AddBackward0>)\n",
      "tensor([1.4431, 1.9412, 2.4392, 2.9372, 3.4352, 3.9332, 4.4313, 4.9293, 5.4273,\n",
      "        5.9253], grad_fn=<AddBackward0>)\n",
      "tensor([1.4435, 1.9415, 2.4394, 2.9374, 3.4353, 3.9333, 4.4313, 4.9292, 5.4272,\n",
      "        5.9251], grad_fn=<AddBackward0>)\n",
      "tensor([1.4439, 1.9418, 2.4397, 2.9376, 3.4355, 3.9334, 4.4313, 4.9292, 5.4271,\n",
      "        5.9249], grad_fn=<AddBackward0>)\n",
      "tensor([1.4443, 1.9421, 2.4399, 2.9378, 3.4356, 3.9334, 4.4313, 4.9291, 5.4269,\n",
      "        5.9248], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4446, 1.9424, 2.4402, 2.9379, 3.4357, 3.9335, 4.4313, 4.9290, 5.4268,\n",
      "        5.9246], grad_fn=<AddBackward0>)\n",
      "tensor([1.4450, 1.9427, 2.4404, 2.9381, 3.4358, 3.9335, 4.4313, 4.9290, 5.4267,\n",
      "        5.9244], grad_fn=<AddBackward0>)\n",
      "tensor([1.4454, 1.9430, 2.4407, 2.9383, 3.4360, 3.9336, 4.4313, 4.9289, 5.4265,\n",
      "        5.9242], grad_fn=<AddBackward0>)\n",
      "tensor([1.4457, 1.9433, 2.4409, 2.9385, 3.4361, 3.9337, 4.4312, 4.9288, 5.4264,\n",
      "        5.9240], grad_fn=<AddBackward0>)\n",
      "tensor([1.4461, 1.9436, 2.4411, 2.9387, 3.4362, 3.9337, 4.4312, 4.9288, 5.4263,\n",
      "        5.9238], grad_fn=<AddBackward0>)\n",
      "tensor([1.4465, 1.9439, 2.4414, 2.9389, 3.4363, 3.9338, 4.4312, 4.9287, 5.4262,\n",
      "        5.9236], grad_fn=<AddBackward0>)\n",
      "tensor([1.4468, 1.9442, 2.4416, 2.9390, 3.4364, 3.9338, 4.4312, 4.9286, 5.4261,\n",
      "        5.9235], grad_fn=<AddBackward0>)\n",
      "tensor([1.4472, 1.9445, 2.4419, 2.9392, 3.4366, 3.9339, 4.4312, 4.9286, 5.4259,\n",
      "        5.9233], grad_fn=<AddBackward0>)\n",
      "tensor([1.4475, 1.9448, 2.4421, 2.9394, 3.4367, 3.9340, 4.4312, 4.9285, 5.4258,\n",
      "        5.9231], grad_fn=<AddBackward0>)\n",
      "tensor([1.4479, 1.9451, 2.4423, 2.9396, 3.4368, 3.9340, 4.4312, 4.9285, 5.4257,\n",
      "        5.9229], grad_fn=<AddBackward0>)\n",
      "tensor([1.4482, 1.9454, 2.4426, 2.9397, 3.4369, 3.9341, 4.4312, 4.9284, 5.4256,\n",
      "        5.9227], grad_fn=<AddBackward0>)\n",
      "tensor([1.4486, 1.9457, 2.4428, 2.9399, 3.4370, 3.9341, 4.4312, 4.9283, 5.4254,\n",
      "        5.9225], grad_fn=<AddBackward0>)\n",
      "tensor([1.4489, 1.9460, 2.4430, 2.9401, 3.4371, 3.9342, 4.4312, 4.9283, 5.4253,\n",
      "        5.9224], grad_fn=<AddBackward0>)\n",
      "tensor([1.4493, 1.9463, 2.4433, 2.9403, 3.4373, 3.9342, 4.4312, 4.9282, 5.4252,\n",
      "        5.9222], grad_fn=<AddBackward0>)\n",
      "tensor([1.4496, 1.9466, 2.4435, 2.9404, 3.4374, 3.9343, 4.4312, 4.9282, 5.4251,\n",
      "        5.9220], grad_fn=<AddBackward0>)\n",
      "tensor([1.4500, 1.9469, 2.4437, 2.9406, 3.4375, 3.9344, 4.4312, 4.9281, 5.4250,\n",
      "        5.9218], grad_fn=<AddBackward0>)\n",
      "tensor([1.4503, 1.9472, 2.4440, 2.9408, 3.4376, 3.9344, 4.4312, 4.9280, 5.4248,\n",
      "        5.9217], grad_fn=<AddBackward0>)\n",
      "tensor([1.4507, 1.9474, 2.4442, 2.9410, 3.4377, 3.9345, 4.4312, 4.9280, 5.4247,\n",
      "        5.9215], grad_fn=<AddBackward0>)\n",
      "tensor([1.4510, 1.9477, 2.4444, 2.9411, 3.4378, 3.9345, 4.4312, 4.9279, 5.4246,\n",
      "        5.9213], grad_fn=<AddBackward0>)\n",
      "tensor([1.4514, 1.9480, 2.4446, 2.9413, 3.4379, 3.9346, 4.4312, 4.9279, 5.4245,\n",
      "        5.9211], grad_fn=<AddBackward0>)\n",
      "tensor([1.4517, 1.9483, 2.4449, 2.9415, 3.4380, 3.9346, 4.4312, 4.9278, 5.4244,\n",
      "        5.9210], grad_fn=<AddBackward0>)\n",
      "tensor([1.4520, 1.9486, 2.4451, 2.9416, 3.4382, 3.9347, 4.4312, 4.9277, 5.4243,\n",
      "        5.9208], grad_fn=<AddBackward0>)\n",
      "tensor([1.4524, 1.9489, 2.4453, 2.9418, 3.4383, 3.9347, 4.4312, 4.9277, 5.4241,\n",
      "        5.9206], grad_fn=<AddBackward0>)\n",
      "tensor([1.4527, 1.9491, 2.4455, 2.9420, 3.4384, 3.9348, 4.4312, 4.9276, 5.4240,\n",
      "        5.9204], grad_fn=<AddBackward0>)\n",
      "tensor([1.4531, 1.9494, 2.4458, 2.9421, 3.4385, 3.9348, 4.4312, 4.9276, 5.4239,\n",
      "        5.9203], grad_fn=<AddBackward0>)\n",
      "tensor([1.4534, 1.9497, 2.4460, 2.9423, 3.4386, 3.9349, 4.4312, 4.9275, 5.4238,\n",
      "        5.9201], grad_fn=<AddBackward0>)\n",
      "tensor([1.4537, 1.9500, 2.4462, 2.9425, 3.4387, 3.9350, 4.4312, 4.9274, 5.4237,\n",
      "        5.9199], grad_fn=<AddBackward0>)\n",
      "tensor([1.4540, 1.9502, 2.4464, 2.9426, 3.4388, 3.9350, 4.4312, 4.9274, 5.4236,\n",
      "        5.9198], grad_fn=<AddBackward0>)\n",
      "tensor([1.4544, 1.9505, 2.4466, 2.9428, 3.4389, 3.9351, 4.4312, 4.9273, 5.4235,\n",
      "        5.9196], grad_fn=<AddBackward0>)\n",
      "tensor([1.4547, 1.9508, 2.4469, 2.9429, 3.4390, 3.9351, 4.4312, 4.9273, 5.4234,\n",
      "        5.9194], grad_fn=<AddBackward0>)\n",
      "tensor([1.4550, 1.9511, 2.4471, 2.9431, 3.4391, 3.9352, 4.4312, 4.9272, 5.4232,\n",
      "        5.9193], grad_fn=<AddBackward0>)\n",
      "tensor([1.4554, 1.9513, 2.4473, 2.9433, 3.4392, 3.9352, 4.4312, 4.9272, 5.4231,\n",
      "        5.9191], grad_fn=<AddBackward0>)\n",
      "tensor([1.4557, 1.9516, 2.4475, 2.9434, 3.4393, 3.9353, 4.4312, 4.9271, 5.4230,\n",
      "        5.9189], grad_fn=<AddBackward0>)\n",
      "tensor([1.4560, 1.9519, 2.4477, 2.9436, 3.4395, 3.9353, 4.4312, 4.9270, 5.4229,\n",
      "        5.9188], grad_fn=<AddBackward0>)\n",
      "tensor([1.4563, 1.9521, 2.4479, 2.9437, 3.4396, 3.9354, 4.4312, 4.9270, 5.4228,\n",
      "        5.9186], grad_fn=<AddBackward0>)\n",
      "tensor([1.4566, 1.9524, 2.4482, 2.9439, 3.4397, 3.9354, 4.4312, 4.9269, 5.4227,\n",
      "        5.9185], grad_fn=<AddBackward0>)\n",
      "tensor([1.4570, 1.9527, 2.4484, 2.9441, 3.4398, 3.9355, 4.4312, 4.9269, 5.4226,\n",
      "        5.9183], grad_fn=<AddBackward0>)\n",
      "tensor([1.4573, 1.9529, 2.4486, 2.9442, 3.4399, 3.9355, 4.4312, 4.9268, 5.4225,\n",
      "        5.9181], grad_fn=<AddBackward0>)\n",
      "tensor([1.4576, 1.9532, 2.4488, 2.9444, 3.4400, 3.9356, 4.4312, 4.9268, 5.4224,\n",
      "        5.9180], grad_fn=<AddBackward0>)\n",
      "tensor([1.4579, 1.9534, 2.4490, 2.9445, 3.4401, 3.9356, 4.4312, 4.9267, 5.4223,\n",
      "        5.9178], grad_fn=<AddBackward0>)\n",
      "tensor([1.4582, 1.9537, 2.4492, 2.9447, 3.4402, 3.9357, 4.4312, 4.9267, 5.4222,\n",
      "        5.9176], grad_fn=<AddBackward0>)\n",
      "tensor([1.4585, 1.9540, 2.4494, 2.9448, 3.4403, 3.9357, 4.4312, 4.9266, 5.4221,\n",
      "        5.9175], grad_fn=<AddBackward0>)\n",
      "tensor([1.4588, 1.9542, 2.4496, 2.9450, 3.4404, 3.9358, 4.4312, 4.9266, 5.4219,\n",
      "        5.9173], grad_fn=<AddBackward0>)\n",
      "tensor([1.4591, 1.9545, 2.4498, 2.9452, 3.4405, 3.9358, 4.4312, 4.9265, 5.4218,\n",
      "        5.9172], grad_fn=<AddBackward0>)\n",
      "tensor([1.4594, 1.9547, 2.4500, 2.9453, 3.4406, 3.9359, 4.4312, 4.9264, 5.4217,\n",
      "        5.9170], grad_fn=<AddBackward0>)\n",
      "tensor([1.4598, 1.9550, 2.4502, 2.9455, 3.4407, 3.9359, 4.4312, 4.9264, 5.4216,\n",
      "        5.9169], grad_fn=<AddBackward0>)\n",
      "tensor([1.4601, 1.9552, 2.4504, 2.9456, 3.4408, 3.9360, 4.4312, 4.9263, 5.4215,\n",
      "        5.9167], grad_fn=<AddBackward0>)\n",
      "tensor([1.4604, 1.9555, 2.4506, 2.9458, 3.4409, 3.9360, 4.4312, 4.9263, 5.4214,\n",
      "        5.9166], grad_fn=<AddBackward0>)\n",
      "tensor([1.4607, 1.9557, 2.4508, 2.9459, 3.4410, 3.9361, 4.4312, 4.9262, 5.4213,\n",
      "        5.9164], grad_fn=<AddBackward0>)\n",
      "tensor([1.4610, 1.9560, 2.4510, 2.9461, 3.4411, 3.9361, 4.4312, 4.9262, 5.4212,\n",
      "        5.9162], grad_fn=<AddBackward0>)\n",
      "tensor([1.4613, 1.9562, 2.4512, 2.9462, 3.4412, 3.9362, 4.4312, 4.9261, 5.4211,\n",
      "        5.9161], grad_fn=<AddBackward0>)\n",
      "tensor([1.4616, 1.9565, 2.4514, 2.9464, 3.4413, 3.9362, 4.4311, 4.9261, 5.4210,\n",
      "        5.9159], grad_fn=<AddBackward0>)\n",
      "tensor([1.4619, 1.9567, 2.4516, 2.9465, 3.4414, 3.9363, 4.4311, 4.9260, 5.4209,\n",
      "        5.9158], grad_fn=<AddBackward0>)\n",
      "tensor([1.4622, 1.9570, 2.4518, 2.9466, 3.4415, 3.9363, 4.4311, 4.9260, 5.4208,\n",
      "        5.9156], grad_fn=<AddBackward0>)\n",
      "tensor([1.4624, 1.9572, 2.4520, 2.9468, 3.4416, 3.9364, 4.4311, 4.9259, 5.4207,\n",
      "        5.9155], grad_fn=<AddBackward0>)\n",
      "tensor([1.4627, 1.9575, 2.4522, 2.9469, 3.4417, 3.9364, 4.4311, 4.9259, 5.4206,\n",
      "        5.9153], grad_fn=<AddBackward0>)\n",
      "tensor([1.4630, 1.9577, 2.4524, 2.9471, 3.4418, 3.9365, 4.4311, 4.9258, 5.4205,\n",
      "        5.9152], grad_fn=<AddBackward0>)\n",
      "tensor([1.4633, 1.9580, 2.4526, 2.9472, 3.4419, 3.9365, 4.4311, 4.9258, 5.4204,\n",
      "        5.9150], grad_fn=<AddBackward0>)\n",
      "tensor([1.4636, 1.9582, 2.4528, 2.9474, 3.4420, 3.9365, 4.4311, 4.9257, 5.4203,\n",
      "        5.9149], grad_fn=<AddBackward0>)\n",
      "tensor([1.4639, 1.9584, 2.4530, 2.9475, 3.4421, 3.9366, 4.4311, 4.9257, 5.4202,\n",
      "        5.9148], grad_fn=<AddBackward0>)\n",
      "tensor([1.4642, 1.9587, 2.4532, 2.9477, 3.4422, 3.9366, 4.4311, 4.9256, 5.4201,\n",
      "        5.9146], grad_fn=<AddBackward0>)\n",
      "tensor([1.4645, 1.9589, 2.4534, 2.9478, 3.4422, 3.9367, 4.4311, 4.9256, 5.4200,\n",
      "        5.9145], grad_fn=<AddBackward0>)\n",
      "tensor([1.4648, 1.9592, 2.4535, 2.9479, 3.4423, 3.9367, 4.4311, 4.9255, 5.4199,\n",
      "        5.9143], grad_fn=<AddBackward0>)\n",
      "tensor([1.4650, 1.9594, 2.4537, 2.9481, 3.4424, 3.9368, 4.4311, 4.9255, 5.4198,\n",
      "        5.9142], grad_fn=<AddBackward0>)\n",
      "tensor([1.4653, 1.9596, 2.4539, 2.9482, 3.4425, 3.9368, 4.4311, 4.9254, 5.4197,\n",
      "        5.9140], grad_fn=<AddBackward0>)\n",
      "tensor([1.4656, 1.9599, 2.4541, 2.9484, 3.4426, 3.9369, 4.4311, 4.9254, 5.4196,\n",
      "        5.9139], grad_fn=<AddBackward0>)\n",
      "tensor([1.4659, 1.9601, 2.4543, 2.9485, 3.4427, 3.9369, 4.4311, 4.9253, 5.4195,\n",
      "        5.9137], grad_fn=<AddBackward0>)\n",
      "tensor([1.4662, 1.9603, 2.4545, 2.9486, 3.4428, 3.9370, 4.4311, 4.9253, 5.4194,\n",
      "        5.9136], grad_fn=<AddBackward0>)\n",
      "tensor([1.4664, 1.9606, 2.4547, 2.9488, 3.4429, 3.9370, 4.4311, 4.9252, 5.4193,\n",
      "        5.9135], grad_fn=<AddBackward0>)\n",
      "Epoch :  600 cost :  0.04188404604792595\n",
      "tensor([1.4667, 1.9608, 2.4548, 2.9489, 3.4430, 3.9370, 4.4311, 4.9252, 5.4192,\n",
      "        5.9133], grad_fn=<AddBackward0>)\n",
      "tensor([1.4670, 1.9610, 2.4550, 2.9491, 3.4431, 3.9371, 4.4311, 4.9251, 5.4192,\n",
      "        5.9132], grad_fn=<AddBackward0>)\n",
      "tensor([1.4673, 1.9612, 2.4552, 2.9492, 3.4432, 3.9371, 4.4311, 4.9251, 5.4191,\n",
      "        5.9130], grad_fn=<AddBackward0>)\n",
      "tensor([1.4675, 1.9615, 2.4554, 2.9493, 3.4433, 3.9372, 4.4311, 4.9250, 5.4190,\n",
      "        5.9129], grad_fn=<AddBackward0>)\n",
      "tensor([1.4678, 1.9617, 2.4556, 2.9495, 3.4433, 3.9372, 4.4311, 4.9250, 5.4189,\n",
      "        5.9128], grad_fn=<AddBackward0>)\n",
      "tensor([1.4681, 1.9619, 2.4558, 2.9496, 3.4434, 3.9373, 4.4311, 4.9249, 5.4188,\n",
      "        5.9126], grad_fn=<AddBackward0>)\n",
      "tensor([1.4684, 1.9621, 2.4559, 2.9497, 3.4435, 3.9373, 4.4311, 4.9249, 5.4187,\n",
      "        5.9125], grad_fn=<AddBackward0>)\n",
      "tensor([1.4686, 1.9624, 2.4561, 2.9499, 3.4436, 3.9374, 4.4311, 4.9249, 5.4186,\n",
      "        5.9123], grad_fn=<AddBackward0>)\n",
      "tensor([1.4689, 1.9626, 2.4563, 2.9500, 3.4437, 3.9374, 4.4311, 4.9248, 5.4185,\n",
      "        5.9122], grad_fn=<AddBackward0>)\n",
      "tensor([1.4692, 1.9628, 2.4565, 2.9501, 3.4438, 3.9374, 4.4311, 4.9248, 5.4184,\n",
      "        5.9121], grad_fn=<AddBackward0>)\n",
      "tensor([1.4694, 1.9630, 2.4566, 2.9503, 3.4439, 3.9375, 4.4311, 4.9247, 5.4183,\n",
      "        5.9119], grad_fn=<AddBackward0>)\n",
      "tensor([1.4697, 1.9633, 2.4568, 2.9504, 3.4440, 3.9375, 4.4311, 4.9247, 5.4182,\n",
      "        5.9118], grad_fn=<AddBackward0>)\n",
      "tensor([1.4699, 1.9635, 2.4570, 2.9505, 3.4440, 3.9376, 4.4311, 4.9246, 5.4181,\n",
      "        5.9117], grad_fn=<AddBackward0>)\n",
      "tensor([1.4702, 1.9637, 2.4572, 2.9506, 3.4441, 3.9376, 4.4311, 4.9246, 5.4181,\n",
      "        5.9115], grad_fn=<AddBackward0>)\n",
      "tensor([1.4705, 1.9639, 2.4573, 2.9508, 3.4442, 3.9377, 4.4311, 4.9245, 5.4180,\n",
      "        5.9114], grad_fn=<AddBackward0>)\n",
      "tensor([1.4707, 1.9641, 2.4575, 2.9509, 3.4443, 3.9377, 4.4311, 4.9245, 5.4179,\n",
      "        5.9113], grad_fn=<AddBackward0>)\n",
      "tensor([1.4710, 1.9643, 2.4577, 2.9510, 3.4444, 3.9377, 4.4311, 4.9244, 5.4178,\n",
      "        5.9111], grad_fn=<AddBackward0>)\n",
      "tensor([1.4712, 1.9645, 2.4579, 2.9512, 3.4445, 3.9378, 4.4311, 4.9244, 5.4177,\n",
      "        5.9110], grad_fn=<AddBackward0>)\n",
      "tensor([1.4715, 1.9648, 2.4580, 2.9513, 3.4446, 3.9378, 4.4311, 4.9243, 5.4176,\n",
      "        5.9109], grad_fn=<AddBackward0>)\n",
      "tensor([1.4718, 1.9650, 2.4582, 2.9514, 3.4446, 3.9379, 4.4311, 4.9243, 5.4175,\n",
      "        5.9107], grad_fn=<AddBackward0>)\n",
      "tensor([1.4720, 1.9652, 2.4584, 2.9515, 3.4447, 3.9379, 4.4311, 4.9243, 5.4174,\n",
      "        5.9106], grad_fn=<AddBackward0>)\n",
      "tensor([1.4723, 1.9654, 2.4585, 2.9517, 3.4448, 3.9379, 4.4311, 4.9242, 5.4174,\n",
      "        5.9105], grad_fn=<AddBackward0>)\n",
      "tensor([1.4725, 1.9656, 2.4587, 2.9518, 3.4449, 3.9380, 4.4311, 4.9242, 5.4173,\n",
      "        5.9104], grad_fn=<AddBackward0>)\n",
      "tensor([1.4728, 1.9658, 2.4589, 2.9519, 3.4450, 3.9380, 4.4311, 4.9241, 5.4172,\n",
      "        5.9102], grad_fn=<AddBackward0>)\n",
      "tensor([1.4730, 1.9660, 2.4590, 2.9520, 3.4451, 3.9381, 4.4311, 4.9241, 5.4171,\n",
      "        5.9101], grad_fn=<AddBackward0>)\n",
      "tensor([1.4733, 1.9662, 2.4592, 2.9522, 3.4451, 3.9381, 4.4311, 4.9240, 5.4170,\n",
      "        5.9100], grad_fn=<AddBackward0>)\n",
      "tensor([1.4735, 1.9664, 2.4594, 2.9523, 3.4452, 3.9381, 4.4311, 4.9240, 5.4169,\n",
      "        5.9099], grad_fn=<AddBackward0>)\n",
      "tensor([1.4738, 1.9666, 2.4595, 2.9524, 3.4453, 3.9382, 4.4311, 4.9240, 5.4168,\n",
      "        5.9097], grad_fn=<AddBackward0>)\n",
      "tensor([1.4740, 1.9668, 2.4597, 2.9525, 3.4454, 3.9382, 4.4311, 4.9239, 5.4168,\n",
      "        5.9096], grad_fn=<AddBackward0>)\n",
      "tensor([1.4742, 1.9670, 2.4599, 2.9527, 3.4455, 3.9383, 4.4311, 4.9239, 5.4167,\n",
      "        5.9095], grad_fn=<AddBackward0>)\n",
      "tensor([1.4745, 1.9673, 2.4600, 2.9528, 3.4455, 3.9383, 4.4311, 4.9238, 5.4166,\n",
      "        5.9094], grad_fn=<AddBackward0>)\n",
      "tensor([1.4747, 1.9675, 2.4602, 2.9529, 3.4456, 3.9383, 4.4311, 4.9238, 5.4165,\n",
      "        5.9092], grad_fn=<AddBackward0>)\n",
      "tensor([1.4750, 1.9677, 2.4603, 2.9530, 3.4457, 3.9384, 4.4311, 4.9237, 5.4164,\n",
      "        5.9091], grad_fn=<AddBackward0>)\n",
      "tensor([1.4752, 1.9679, 2.4605, 2.9531, 3.4458, 3.9384, 4.4311, 4.9237, 5.4163,\n",
      "        5.9090], grad_fn=<AddBackward0>)\n",
      "tensor([1.4755, 1.9681, 2.4607, 2.9533, 3.4459, 3.9385, 4.4311, 4.9237, 5.4163,\n",
      "        5.9089], grad_fn=<AddBackward0>)\n",
      "tensor([1.4757, 1.9683, 2.4608, 2.9534, 3.4459, 3.9385, 4.4311, 4.9236, 5.4162,\n",
      "        5.9087], grad_fn=<AddBackward0>)\n",
      "tensor([1.4759, 1.9685, 2.4610, 2.9535, 3.4460, 3.9385, 4.4311, 4.9236, 5.4161,\n",
      "        5.9086], grad_fn=<AddBackward0>)\n",
      "tensor([1.4762, 1.9686, 2.4611, 2.9536, 3.4461, 3.9386, 4.4311, 4.9235, 5.4160,\n",
      "        5.9085], grad_fn=<AddBackward0>)\n",
      "tensor([1.4764, 1.9688, 2.4613, 2.9537, 3.4462, 3.9386, 4.4311, 4.9235, 5.4159,\n",
      "        5.9084], grad_fn=<AddBackward0>)\n",
      "tensor([1.4766, 1.9690, 2.4614, 2.9538, 3.4462, 3.9386, 4.4311, 4.9235, 5.4159,\n",
      "        5.9083], grad_fn=<AddBackward0>)\n",
      "tensor([1.4769, 1.9692, 2.4616, 2.9540, 3.4463, 3.9387, 4.4311, 4.9234, 5.4158,\n",
      "        5.9081], grad_fn=<AddBackward0>)\n",
      "tensor([1.4771, 1.9694, 2.4618, 2.9541, 3.4464, 3.9387, 4.4310, 4.9234, 5.4157,\n",
      "        5.9080], grad_fn=<AddBackward0>)\n",
      "tensor([1.4773, 1.9696, 2.4619, 2.9542, 3.4465, 3.9388, 4.4310, 4.9233, 5.4156,\n",
      "        5.9079], grad_fn=<AddBackward0>)\n",
      "tensor([1.4776, 1.9698, 2.4621, 2.9543, 3.4466, 3.9388, 4.4310, 4.9233, 5.4155,\n",
      "        5.9078], grad_fn=<AddBackward0>)\n",
      "tensor([1.4778, 1.9700, 2.4622, 2.9544, 3.4466, 3.9388, 4.4310, 4.9233, 5.4155,\n",
      "        5.9077], grad_fn=<AddBackward0>)\n",
      "tensor([1.4780, 1.9702, 2.4624, 2.9545, 3.4467, 3.9389, 4.4310, 4.9232, 5.4154,\n",
      "        5.9076], grad_fn=<AddBackward0>)\n",
      "tensor([1.4783, 1.9704, 2.4625, 2.9546, 3.4468, 3.9389, 4.4310, 4.9232, 5.4153,\n",
      "        5.9074], grad_fn=<AddBackward0>)\n",
      "tensor([1.4785, 1.9706, 2.4627, 2.9548, 3.4469, 3.9389, 4.4310, 4.9231, 5.4152,\n",
      "        5.9073], grad_fn=<AddBackward0>)\n",
      "tensor([1.4787, 1.9708, 2.4628, 2.9549, 3.4469, 3.9390, 4.4310, 4.9231, 5.4152,\n",
      "        5.9072], grad_fn=<AddBackward0>)\n",
      "tensor([1.4789, 1.9709, 2.4630, 2.9550, 3.4470, 3.9390, 4.4310, 4.9231, 5.4151,\n",
      "        5.9071], grad_fn=<AddBackward0>)\n",
      "tensor([1.4792, 1.9711, 2.4631, 2.9551, 3.4471, 3.9391, 4.4310, 4.9230, 5.4150,\n",
      "        5.9070], grad_fn=<AddBackward0>)\n",
      "tensor([1.4794, 1.9713, 2.4633, 2.9552, 3.4471, 3.9391, 4.4310, 4.9230, 5.4149,\n",
      "        5.9069], grad_fn=<AddBackward0>)\n",
      "tensor([1.4796, 1.9715, 2.4634, 2.9553, 3.4472, 3.9391, 4.4310, 4.9229, 5.4148,\n",
      "        5.9068], grad_fn=<AddBackward0>)\n",
      "tensor([1.4798, 1.9717, 2.4636, 2.9554, 3.4473, 3.9392, 4.4310, 4.9229, 5.4148,\n",
      "        5.9066], grad_fn=<AddBackward0>)\n",
      "tensor([1.4800, 1.9719, 2.4637, 2.9555, 3.4474, 3.9392, 4.4310, 4.9229, 5.4147,\n",
      "        5.9065], grad_fn=<AddBackward0>)\n",
      "tensor([1.4803, 1.9721, 2.4638, 2.9556, 3.4474, 3.9392, 4.4310, 4.9228, 5.4146,\n",
      "        5.9064], grad_fn=<AddBackward0>)\n",
      "tensor([1.4805, 1.9722, 2.4640, 2.9558, 3.4475, 3.9393, 4.4310, 4.9228, 5.4145,\n",
      "        5.9063], grad_fn=<AddBackward0>)\n",
      "tensor([1.4807, 1.9724, 2.4641, 2.9559, 3.4476, 3.9393, 4.4310, 4.9227, 5.4145,\n",
      "        5.9062], grad_fn=<AddBackward0>)\n",
      "tensor([1.4809, 1.9726, 2.4643, 2.9560, 3.4477, 3.9393, 4.4310, 4.9227, 5.4144,\n",
      "        5.9061], grad_fn=<AddBackward0>)\n",
      "tensor([1.4811, 1.9728, 2.4644, 2.9561, 3.4477, 3.9394, 4.4310, 4.9227, 5.4143,\n",
      "        5.9060], grad_fn=<AddBackward0>)\n",
      "tensor([1.4813, 1.9730, 2.4646, 2.9562, 3.4478, 3.9394, 4.4310, 4.9226, 5.4142,\n",
      "        5.9059], grad_fn=<AddBackward0>)\n",
      "tensor([1.4816, 1.9731, 2.4647, 2.9563, 3.4479, 3.9394, 4.4310, 4.9226, 5.4142,\n",
      "        5.9058], grad_fn=<AddBackward0>)\n",
      "tensor([1.4818, 1.9733, 2.4649, 2.9564, 3.4479, 3.9395, 4.4310, 4.9226, 5.4141,\n",
      "        5.9056], grad_fn=<AddBackward0>)\n",
      "tensor([1.4820, 1.9735, 2.4650, 2.9565, 3.4480, 3.9395, 4.4310, 4.9225, 5.4140,\n",
      "        5.9055], grad_fn=<AddBackward0>)\n",
      "tensor([1.4822, 1.9737, 2.4651, 2.9566, 3.4481, 3.9395, 4.4310, 4.9225, 5.4140,\n",
      "        5.9054], grad_fn=<AddBackward0>)\n",
      "tensor([1.4824, 1.9738, 2.4653, 2.9567, 3.4481, 3.9396, 4.4310, 4.9225, 5.4139,\n",
      "        5.9053], grad_fn=<AddBackward0>)\n",
      "tensor([1.4826, 1.9740, 2.4654, 2.9568, 3.4482, 3.9396, 4.4310, 4.9224, 5.4138,\n",
      "        5.9052], grad_fn=<AddBackward0>)\n",
      "tensor([1.4828, 1.9742, 2.4655, 2.9569, 3.4483, 3.9396, 4.4310, 4.9224, 5.4137,\n",
      "        5.9051], grad_fn=<AddBackward0>)\n",
      "tensor([1.4830, 1.9744, 2.4657, 2.9570, 3.4483, 3.9397, 4.4310, 4.9223, 5.4137,\n",
      "        5.9050], grad_fn=<AddBackward0>)\n",
      "tensor([1.4832, 1.9745, 2.4658, 2.9571, 3.4484, 3.9397, 4.4310, 4.9223, 5.4136,\n",
      "        5.9049], grad_fn=<AddBackward0>)\n",
      "tensor([1.4834, 1.9747, 2.4660, 2.9572, 3.4485, 3.9397, 4.4310, 4.9223, 5.4135,\n",
      "        5.9048], grad_fn=<AddBackward0>)\n",
      "tensor([1.4836, 1.9749, 2.4661, 2.9573, 3.4486, 3.9398, 4.4310, 4.9222, 5.4135,\n",
      "        5.9047], grad_fn=<AddBackward0>)\n",
      "tensor([1.4838, 1.9750, 2.4662, 2.9574, 3.4486, 3.9398, 4.4310, 4.9222, 5.4134,\n",
      "        5.9046], grad_fn=<AddBackward0>)\n",
      "tensor([1.4841, 1.9752, 2.4664, 2.9575, 3.4487, 3.9398, 4.4310, 4.9222, 5.4133,\n",
      "        5.9045], grad_fn=<AddBackward0>)\n",
      "tensor([1.4843, 1.9754, 2.4665, 2.9576, 3.4488, 3.9399, 4.4310, 4.9221, 5.4133,\n",
      "        5.9044], grad_fn=<AddBackward0>)\n",
      "tensor([1.4845, 1.9755, 2.4666, 2.9577, 3.4488, 3.9399, 4.4310, 4.9221, 5.4132,\n",
      "        5.9043], grad_fn=<AddBackward0>)\n",
      "tensor([1.4847, 1.9757, 2.4668, 2.9578, 3.4489, 3.9399, 4.4310, 4.9221, 5.4131,\n",
      "        5.9042], grad_fn=<AddBackward0>)\n",
      "tensor([1.4849, 1.9759, 2.4669, 2.9579, 3.4490, 3.9400, 4.4310, 4.9220, 5.4130,\n",
      "        5.9041], grad_fn=<AddBackward0>)\n",
      "tensor([1.4851, 1.9760, 2.4670, 2.9580, 3.4490, 3.9400, 4.4310, 4.9220, 5.4130,\n",
      "        5.9040], grad_fn=<AddBackward0>)\n",
      "tensor([1.4853, 1.9762, 2.4672, 2.9581, 3.4491, 3.9400, 4.4310, 4.9220, 5.4129,\n",
      "        5.9039], grad_fn=<AddBackward0>)\n",
      "tensor([1.4855, 1.9764, 2.4673, 2.9582, 3.4491, 3.9401, 4.4310, 4.9219, 5.4128,\n",
      "        5.9038], grad_fn=<AddBackward0>)\n",
      "tensor([1.4856, 1.9765, 2.4674, 2.9583, 3.4492, 3.9401, 4.4310, 4.9219, 5.4128,\n",
      "        5.9037], grad_fn=<AddBackward0>)\n",
      "tensor([1.4858, 1.9767, 2.4676, 2.9584, 3.4493, 3.9401, 4.4310, 4.9219, 5.4127,\n",
      "        5.9036], grad_fn=<AddBackward0>)\n",
      "tensor([1.4860, 1.9769, 2.4677, 2.9585, 3.4493, 3.9402, 4.4310, 4.9218, 5.4126,\n",
      "        5.9035], grad_fn=<AddBackward0>)\n",
      "tensor([1.4862, 1.9770, 2.4678, 2.9586, 3.4494, 3.9402, 4.4310, 4.9218, 5.4126,\n",
      "        5.9034], grad_fn=<AddBackward0>)\n",
      "tensor([1.4864, 1.9772, 2.4679, 2.9587, 3.4495, 3.9402, 4.4310, 4.9218, 5.4125,\n",
      "        5.9033], grad_fn=<AddBackward0>)\n",
      "tensor([1.4866, 1.9773, 2.4681, 2.9588, 3.4495, 3.9403, 4.4310, 4.9217, 5.4124,\n",
      "        5.9032], grad_fn=<AddBackward0>)\n",
      "tensor([1.4868, 1.9775, 2.4682, 2.9589, 3.4496, 3.9403, 4.4310, 4.9217, 5.4124,\n",
      "        5.9031], grad_fn=<AddBackward0>)\n",
      "tensor([1.4870, 1.9777, 2.4683, 2.9590, 3.4497, 3.9403, 4.4310, 4.9216, 5.4123,\n",
      "        5.9030], grad_fn=<AddBackward0>)\n",
      "tensor([1.4872, 1.9778, 2.4685, 2.9591, 3.4497, 3.9404, 4.4310, 4.9216, 5.4122,\n",
      "        5.9029], grad_fn=<AddBackward0>)\n",
      "tensor([1.4874, 1.9780, 2.4686, 2.9592, 3.4498, 3.9404, 4.4310, 4.9216, 5.4122,\n",
      "        5.9028], grad_fn=<AddBackward0>)\n",
      "tensor([1.4876, 1.9781, 2.4687, 2.9593, 3.4498, 3.9404, 4.4310, 4.9216, 5.4121,\n",
      "        5.9027], grad_fn=<AddBackward0>)\n",
      "tensor([1.4878, 1.9783, 2.4688, 2.9594, 3.4499, 3.9404, 4.4310, 4.9215, 5.4121,\n",
      "        5.9026], grad_fn=<AddBackward0>)\n",
      "tensor([1.4879, 1.9784, 2.4690, 2.9595, 3.4500, 3.9405, 4.4310, 4.9215, 5.4120,\n",
      "        5.9025], grad_fn=<AddBackward0>)\n",
      "tensor([1.4881, 1.9786, 2.4691, 2.9596, 3.4500, 3.9405, 4.4310, 4.9215, 5.4119,\n",
      "        5.9024], grad_fn=<AddBackward0>)\n",
      "tensor([1.4883, 1.9788, 2.4692, 2.9596, 3.4501, 3.9405, 4.4310, 4.9214, 5.4119,\n",
      "        5.9023], grad_fn=<AddBackward0>)\n",
      "tensor([1.4885, 1.9789, 2.4693, 2.9597, 3.4502, 3.9406, 4.4310, 4.9214, 5.4118,\n",
      "        5.9022], grad_fn=<AddBackward0>)\n",
      "tensor([1.4887, 1.9791, 2.4694, 2.9598, 3.4502, 3.9406, 4.4310, 4.9214, 5.4117,\n",
      "        5.9021], grad_fn=<AddBackward0>)\n",
      "tensor([1.4889, 1.9792, 2.4696, 2.9599, 3.4503, 3.9406, 4.4310, 4.9213, 5.4117,\n",
      "        5.9020], grad_fn=<AddBackward0>)\n",
      "tensor([1.4890, 1.9794, 2.4697, 2.9600, 3.4503, 3.9407, 4.4310, 4.9213, 5.4116,\n",
      "        5.9019], grad_fn=<AddBackward0>)\n",
      "Epoch :  700 cost :  0.04116382822394371\n",
      "tensor([1.4892, 1.9795, 2.4698, 2.9601, 3.4504, 3.9407, 4.4310, 4.9213, 5.4116,\n",
      "        5.9018], grad_fn=<AddBackward0>)\n",
      "tensor([1.4894, 1.9797, 2.4699, 2.9602, 3.4504, 3.9407, 4.4310, 4.9212, 5.4115,\n",
      "        5.9018], grad_fn=<AddBackward0>)\n",
      "tensor([1.4896, 1.9798, 2.4700, 2.9603, 3.4505, 3.9407, 4.4310, 4.9212, 5.4114,\n",
      "        5.9017], grad_fn=<AddBackward0>)\n",
      "tensor([1.4898, 1.9800, 2.4702, 2.9604, 3.4506, 3.9408, 4.4310, 4.9212, 5.4114,\n",
      "        5.9016], grad_fn=<AddBackward0>)\n",
      "tensor([1.4899, 1.9801, 2.4703, 2.9605, 3.4506, 3.9408, 4.4310, 4.9211, 5.4113,\n",
      "        5.9015], grad_fn=<AddBackward0>)\n",
      "tensor([1.4901, 1.9803, 2.4704, 2.9605, 3.4507, 3.9408, 4.4310, 4.9211, 5.4112,\n",
      "        5.9014], grad_fn=<AddBackward0>)\n",
      "tensor([1.4903, 1.9804, 2.4705, 2.9606, 3.4507, 3.9409, 4.4310, 4.9211, 5.4112,\n",
      "        5.9013], grad_fn=<AddBackward0>)\n",
      "tensor([1.4905, 1.9806, 2.4706, 2.9607, 3.4508, 3.9409, 4.4310, 4.9210, 5.4111,\n",
      "        5.9012], grad_fn=<AddBackward0>)\n",
      "tensor([1.4907, 1.9807, 2.4708, 2.9608, 3.4509, 3.9409, 4.4310, 4.9210, 5.4111,\n",
      "        5.9011], grad_fn=<AddBackward0>)\n",
      "tensor([1.4908, 1.9809, 2.4709, 2.9609, 3.4509, 3.9409, 4.4310, 4.9210, 5.4110,\n",
      "        5.9010], grad_fn=<AddBackward0>)\n",
      "tensor([1.4910, 1.9810, 2.4710, 2.9610, 3.4510, 3.9410, 4.4310, 4.9210, 5.4109,\n",
      "        5.9009], grad_fn=<AddBackward0>)\n",
      "tensor([1.4912, 1.9811, 2.4711, 2.9611, 3.4510, 3.9410, 4.4310, 4.9209, 5.4109,\n",
      "        5.9009], grad_fn=<AddBackward0>)\n",
      "tensor([1.4913, 1.9813, 2.4712, 2.9612, 3.4511, 3.9410, 4.4310, 4.9209, 5.4108,\n",
      "        5.9008], grad_fn=<AddBackward0>)\n",
      "tensor([1.4915, 1.9814, 2.4713, 2.9612, 3.4511, 3.9411, 4.4310, 4.9209, 5.4108,\n",
      "        5.9007], grad_fn=<AddBackward0>)\n",
      "tensor([1.4917, 1.9816, 2.4714, 2.9613, 3.4512, 3.9411, 4.4310, 4.9208, 5.4107,\n",
      "        5.9006], grad_fn=<AddBackward0>)\n",
      "tensor([1.4919, 1.9817, 2.4716, 2.9614, 3.4513, 3.9411, 4.4310, 4.9208, 5.4107,\n",
      "        5.9005], grad_fn=<AddBackward0>)\n",
      "tensor([1.4920, 1.9819, 2.4717, 2.9615, 3.4513, 3.9411, 4.4310, 4.9208, 5.4106,\n",
      "        5.9004], grad_fn=<AddBackward0>)\n",
      "tensor([1.4922, 1.9820, 2.4718, 2.9616, 3.4514, 3.9412, 4.4310, 4.9207, 5.4105,\n",
      "        5.9003], grad_fn=<AddBackward0>)\n",
      "tensor([1.4924, 1.9821, 2.4719, 2.9617, 3.4514, 3.9412, 4.4310, 4.9207, 5.4105,\n",
      "        5.9002], grad_fn=<AddBackward0>)\n",
      "tensor([1.4925, 1.9823, 2.4720, 2.9617, 3.4515, 3.9412, 4.4310, 4.9207, 5.4104,\n",
      "        5.9002], grad_fn=<AddBackward0>)\n",
      "tensor([1.4927, 1.9824, 2.4721, 2.9618, 3.4515, 3.9412, 4.4309, 4.9207, 5.4104,\n",
      "        5.9001], grad_fn=<AddBackward0>)\n",
      "tensor([1.4929, 1.9825, 2.4722, 2.9619, 3.4516, 3.9413, 4.4309, 4.9206, 5.4103,\n",
      "        5.9000], grad_fn=<AddBackward0>)\n",
      "tensor([1.4930, 1.9827, 2.4723, 2.9620, 3.4516, 3.9413, 4.4309, 4.9206, 5.4103,\n",
      "        5.8999], grad_fn=<AddBackward0>)\n",
      "tensor([1.4932, 1.9828, 2.4724, 2.9621, 3.4517, 3.9413, 4.4309, 4.9206, 5.4102,\n",
      "        5.8998], grad_fn=<AddBackward0>)\n",
      "tensor([1.4934, 1.9830, 2.4726, 2.9622, 3.4518, 3.9413, 4.4309, 4.9205, 5.4101,\n",
      "        5.8997], grad_fn=<AddBackward0>)\n",
      "tensor([1.4935, 1.9831, 2.4727, 2.9622, 3.4518, 3.9414, 4.4309, 4.9205, 5.4101,\n",
      "        5.8997], grad_fn=<AddBackward0>)\n",
      "tensor([1.4937, 1.9832, 2.4728, 2.9623, 3.4519, 3.9414, 4.4309, 4.9205, 5.4100,\n",
      "        5.8996], grad_fn=<AddBackward0>)\n",
      "tensor([1.4939, 1.9834, 2.4729, 2.9624, 3.4519, 3.9414, 4.4309, 4.9205, 5.4100,\n",
      "        5.8995], grad_fn=<AddBackward0>)\n",
      "tensor([1.4940, 1.9835, 2.4730, 2.9625, 3.4520, 3.9415, 4.4309, 4.9204, 5.4099,\n",
      "        5.8994], grad_fn=<AddBackward0>)\n",
      "tensor([1.4942, 1.9836, 2.4731, 2.9626, 3.4520, 3.9415, 4.4309, 4.9204, 5.4099,\n",
      "        5.8993], grad_fn=<AddBackward0>)\n",
      "tensor([1.4943, 1.9838, 2.4732, 2.9626, 3.4521, 3.9415, 4.4309, 4.9204, 5.4098,\n",
      "        5.8992], grad_fn=<AddBackward0>)\n",
      "tensor([1.4945, 1.9839, 2.4733, 2.9627, 3.4521, 3.9415, 4.4309, 4.9203, 5.4098,\n",
      "        5.8992], grad_fn=<AddBackward0>)\n",
      "tensor([1.4946, 1.9840, 2.4734, 2.9628, 3.4522, 3.9416, 4.4309, 4.9203, 5.4097,\n",
      "        5.8991], grad_fn=<AddBackward0>)\n",
      "tensor([1.4948, 1.9842, 2.4735, 2.9629, 3.4522, 3.9416, 4.4309, 4.9203, 5.4096,\n",
      "        5.8990], grad_fn=<AddBackward0>)\n",
      "tensor([1.4950, 1.9843, 2.4736, 2.9629, 3.4523, 3.9416, 4.4309, 4.9203, 5.4096,\n",
      "        5.8989], grad_fn=<AddBackward0>)\n",
      "tensor([1.4951, 1.9844, 2.4737, 2.9630, 3.4523, 3.9416, 4.4309, 4.9202, 5.4095,\n",
      "        5.8988], grad_fn=<AddBackward0>)\n",
      "tensor([1.4953, 1.9846, 2.4738, 2.9631, 3.4524, 3.9417, 4.4309, 4.9202, 5.4095,\n",
      "        5.8988], grad_fn=<AddBackward0>)\n",
      "tensor([1.4954, 1.9847, 2.4739, 2.9632, 3.4524, 3.9417, 4.4309, 4.9202, 5.4094,\n",
      "        5.8987], grad_fn=<AddBackward0>)\n",
      "tensor([1.4956, 1.9848, 2.4740, 2.9633, 3.4525, 3.9417, 4.4309, 4.9202, 5.4094,\n",
      "        5.8986], grad_fn=<AddBackward0>)\n",
      "tensor([1.4957, 1.9849, 2.4741, 2.9633, 3.4525, 3.9417, 4.4309, 4.9201, 5.4093,\n",
      "        5.8985], grad_fn=<AddBackward0>)\n",
      "tensor([1.4959, 1.9851, 2.4742, 2.9634, 3.4526, 3.9418, 4.4309, 4.9201, 5.4093,\n",
      "        5.8984], grad_fn=<AddBackward0>)\n",
      "tensor([1.4960, 1.9852, 2.4743, 2.9635, 3.4526, 3.9418, 4.4309, 4.9201, 5.4092,\n",
      "        5.8984], grad_fn=<AddBackward0>)\n",
      "tensor([1.4962, 1.9853, 2.4744, 2.9636, 3.4527, 3.9418, 4.4309, 4.9200, 5.4092,\n",
      "        5.8983], grad_fn=<AddBackward0>)\n",
      "tensor([1.4964, 1.9854, 2.4745, 2.9636, 3.4527, 3.9418, 4.4309, 4.9200, 5.4091,\n",
      "        5.8982], grad_fn=<AddBackward0>)\n",
      "tensor([1.4965, 1.9856, 2.4746, 2.9637, 3.4528, 3.9419, 4.4309, 4.9200, 5.4091,\n",
      "        5.8981], grad_fn=<AddBackward0>)\n",
      "tensor([1.4967, 1.9857, 2.4747, 2.9638, 3.4528, 3.9419, 4.4309, 4.9200, 5.4090,\n",
      "        5.8981], grad_fn=<AddBackward0>)\n",
      "tensor([1.4968, 1.9858, 2.4748, 2.9639, 3.4529, 3.9419, 4.4309, 4.9199, 5.4090,\n",
      "        5.8980], grad_fn=<AddBackward0>)\n",
      "tensor([1.4970, 1.9859, 2.4749, 2.9639, 3.4529, 3.9419, 4.4309, 4.9199, 5.4089,\n",
      "        5.8979], grad_fn=<AddBackward0>)\n",
      "tensor([1.4971, 1.9861, 2.4750, 2.9640, 3.4530, 3.9420, 4.4309, 4.9199, 5.4089,\n",
      "        5.8978], grad_fn=<AddBackward0>)\n",
      "tensor([1.4972, 1.9862, 2.4751, 2.9641, 3.4530, 3.9420, 4.4309, 4.9199, 5.4088,\n",
      "        5.8978], grad_fn=<AddBackward0>)\n",
      "tensor([1.4974, 1.9863, 2.4752, 2.9642, 3.4531, 3.9420, 4.4309, 4.9198, 5.4088,\n",
      "        5.8977], grad_fn=<AddBackward0>)\n",
      "tensor([1.4975, 1.9864, 2.4753, 2.9642, 3.4531, 3.9420, 4.4309, 4.9198, 5.4087,\n",
      "        5.8976], grad_fn=<AddBackward0>)\n",
      "tensor([1.4977, 1.9866, 2.4754, 2.9643, 3.4532, 3.9420, 4.4309, 4.9198, 5.4087,\n",
      "        5.8975], grad_fn=<AddBackward0>)\n",
      "tensor([1.4978, 1.9867, 2.4755, 2.9644, 3.4532, 3.9421, 4.4309, 4.9198, 5.4086,\n",
      "        5.8975], grad_fn=<AddBackward0>)\n",
      "tensor([1.4980, 1.9868, 2.4756, 2.9644, 3.4533, 3.9421, 4.4309, 4.9197, 5.4086,\n",
      "        5.8974], grad_fn=<AddBackward0>)\n",
      "tensor([1.4981, 1.9869, 2.4757, 2.9645, 3.4533, 3.9421, 4.4309, 4.9197, 5.4085,\n",
      "        5.8973], grad_fn=<AddBackward0>)\n",
      "tensor([1.4983, 1.9870, 2.4758, 2.9646, 3.4534, 3.9421, 4.4309, 4.9197, 5.4085,\n",
      "        5.8972], grad_fn=<AddBackward0>)\n",
      "tensor([1.4984, 1.9872, 2.4759, 2.9647, 3.4534, 3.9422, 4.4309, 4.9197, 5.4084,\n",
      "        5.8972], grad_fn=<AddBackward0>)\n",
      "tensor([1.4985, 1.9873, 2.4760, 2.9647, 3.4535, 3.9422, 4.4309, 4.9196, 5.4084,\n",
      "        5.8971], grad_fn=<AddBackward0>)\n",
      "tensor([1.4987, 1.9874, 2.4761, 2.9648, 3.4535, 3.9422, 4.4309, 4.9196, 5.4083,\n",
      "        5.8970], grad_fn=<AddBackward0>)\n",
      "tensor([1.4988, 1.9875, 2.4762, 2.9649, 3.4535, 3.9422, 4.4309, 4.9196, 5.4083,\n",
      "        5.8969], grad_fn=<AddBackward0>)\n",
      "tensor([1.4990, 1.9876, 2.4763, 2.9649, 3.4536, 3.9423, 4.4309, 4.9196, 5.4082,\n",
      "        5.8969], grad_fn=<AddBackward0>)\n",
      "tensor([1.4991, 1.9877, 2.4764, 2.9650, 3.4536, 3.9423, 4.4309, 4.9195, 5.4082,\n",
      "        5.8968], grad_fn=<AddBackward0>)\n",
      "tensor([1.4992, 1.9879, 2.4765, 2.9651, 3.4537, 3.9423, 4.4309, 4.9195, 5.4081,\n",
      "        5.8967], grad_fn=<AddBackward0>)\n",
      "tensor([1.4994, 1.9880, 2.4766, 2.9651, 3.4537, 3.9423, 4.4309, 4.9195, 5.4081,\n",
      "        5.8967], grad_fn=<AddBackward0>)\n",
      "tensor([1.4995, 1.9881, 2.4767, 2.9652, 3.4538, 3.9423, 4.4309, 4.9195, 5.4080,\n",
      "        5.8966], grad_fn=<AddBackward0>)\n",
      "tensor([1.4997, 1.9882, 2.4767, 2.9653, 3.4538, 3.9424, 4.4309, 4.9194, 5.4080,\n",
      "        5.8965], grad_fn=<AddBackward0>)\n",
      "tensor([1.4998, 1.9883, 2.4768, 2.9654, 3.4539, 3.9424, 4.4309, 4.9194, 5.4079,\n",
      "        5.8965], grad_fn=<AddBackward0>)\n",
      "tensor([1.4999, 1.9884, 2.4769, 2.9654, 3.4539, 3.9424, 4.4309, 4.9194, 5.4079,\n",
      "        5.8964], grad_fn=<AddBackward0>)\n",
      "tensor([1.5001, 1.9885, 2.4770, 2.9655, 3.4540, 3.9424, 4.4309, 4.9194, 5.4078,\n",
      "        5.8963], grad_fn=<AddBackward0>)\n",
      "tensor([1.5002, 1.9887, 2.4771, 2.9656, 3.4540, 3.9425, 4.4309, 4.9194, 5.4078,\n",
      "        5.8962], grad_fn=<AddBackward0>)\n",
      "tensor([1.5003, 1.9888, 2.4772, 2.9656, 3.4540, 3.9425, 4.4309, 4.9193, 5.4078,\n",
      "        5.8962], grad_fn=<AddBackward0>)\n",
      "tensor([1.5005, 1.9889, 2.4773, 2.9657, 3.4541, 3.9425, 4.4309, 4.9193, 5.4077,\n",
      "        5.8961], grad_fn=<AddBackward0>)\n",
      "tensor([1.5006, 1.9890, 2.4774, 2.9658, 3.4541, 3.9425, 4.4309, 4.9193, 5.4077,\n",
      "        5.8960], grad_fn=<AddBackward0>)\n",
      "tensor([1.5007, 1.9891, 2.4775, 2.9658, 3.4542, 3.9425, 4.4309, 4.9193, 5.4076,\n",
      "        5.8960], grad_fn=<AddBackward0>)\n",
      "tensor([1.5009, 1.9892, 2.4775, 2.9659, 3.4542, 3.9426, 4.4309, 4.9192, 5.4076,\n",
      "        5.8959], grad_fn=<AddBackward0>)\n",
      "tensor([1.5010, 1.9893, 2.4776, 2.9660, 3.4543, 3.9426, 4.4309, 4.9192, 5.4075,\n",
      "        5.8958], grad_fn=<AddBackward0>)\n",
      "tensor([1.5011, 1.9894, 2.4777, 2.9660, 3.4543, 3.9426, 4.4309, 4.9192, 5.4075,\n",
      "        5.8958], grad_fn=<AddBackward0>)\n",
      "tensor([1.5013, 1.9895, 2.4778, 2.9661, 3.4544, 3.9426, 4.4309, 4.9192, 5.4074,\n",
      "        5.8957], grad_fn=<AddBackward0>)\n",
      "tensor([1.5014, 1.9896, 2.4779, 2.9661, 3.4544, 3.9426, 4.4309, 4.9191, 5.4074,\n",
      "        5.8956], grad_fn=<AddBackward0>)\n",
      "tensor([1.5015, 1.9898, 2.4780, 2.9662, 3.4544, 3.9427, 4.4309, 4.9191, 5.4073,\n",
      "        5.8956], grad_fn=<AddBackward0>)\n",
      "tensor([1.5017, 1.9899, 2.4781, 2.9663, 3.4545, 3.9427, 4.4309, 4.9191, 5.4073,\n",
      "        5.8955], grad_fn=<AddBackward0>)\n",
      "tensor([1.5018, 1.9900, 2.4782, 2.9663, 3.4545, 3.9427, 4.4309, 4.9191, 5.4073,\n",
      "        5.8954], grad_fn=<AddBackward0>)\n",
      "tensor([1.5019, 1.9901, 2.4782, 2.9664, 3.4546, 3.9427, 4.4309, 4.9191, 5.4072,\n",
      "        5.8954], grad_fn=<AddBackward0>)\n",
      "tensor([1.5020, 1.9902, 2.4783, 2.9665, 3.4546, 3.9427, 4.4309, 4.9190, 5.4072,\n",
      "        5.8953], grad_fn=<AddBackward0>)\n",
      "tensor([1.5022, 1.9903, 2.4784, 2.9665, 3.4546, 3.9428, 4.4309, 4.9190, 5.4071,\n",
      "        5.8952], grad_fn=<AddBackward0>)\n",
      "tensor([1.5023, 1.9904, 2.4785, 2.9666, 3.4547, 3.9428, 4.4309, 4.9190, 5.4071,\n",
      "        5.8952], grad_fn=<AddBackward0>)\n",
      "tensor([1.5024, 1.9905, 2.4786, 2.9667, 3.4547, 3.9428, 4.4309, 4.9190, 5.4070,\n",
      "        5.8951], grad_fn=<AddBackward0>)\n",
      "tensor([1.5025, 1.9906, 2.4787, 2.9667, 3.4548, 3.9428, 4.4309, 4.9189, 5.4070,\n",
      "        5.8951], grad_fn=<AddBackward0>)\n",
      "tensor([1.5027, 1.9907, 2.4787, 2.9668, 3.4548, 3.9428, 4.4309, 4.9189, 5.4070,\n",
      "        5.8950], grad_fn=<AddBackward0>)\n",
      "tensor([1.5028, 1.9908, 2.4788, 2.9668, 3.4549, 3.9429, 4.4309, 4.9189, 5.4069,\n",
      "        5.8949], grad_fn=<AddBackward0>)\n",
      "tensor([1.5029, 1.9909, 2.4789, 2.9669, 3.4549, 3.9429, 4.4309, 4.9189, 5.4069,\n",
      "        5.8949], grad_fn=<AddBackward0>)\n",
      "tensor([1.5030, 1.9910, 2.4790, 2.9670, 3.4549, 3.9429, 4.4309, 4.9189, 5.4068,\n",
      "        5.8948], grad_fn=<AddBackward0>)\n",
      "tensor([1.5032, 1.9911, 2.4791, 2.9670, 3.4550, 3.9429, 4.4309, 4.9188, 5.4068,\n",
      "        5.8947], grad_fn=<AddBackward0>)\n",
      "tensor([1.5033, 1.9912, 2.4792, 2.9671, 3.4550, 3.9429, 4.4309, 4.9188, 5.4067,\n",
      "        5.8947], grad_fn=<AddBackward0>)\n",
      "tensor([1.5034, 1.9913, 2.4792, 2.9671, 3.4551, 3.9430, 4.4309, 4.9188, 5.4067,\n",
      "        5.8946], grad_fn=<AddBackward0>)\n",
      "tensor([1.5035, 1.9914, 2.4793, 2.9672, 3.4551, 3.9430, 4.4309, 4.9188, 5.4067,\n",
      "        5.8946], grad_fn=<AddBackward0>)\n",
      "tensor([1.5036, 1.9915, 2.4794, 2.9673, 3.4551, 3.9430, 4.4309, 4.9188, 5.4066,\n",
      "        5.8945], grad_fn=<AddBackward0>)\n",
      "tensor([1.5038, 1.9916, 2.4795, 2.9673, 3.4552, 3.9430, 4.4309, 4.9187, 5.4066,\n",
      "        5.8944], grad_fn=<AddBackward0>)\n",
      "tensor([1.5039, 1.9917, 2.4796, 2.9674, 3.4552, 3.9430, 4.4309, 4.9187, 5.4065,\n",
      "        5.8944], grad_fn=<AddBackward0>)\n",
      "Epoch :  800 cost :  0.04085339978337288\n",
      "tensor([1.5040, 1.9918, 2.4796, 2.9674, 3.4553, 3.9431, 4.4309, 4.9187, 5.4065,\n",
      "        5.8943], grad_fn=<AddBackward0>)\n",
      "tensor([1.5041, 1.9919, 2.4797, 2.9675, 3.4553, 3.9431, 4.4309, 4.9187, 5.4065,\n",
      "        5.8943], grad_fn=<AddBackward0>)\n",
      "tensor([1.5042, 1.9920, 2.4798, 2.9676, 3.4553, 3.9431, 4.4309, 4.9186, 5.4064,\n",
      "        5.8942], grad_fn=<AddBackward0>)\n",
      "tensor([1.5044, 1.9921, 2.4799, 2.9676, 3.4554, 3.9431, 4.4309, 4.9186, 5.4064,\n",
      "        5.8941], grad_fn=<AddBackward0>)\n",
      "tensor([1.5045, 1.9922, 2.4799, 2.9677, 3.4554, 3.9431, 4.4309, 4.9186, 5.4063,\n",
      "        5.8941], grad_fn=<AddBackward0>)\n",
      "tensor([1.5046, 1.9923, 2.4800, 2.9677, 3.4554, 3.9432, 4.4309, 4.9186, 5.4063,\n",
      "        5.8940], grad_fn=<AddBackward0>)\n",
      "tensor([1.5047, 1.9924, 2.4801, 2.9678, 3.4555, 3.9432, 4.4309, 4.9186, 5.4063,\n",
      "        5.8940], grad_fn=<AddBackward0>)\n",
      "tensor([1.5048, 1.9925, 2.4802, 2.9678, 3.4555, 3.9432, 4.4309, 4.9185, 5.4062,\n",
      "        5.8939], grad_fn=<AddBackward0>)\n",
      "tensor([1.5049, 1.9926, 2.4803, 2.9679, 3.4556, 3.9432, 4.4309, 4.9185, 5.4062,\n",
      "        5.8938], grad_fn=<AddBackward0>)\n",
      "tensor([1.5051, 1.9927, 2.4803, 2.9680, 3.4556, 3.9432, 4.4309, 4.9185, 5.4061,\n",
      "        5.8938], grad_fn=<AddBackward0>)\n",
      "tensor([1.5052, 1.9928, 2.4804, 2.9680, 3.4556, 3.9433, 4.4309, 4.9185, 5.4061,\n",
      "        5.8937], grad_fn=<AddBackward0>)\n",
      "tensor([1.5053, 1.9929, 2.4805, 2.9681, 3.4557, 3.9433, 4.4309, 4.9185, 5.4061,\n",
      "        5.8937], grad_fn=<AddBackward0>)\n",
      "tensor([1.5054, 1.9930, 2.4806, 2.9681, 3.4557, 3.9433, 4.4309, 4.9184, 5.4060,\n",
      "        5.8936], grad_fn=<AddBackward0>)\n",
      "tensor([1.5055, 1.9931, 2.4806, 2.9682, 3.4557, 3.9433, 4.4309, 4.9184, 5.4060,\n",
      "        5.8935], grad_fn=<AddBackward0>)\n",
      "tensor([1.5056, 1.9932, 2.4807, 2.9682, 3.4558, 3.9433, 4.4309, 4.9184, 5.4059,\n",
      "        5.8935], grad_fn=<AddBackward0>)\n",
      "tensor([1.5057, 1.9933, 2.4808, 2.9683, 3.4558, 3.9433, 4.4309, 4.9184, 5.4059,\n",
      "        5.8934], grad_fn=<AddBackward0>)\n",
      "tensor([1.5058, 1.9934, 2.4809, 2.9684, 3.4559, 3.9434, 4.4309, 4.9184, 5.4059,\n",
      "        5.8934], grad_fn=<AddBackward0>)\n",
      "tensor([1.5060, 1.9934, 2.4809, 2.9684, 3.4559, 3.9434, 4.4309, 4.9183, 5.4058,\n",
      "        5.8933], grad_fn=<AddBackward0>)\n",
      "tensor([1.5061, 1.9935, 2.4810, 2.9685, 3.4559, 3.9434, 4.4309, 4.9183, 5.4058,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        5.8933], grad_fn=<AddBackward0>)\n",
      "tensor([1.5062, 1.9936, 2.4811, 2.9685, 3.4560, 3.9434, 4.4309, 4.9183, 5.4058,\n",
      "        5.8932], grad_fn=<AddBackward0>)\n",
      "tensor([1.5063, 1.9937, 2.4811, 2.9686, 3.4560, 3.9434, 4.4309, 4.9183, 5.4057,\n",
      "        5.8931], grad_fn=<AddBackward0>)\n",
      "tensor([1.5064, 1.9938, 2.4812, 2.9686, 3.4560, 3.9435, 4.4309, 4.9183, 5.4057,\n",
      "        5.8931], grad_fn=<AddBackward0>)\n",
      "tensor([1.5065, 1.9939, 2.4813, 2.9687, 3.4561, 3.9435, 4.4309, 4.9183, 5.4056,\n",
      "        5.8930], grad_fn=<AddBackward0>)\n",
      "tensor([1.5066, 1.9940, 2.4814, 2.9687, 3.4561, 3.9435, 4.4309, 4.9182, 5.4056,\n",
      "        5.8930], grad_fn=<AddBackward0>)\n",
      "tensor([1.5067, 1.9941, 2.4814, 2.9688, 3.4561, 3.9435, 4.4309, 4.9182, 5.4056,\n",
      "        5.8929], grad_fn=<AddBackward0>)\n",
      "tensor([1.5068, 1.9942, 2.4815, 2.9688, 3.4562, 3.9435, 4.4309, 4.9182, 5.4055,\n",
      "        5.8929], grad_fn=<AddBackward0>)\n",
      "tensor([1.5069, 1.9943, 2.4816, 2.9689, 3.4562, 3.9435, 4.4309, 4.9182, 5.4055,\n",
      "        5.8928], grad_fn=<AddBackward0>)\n",
      "tensor([1.5070, 1.9943, 2.4816, 2.9689, 3.4563, 3.9436, 4.4309, 4.9182, 5.4055,\n",
      "        5.8928], grad_fn=<AddBackward0>)\n",
      "tensor([1.5071, 1.9944, 2.4817, 2.9690, 3.4563, 3.9436, 4.4309, 4.9181, 5.4054,\n",
      "        5.8927], grad_fn=<AddBackward0>)\n",
      "tensor([1.5073, 1.9945, 2.4818, 2.9691, 3.4563, 3.9436, 4.4309, 4.9181, 5.4054,\n",
      "        5.8927], grad_fn=<AddBackward0>)\n",
      "tensor([1.5074, 1.9946, 2.4819, 2.9691, 3.4564, 3.9436, 4.4309, 4.9181, 5.4054,\n",
      "        5.8926], grad_fn=<AddBackward0>)\n",
      "tensor([1.5075, 1.9947, 2.4819, 2.9692, 3.4564, 3.9436, 4.4309, 4.9181, 5.4053,\n",
      "        5.8926], grad_fn=<AddBackward0>)\n",
      "tensor([1.5076, 1.9948, 2.4820, 2.9692, 3.4564, 3.9436, 4.4309, 4.9181, 5.4053,\n",
      "        5.8925], grad_fn=<AddBackward0>)\n",
      "tensor([1.5077, 1.9949, 2.4821, 2.9693, 3.4565, 3.9437, 4.4309, 4.9181, 5.4052,\n",
      "        5.8924], grad_fn=<AddBackward0>)\n",
      "tensor([1.5078, 1.9950, 2.4821, 2.9693, 3.4565, 3.9437, 4.4309, 4.9180, 5.4052,\n",
      "        5.8924], grad_fn=<AddBackward0>)\n",
      "tensor([1.5079, 1.9950, 2.4822, 2.9694, 3.4565, 3.9437, 4.4309, 4.9180, 5.4052,\n",
      "        5.8923], grad_fn=<AddBackward0>)\n",
      "tensor([1.5080, 1.9951, 2.4823, 2.9694, 3.4566, 3.9437, 4.4309, 4.9180, 5.4051,\n",
      "        5.8923], grad_fn=<AddBackward0>)\n",
      "tensor([1.5081, 1.9952, 2.4823, 2.9695, 3.4566, 3.9437, 4.4309, 4.9180, 5.4051,\n",
      "        5.8922], grad_fn=<AddBackward0>)\n",
      "tensor([1.5082, 1.9953, 2.4824, 2.9695, 3.4566, 3.9437, 4.4309, 4.9180, 5.4051,\n",
      "        5.8922], grad_fn=<AddBackward0>)\n",
      "tensor([1.5083, 1.9954, 2.4825, 2.9696, 3.4567, 3.9438, 4.4308, 4.9179, 5.4050,\n",
      "        5.8921], grad_fn=<AddBackward0>)\n",
      "tensor([1.5084, 1.9955, 2.4825, 2.9696, 3.4567, 3.9438, 4.4308, 4.9179, 5.4050,\n",
      "        5.8921], grad_fn=<AddBackward0>)\n",
      "tensor([1.5085, 1.9955, 2.4826, 2.9697, 3.4567, 3.9438, 4.4308, 4.9179, 5.4050,\n",
      "        5.8920], grad_fn=<AddBackward0>)\n",
      "tensor([1.5086, 1.9956, 2.4827, 2.9697, 3.4568, 3.9438, 4.4308, 4.9179, 5.4049,\n",
      "        5.8920], grad_fn=<AddBackward0>)\n",
      "tensor([1.5087, 1.9957, 2.4827, 2.9698, 3.4568, 3.9438, 4.4308, 4.9179, 5.4049,\n",
      "        5.8919], grad_fn=<AddBackward0>)\n",
      "tensor([1.5088, 1.9958, 2.4828, 2.9698, 3.4568, 3.9438, 4.4308, 4.9179, 5.4049,\n",
      "        5.8919], grad_fn=<AddBackward0>)\n",
      "tensor([1.5089, 1.9959, 2.4829, 2.9699, 3.4569, 3.9439, 4.4308, 4.9178, 5.4048,\n",
      "        5.8918], grad_fn=<AddBackward0>)\n",
      "tensor([1.5090, 1.9960, 2.4829, 2.9699, 3.4569, 3.9439, 4.4308, 4.9178, 5.4048,\n",
      "        5.8918], grad_fn=<AddBackward0>)\n",
      "tensor([1.5091, 1.9960, 2.4830, 2.9700, 3.4569, 3.9439, 4.4308, 4.9178, 5.4048,\n",
      "        5.8917], grad_fn=<AddBackward0>)\n",
      "tensor([1.5092, 1.9961, 2.4831, 2.9700, 3.4570, 3.9439, 4.4308, 4.9178, 5.4047,\n",
      "        5.8917], grad_fn=<AddBackward0>)\n",
      "tensor([1.5093, 1.9962, 2.4831, 2.9701, 3.4570, 3.9439, 4.4308, 4.9178, 5.4047,\n",
      "        5.8916], grad_fn=<AddBackward0>)\n",
      "tensor([1.5094, 1.9963, 2.4832, 2.9701, 3.4570, 3.9439, 4.4308, 4.9178, 5.4047,\n",
      "        5.8916], grad_fn=<AddBackward0>)\n",
      "tensor([1.5095, 1.9964, 2.4833, 2.9702, 3.4570, 3.9439, 4.4308, 4.9177, 5.4046,\n",
      "        5.8915], grad_fn=<AddBackward0>)\n",
      "tensor([1.5096, 1.9964, 2.4833, 2.9702, 3.4571, 3.9440, 4.4308, 4.9177, 5.4046,\n",
      "        5.8915], grad_fn=<AddBackward0>)\n",
      "tensor([1.5097, 1.9965, 2.4834, 2.9702, 3.4571, 3.9440, 4.4308, 4.9177, 5.4046,\n",
      "        5.8914], grad_fn=<AddBackward0>)\n",
      "tensor([1.5097, 1.9966, 2.4834, 2.9703, 3.4571, 3.9440, 4.4308, 4.9177, 5.4045,\n",
      "        5.8914], grad_fn=<AddBackward0>)\n",
      "tensor([1.5098, 1.9967, 2.4835, 2.9703, 3.4572, 3.9440, 4.4308, 4.9177, 5.4045,\n",
      "        5.8913], grad_fn=<AddBackward0>)\n",
      "tensor([1.5099, 1.9968, 2.4836, 2.9704, 3.4572, 3.9440, 4.4308, 4.9177, 5.4045,\n",
      "        5.8913], grad_fn=<AddBackward0>)\n",
      "tensor([1.5100, 1.9968, 2.4836, 2.9704, 3.4572, 3.9440, 4.4308, 4.9176, 5.4044,\n",
      "        5.8912], grad_fn=<AddBackward0>)\n",
      "tensor([1.5101, 1.9969, 2.4837, 2.9705, 3.4573, 3.9441, 4.4308, 4.9176, 5.4044,\n",
      "        5.8912], grad_fn=<AddBackward0>)\n",
      "tensor([1.5102, 1.9970, 2.4838, 2.9705, 3.4573, 3.9441, 4.4308, 4.9176, 5.4044,\n",
      "        5.8911], grad_fn=<AddBackward0>)\n",
      "tensor([1.5103, 1.9971, 2.4838, 2.9706, 3.4573, 3.9441, 4.4308, 4.9176, 5.4043,\n",
      "        5.8911], grad_fn=<AddBackward0>)\n",
      "tensor([1.5104, 1.9971, 2.4839, 2.9706, 3.4574, 3.9441, 4.4308, 4.9176, 5.4043,\n",
      "        5.8911], grad_fn=<AddBackward0>)\n",
      "tensor([1.5105, 1.9972, 2.4839, 2.9707, 3.4574, 3.9441, 4.4308, 4.9176, 5.4043,\n",
      "        5.8910], grad_fn=<AddBackward0>)\n",
      "tensor([1.5106, 1.9973, 2.4840, 2.9707, 3.4574, 3.9441, 4.4308, 4.9175, 5.4043,\n",
      "        5.8910], grad_fn=<AddBackward0>)\n",
      "tensor([1.5107, 1.9974, 2.4841, 2.9708, 3.4574, 3.9441, 4.4308, 4.9175, 5.4042,\n",
      "        5.8909], grad_fn=<AddBackward0>)\n",
      "tensor([1.5108, 1.9974, 2.4841, 2.9708, 3.4575, 3.9442, 4.4308, 4.9175, 5.4042,\n",
      "        5.8909], grad_fn=<AddBackward0>)\n",
      "tensor([1.5109, 1.9975, 2.4842, 2.9708, 3.4575, 3.9442, 4.4308, 4.9175, 5.4042,\n",
      "        5.8908], grad_fn=<AddBackward0>)\n",
      "tensor([1.5109, 1.9976, 2.4842, 2.9709, 3.4575, 3.9442, 4.4308, 4.9175, 5.4041,\n",
      "        5.8908], grad_fn=<AddBackward0>)\n",
      "tensor([1.5110, 1.9977, 2.4843, 2.9709, 3.4576, 3.9442, 4.4308, 4.9175, 5.4041,\n",
      "        5.8907], grad_fn=<AddBackward0>)\n",
      "tensor([1.5111, 1.9977, 2.4844, 2.9710, 3.4576, 3.9442, 4.4308, 4.9174, 5.4041,\n",
      "        5.8907], grad_fn=<AddBackward0>)\n",
      "tensor([1.5112, 1.9978, 2.4844, 2.9710, 3.4576, 3.9442, 4.4308, 4.9174, 5.4040,\n",
      "        5.8906], grad_fn=<AddBackward0>)\n",
      "tensor([1.5113, 1.9979, 2.4845, 2.9711, 3.4577, 3.9442, 4.4308, 4.9174, 5.4040,\n",
      "        5.8906], grad_fn=<AddBackward0>)\n",
      "tensor([1.5114, 1.9980, 2.4845, 2.9711, 3.4577, 3.9443, 4.4308, 4.9174, 5.4040,\n",
      "        5.8905], grad_fn=<AddBackward0>)\n",
      "tensor([1.5115, 1.9980, 2.4846, 2.9712, 3.4577, 3.9443, 4.4308, 4.9174, 5.4039,\n",
      "        5.8905], grad_fn=<AddBackward0>)\n",
      "tensor([1.5116, 1.9981, 2.4847, 2.9712, 3.4577, 3.9443, 4.4308, 4.9174, 5.4039,\n",
      "        5.8905], grad_fn=<AddBackward0>)\n",
      "tensor([1.5117, 1.9982, 2.4847, 2.9712, 3.4578, 3.9443, 4.4308, 4.9174, 5.4039,\n",
      "        5.8904], grad_fn=<AddBackward0>)\n",
      "tensor([1.5117, 1.9983, 2.4848, 2.9713, 3.4578, 3.9443, 4.4308, 4.9173, 5.4039,\n",
      "        5.8904], grad_fn=<AddBackward0>)\n",
      "tensor([1.5118, 1.9983, 2.4848, 2.9713, 3.4578, 3.9443, 4.4308, 4.9173, 5.4038,\n",
      "        5.8903], grad_fn=<AddBackward0>)\n",
      "tensor([1.5119, 1.9984, 2.4849, 2.9714, 3.4579, 3.9443, 4.4308, 4.9173, 5.4038,\n",
      "        5.8903], grad_fn=<AddBackward0>)\n",
      "tensor([1.5120, 1.9985, 2.4849, 2.9714, 3.4579, 3.9444, 4.4308, 4.9173, 5.4038,\n",
      "        5.8902], grad_fn=<AddBackward0>)\n",
      "tensor([1.5121, 1.9985, 2.4850, 2.9715, 3.4579, 3.9444, 4.4308, 4.9173, 5.4037,\n",
      "        5.8902], grad_fn=<AddBackward0>)\n",
      "tensor([1.5122, 1.9986, 2.4851, 2.9715, 3.4579, 3.9444, 4.4308, 4.9173, 5.4037,\n",
      "        5.8902], grad_fn=<AddBackward0>)\n",
      "tensor([1.5123, 1.9987, 2.4851, 2.9715, 3.4580, 3.9444, 4.4308, 4.9173, 5.4037,\n",
      "        5.8901], grad_fn=<AddBackward0>)\n",
      "tensor([1.5123, 1.9987, 2.4852, 2.9716, 3.4580, 3.9444, 4.4308, 4.9172, 5.4037,\n",
      "        5.8901], grad_fn=<AddBackward0>)\n",
      "tensor([1.5124, 1.9988, 2.4852, 2.9716, 3.4580, 3.9444, 4.4308, 4.9172, 5.4036,\n",
      "        5.8900], grad_fn=<AddBackward0>)\n",
      "tensor([1.5125, 1.9989, 2.4853, 2.9717, 3.4580, 3.9444, 4.4308, 4.9172, 5.4036,\n",
      "        5.8900], grad_fn=<AddBackward0>)\n",
      "tensor([1.5126, 1.9990, 2.4853, 2.9717, 3.4581, 3.9444, 4.4308, 4.9172, 5.4036,\n",
      "        5.8899], grad_fn=<AddBackward0>)\n",
      "tensor([1.5127, 1.9990, 2.4854, 2.9717, 3.4581, 3.9445, 4.4308, 4.9172, 5.4035,\n",
      "        5.8899], grad_fn=<AddBackward0>)\n",
      "tensor([1.5127, 1.9991, 2.4854, 2.9718, 3.4581, 3.9445, 4.4308, 4.9172, 5.4035,\n",
      "        5.8899], grad_fn=<AddBackward0>)\n",
      "tensor([1.5128, 1.9992, 2.4855, 2.9718, 3.4582, 3.9445, 4.4308, 4.9172, 5.4035,\n",
      "        5.8898], grad_fn=<AddBackward0>)\n",
      "tensor([1.5129, 1.9992, 2.4855, 2.9719, 3.4582, 3.9445, 4.4308, 4.9171, 5.4035,\n",
      "        5.8898], grad_fn=<AddBackward0>)\n",
      "tensor([1.5130, 1.9993, 2.4856, 2.9719, 3.4582, 3.9445, 4.4308, 4.9171, 5.4034,\n",
      "        5.8897], grad_fn=<AddBackward0>)\n",
      "tensor([1.5131, 1.9994, 2.4857, 2.9719, 3.4582, 3.9445, 4.4308, 4.9171, 5.4034,\n",
      "        5.8897], grad_fn=<AddBackward0>)\n",
      "tensor([1.5132, 1.9994, 2.4857, 2.9720, 3.4583, 3.9445, 4.4308, 4.9171, 5.4034,\n",
      "        5.8896], grad_fn=<AddBackward0>)\n",
      "tensor([1.5132, 1.9995, 2.4858, 2.9720, 3.4583, 3.9446, 4.4308, 4.9171, 5.4033,\n",
      "        5.8896], grad_fn=<AddBackward0>)\n",
      "tensor([1.5133, 1.9996, 2.4858, 2.9721, 3.4583, 3.9446, 4.4308, 4.9171, 5.4033,\n",
      "        5.8896], grad_fn=<AddBackward0>)\n",
      "tensor([1.5134, 1.9996, 2.4859, 2.9721, 3.4583, 3.9446, 4.4308, 4.9171, 5.4033,\n",
      "        5.8895], grad_fn=<AddBackward0>)\n",
      "tensor([1.5135, 1.9997, 2.4859, 2.9721, 3.4584, 3.9446, 4.4308, 4.9170, 5.4033,\n",
      "        5.8895], grad_fn=<AddBackward0>)\n",
      "tensor([1.5136, 1.9998, 2.4860, 2.9722, 3.4584, 3.9446, 4.4308, 4.9170, 5.4032,\n",
      "        5.8894], grad_fn=<AddBackward0>)\n",
      "tensor([1.5136, 1.9998, 2.4860, 2.9722, 3.4584, 3.9446, 4.4308, 4.9170, 5.4032,\n",
      "        5.8894], grad_fn=<AddBackward0>)\n",
      "Epoch :  900 cost :  0.04071960970759392\n",
      "tensor([1.5137, 1.9999, 2.4861, 2.9723, 3.4584, 3.9446, 4.4308, 4.9170, 5.4032,\n",
      "        5.8894], grad_fn=<AddBackward0>)\n",
      "tensor([1.5138, 2.0000, 2.4861, 2.9723, 3.4585, 3.9446, 4.4308, 4.9170, 5.4032,\n",
      "        5.8893], grad_fn=<AddBackward0>)\n",
      "tensor([1.5139, 2.0000, 2.4862, 2.9723, 3.4585, 3.9447, 4.4308, 4.9170, 5.4031,\n",
      "        5.8893], grad_fn=<AddBackward0>)\n",
      "tensor([1.5139, 2.0001, 2.4862, 2.9724, 3.4585, 3.9447, 4.4308, 4.9170, 5.4031,\n",
      "        5.8892], grad_fn=<AddBackward0>)\n",
      "tensor([1.5140, 2.0002, 2.4863, 2.9724, 3.4585, 3.9447, 4.4308, 4.9169, 5.4031,\n",
      "        5.8892], grad_fn=<AddBackward0>)\n",
      "tensor([1.5141, 2.0002, 2.4863, 2.9725, 3.4586, 3.9447, 4.4308, 4.9169, 5.4031,\n",
      "        5.8892], grad_fn=<AddBackward0>)\n",
      "tensor([1.5142, 2.0003, 2.4864, 2.9725, 3.4586, 3.9447, 4.4308, 4.9169, 5.4030,\n",
      "        5.8891], grad_fn=<AddBackward0>)\n",
      "tensor([1.5142, 2.0003, 2.4864, 2.9725, 3.4586, 3.9447, 4.4308, 4.9169, 5.4030,\n",
      "        5.8891], grad_fn=<AddBackward0>)\n",
      "tensor([1.5143, 2.0004, 2.4865, 2.9726, 3.4586, 3.9447, 4.4308, 4.9169, 5.4030,\n",
      "        5.8891], grad_fn=<AddBackward0>)\n",
      "tensor([1.5144, 2.0005, 2.4865, 2.9726, 3.4587, 3.9447, 4.4308, 4.9169, 5.4029,\n",
      "        5.8890], grad_fn=<AddBackward0>)\n",
      "tensor([1.5145, 2.0005, 2.4866, 2.9726, 3.4587, 3.9448, 4.4308, 4.9169, 5.4029,\n",
      "        5.8890], grad_fn=<AddBackward0>)\n",
      "tensor([1.5145, 2.0006, 2.4866, 2.9727, 3.4587, 3.9448, 4.4308, 4.9169, 5.4029,\n",
      "        5.8889], grad_fn=<AddBackward0>)\n",
      "tensor([1.5146, 2.0007, 2.4867, 2.9727, 3.4587, 3.9448, 4.4308, 4.9168, 5.4029,\n",
      "        5.8889], grad_fn=<AddBackward0>)\n",
      "tensor([1.5147, 2.0007, 2.4867, 2.9728, 3.4588, 3.9448, 4.4308, 4.9168, 5.4028,\n",
      "        5.8889], grad_fn=<AddBackward0>)\n",
      "tensor([1.5148, 2.0008, 2.4868, 2.9728, 3.4588, 3.9448, 4.4308, 4.9168, 5.4028,\n",
      "        5.8888], grad_fn=<AddBackward0>)\n",
      "tensor([1.5148, 2.0008, 2.4868, 2.9728, 3.4588, 3.9448, 4.4308, 4.9168, 5.4028,\n",
      "        5.8888], grad_fn=<AddBackward0>)\n",
      "tensor([1.5149, 2.0009, 2.4869, 2.9729, 3.4588, 3.9448, 4.4308, 4.9168, 5.4028,\n",
      "        5.8888], grad_fn=<AddBackward0>)\n",
      "tensor([1.5150, 2.0010, 2.4869, 2.9729, 3.4589, 3.9448, 4.4308, 4.9168, 5.4027,\n",
      "        5.8887], grad_fn=<AddBackward0>)\n",
      "tensor([1.5151, 2.0010, 2.4870, 2.9729, 3.4589, 3.9448, 4.4308, 4.9168, 5.4027,\n",
      "        5.8887], grad_fn=<AddBackward0>)\n",
      "tensor([1.5151, 2.0011, 2.4870, 2.9730, 3.4589, 3.9449, 4.4308, 4.9168, 5.4027,\n",
      "        5.8886], grad_fn=<AddBackward0>)\n",
      "tensor([1.5152, 2.0011, 2.4871, 2.9730, 3.4589, 3.9449, 4.4308, 4.9167, 5.4027,\n",
      "        5.8886], grad_fn=<AddBackward0>)\n",
      "tensor([1.5153, 2.0012, 2.4871, 2.9730, 3.4590, 3.9449, 4.4308, 4.9167, 5.4026,\n",
      "        5.8886], grad_fn=<AddBackward0>)\n",
      "tensor([1.5153, 2.0013, 2.4872, 2.9731, 3.4590, 3.9449, 4.4308, 4.9167, 5.4026,\n",
      "        5.8885], grad_fn=<AddBackward0>)\n",
      "tensor([1.5154, 2.0013, 2.4872, 2.9731, 3.4590, 3.9449, 4.4308, 4.9167, 5.4026,\n",
      "        5.8885], grad_fn=<AddBackward0>)\n",
      "tensor([1.5155, 2.0014, 2.4873, 2.9731, 3.4590, 3.9449, 4.4308, 4.9167, 5.4026,\n",
      "        5.8885], grad_fn=<AddBackward0>)\n",
      "tensor([1.5156, 2.0014, 2.4873, 2.9732, 3.4591, 3.9449, 4.4308, 4.9167, 5.4025,\n",
      "        5.8884], grad_fn=<AddBackward0>)\n",
      "tensor([1.5156, 2.0015, 2.4874, 2.9732, 3.4591, 3.9449, 4.4308, 4.9167, 5.4025,\n",
      "        5.8884], grad_fn=<AddBackward0>)\n",
      "tensor([1.5157, 2.0016, 2.4874, 2.9733, 3.4591, 3.9450, 4.4308, 4.9167, 5.4025,\n",
      "        5.8884], grad_fn=<AddBackward0>)\n",
      "tensor([1.5158, 2.0016, 2.4874, 2.9733, 3.4591, 3.9450, 4.4308, 4.9166, 5.4025,\n",
      "        5.8883], grad_fn=<AddBackward0>)\n",
      "tensor([1.5158, 2.0017, 2.4875, 2.9733, 3.4591, 3.9450, 4.4308, 4.9166, 5.4025,\n",
      "        5.8883], grad_fn=<AddBackward0>)\n",
      "tensor([1.5159, 2.0017, 2.4875, 2.9734, 3.4592, 3.9450, 4.4308, 4.9166, 5.4024,\n",
      "        5.8882], grad_fn=<AddBackward0>)\n",
      "tensor([1.5160, 2.0018, 2.4876, 2.9734, 3.4592, 3.9450, 4.4308, 4.9166, 5.4024,\n",
      "        5.8882], grad_fn=<AddBackward0>)\n",
      "tensor([1.5160, 2.0018, 2.4876, 2.9734, 3.4592, 3.9450, 4.4308, 4.9166, 5.4024,\n",
      "        5.8882], grad_fn=<AddBackward0>)\n",
      "tensor([1.5161, 2.0019, 2.4877, 2.9735, 3.4592, 3.9450, 4.4308, 4.9166, 5.4024,\n",
      "        5.8881], grad_fn=<AddBackward0>)\n",
      "tensor([1.5162, 2.0020, 2.4877, 2.9735, 3.4593, 3.9450, 4.4308, 4.9166, 5.4023,\n",
      "        5.8881], grad_fn=<AddBackward0>)\n",
      "tensor([1.5162, 2.0020, 2.4878, 2.9735, 3.4593, 3.9450, 4.4308, 4.9166, 5.4023,\n",
      "        5.8881], grad_fn=<AddBackward0>)\n",
      "tensor([1.5163, 2.0021, 2.4878, 2.9736, 3.4593, 3.9451, 4.4308, 4.9165, 5.4023,\n",
      "        5.8880], grad_fn=<AddBackward0>)\n",
      "tensor([1.5164, 2.0021, 2.4879, 2.9736, 3.4593, 3.9451, 4.4308, 4.9165, 5.4023,\n",
      "        5.8880], grad_fn=<AddBackward0>)\n",
      "tensor([1.5165, 2.0022, 2.4879, 2.9736, 3.4593, 3.9451, 4.4308, 4.9165, 5.4022,\n",
      "        5.8880], grad_fn=<AddBackward0>)\n",
      "tensor([1.5165, 2.0022, 2.4879, 2.9737, 3.4594, 3.9451, 4.4308, 4.9165, 5.4022,\n",
      "        5.8879], grad_fn=<AddBackward0>)\n",
      "tensor([1.5166, 2.0023, 2.4880, 2.9737, 3.4594, 3.9451, 4.4308, 4.9165, 5.4022,\n",
      "        5.8879], grad_fn=<AddBackward0>)\n",
      "tensor([1.5166, 2.0023, 2.4880, 2.9737, 3.4594, 3.9451, 4.4308, 4.9165, 5.4022,\n",
      "        5.8879], grad_fn=<AddBackward0>)\n",
      "tensor([1.5167, 2.0024, 2.4881, 2.9738, 3.4594, 3.9451, 4.4308, 4.9165, 5.4022,\n",
      "        5.8878], grad_fn=<AddBackward0>)\n",
      "tensor([1.5168, 2.0024, 2.4881, 2.9738, 3.4595, 3.9451, 4.4308, 4.9165, 5.4021,\n",
      "        5.8878], grad_fn=<AddBackward0>)\n",
      "tensor([1.5168, 2.0025, 2.4882, 2.9738, 3.4595, 3.9451, 4.4308, 4.9165, 5.4021,\n",
      "        5.8878], grad_fn=<AddBackward0>)\n",
      "tensor([1.5169, 2.0026, 2.4882, 2.9739, 3.4595, 3.9451, 4.4308, 4.9164, 5.4021,\n",
      "        5.8877], grad_fn=<AddBackward0>)\n",
      "tensor([1.5170, 2.0026, 2.4882, 2.9739, 3.4595, 3.9452, 4.4308, 4.9164, 5.4021,\n",
      "        5.8877], grad_fn=<AddBackward0>)\n",
      "tensor([1.5170, 2.0027, 2.4883, 2.9739, 3.4595, 3.9452, 4.4308, 4.9164, 5.4020,\n",
      "        5.8877], grad_fn=<AddBackward0>)\n",
      "tensor([1.5171, 2.0027, 2.4883, 2.9739, 3.4596, 3.9452, 4.4308, 4.9164, 5.4020,\n",
      "        5.8876], grad_fn=<AddBackward0>)\n",
      "tensor([1.5172, 2.0028, 2.4884, 2.9740, 3.4596, 3.9452, 4.4308, 4.9164, 5.4020,\n",
      "        5.8876], grad_fn=<AddBackward0>)\n",
      "tensor([1.5172, 2.0028, 2.4884, 2.9740, 3.4596, 3.9452, 4.4308, 4.9164, 5.4020,\n",
      "        5.8876], grad_fn=<AddBackward0>)\n",
      "tensor([1.5173, 2.0029, 2.4885, 2.9740, 3.4596, 3.9452, 4.4308, 4.9164, 5.4020,\n",
      "        5.8875], grad_fn=<AddBackward0>)\n",
      "tensor([1.5174, 2.0029, 2.4885, 2.9741, 3.4596, 3.9452, 4.4308, 4.9164, 5.4019,\n",
      "        5.8875], grad_fn=<AddBackward0>)\n",
      "tensor([1.5174, 2.0030, 2.4885, 2.9741, 3.4597, 3.9452, 4.4308, 4.9164, 5.4019,\n",
      "        5.8875], grad_fn=<AddBackward0>)\n",
      "tensor([1.5175, 2.0030, 2.4886, 2.9741, 3.4597, 3.9452, 4.4308, 4.9163, 5.4019,\n",
      "        5.8874], grad_fn=<AddBackward0>)\n",
      "tensor([1.5175, 2.0031, 2.4886, 2.9742, 3.4597, 3.9452, 4.4308, 4.9163, 5.4019,\n",
      "        5.8874], grad_fn=<AddBackward0>)\n",
      "tensor([1.5176, 2.0031, 2.4887, 2.9742, 3.4597, 3.9453, 4.4308, 4.9163, 5.4019,\n",
      "        5.8874], grad_fn=<AddBackward0>)\n",
      "tensor([1.5177, 2.0032, 2.4887, 2.9742, 3.4597, 3.9453, 4.4308, 4.9163, 5.4018,\n",
      "        5.8874], grad_fn=<AddBackward0>)\n",
      "tensor([1.5177, 2.0032, 2.4887, 2.9743, 3.4598, 3.9453, 4.4308, 4.9163, 5.4018,\n",
      "        5.8873], grad_fn=<AddBackward0>)\n",
      "tensor([1.5178, 2.0033, 2.4888, 2.9743, 3.4598, 3.9453, 4.4308, 4.9163, 5.4018,\n",
      "        5.8873], grad_fn=<AddBackward0>)\n",
      "tensor([1.5178, 2.0033, 2.4888, 2.9743, 3.4598, 3.9453, 4.4308, 4.9163, 5.4018,\n",
      "        5.8873], grad_fn=<AddBackward0>)\n",
      "tensor([1.5179, 2.0034, 2.4889, 2.9743, 3.4598, 3.9453, 4.4308, 4.9163, 5.4017,\n",
      "        5.8872], grad_fn=<AddBackward0>)\n",
      "tensor([1.5180, 2.0034, 2.4889, 2.9744, 3.4598, 3.9453, 4.4308, 4.9163, 5.4017,\n",
      "        5.8872], grad_fn=<AddBackward0>)\n",
      "tensor([1.5180, 2.0035, 2.4889, 2.9744, 3.4599, 3.9453, 4.4308, 4.9162, 5.4017,\n",
      "        5.8872], grad_fn=<AddBackward0>)\n",
      "tensor([1.5181, 2.0035, 2.4890, 2.9744, 3.4599, 3.9453, 4.4308, 4.9162, 5.4017,\n",
      "        5.8871], grad_fn=<AddBackward0>)\n",
      "tensor([1.5181, 2.0036, 2.4890, 2.9745, 3.4599, 3.9453, 4.4308, 4.9162, 5.4017,\n",
      "        5.8871], grad_fn=<AddBackward0>)\n",
      "tensor([1.5182, 2.0036, 2.4891, 2.9745, 3.4599, 3.9454, 4.4308, 4.9162, 5.4016,\n",
      "        5.8871], grad_fn=<AddBackward0>)\n",
      "tensor([1.5183, 2.0037, 2.4891, 2.9745, 3.4599, 3.9454, 4.4308, 4.9162, 5.4016,\n",
      "        5.8870], grad_fn=<AddBackward0>)\n",
      "tensor([1.5183, 2.0037, 2.4891, 2.9746, 3.4600, 3.9454, 4.4308, 4.9162, 5.4016,\n",
      "        5.8870], grad_fn=<AddBackward0>)\n",
      "tensor([1.5184, 2.0038, 2.4892, 2.9746, 3.4600, 3.9454, 4.4308, 4.9162, 5.4016,\n",
      "        5.8870], grad_fn=<AddBackward0>)\n",
      "tensor([1.5184, 2.0038, 2.4892, 2.9746, 3.4600, 3.9454, 4.4308, 4.9162, 5.4016,\n",
      "        5.8870], grad_fn=<AddBackward0>)\n",
      "tensor([1.5185, 2.0039, 2.4893, 2.9746, 3.4600, 3.9454, 4.4308, 4.9162, 5.4015,\n",
      "        5.8869], grad_fn=<AddBackward0>)\n",
      "tensor([1.5186, 2.0039, 2.4893, 2.9747, 3.4600, 3.9454, 4.4308, 4.9162, 5.4015,\n",
      "        5.8869], grad_fn=<AddBackward0>)\n",
      "tensor([1.5186, 2.0040, 2.4893, 2.9747, 3.4601, 3.9454, 4.4308, 4.9161, 5.4015,\n",
      "        5.8869], grad_fn=<AddBackward0>)\n",
      "tensor([1.5187, 2.0040, 2.4894, 2.9747, 3.4601, 3.9454, 4.4308, 4.9161, 5.4015,\n",
      "        5.8868], grad_fn=<AddBackward0>)\n",
      "tensor([1.5187, 2.0041, 2.4894, 2.9748, 3.4601, 3.9454, 4.4308, 4.9161, 5.4015,\n",
      "        5.8868], grad_fn=<AddBackward0>)\n",
      "tensor([1.5188, 2.0041, 2.4895, 2.9748, 3.4601, 3.9454, 4.4308, 4.9161, 5.4014,\n",
      "        5.8868], grad_fn=<AddBackward0>)\n",
      "tensor([1.5188, 2.0042, 2.4895, 2.9748, 3.4601, 3.9455, 4.4308, 4.9161, 5.4014,\n",
      "        5.8868], grad_fn=<AddBackward0>)\n",
      "tensor([1.5189, 2.0042, 2.4895, 2.9748, 3.4602, 3.9455, 4.4308, 4.9161, 5.4014,\n",
      "        5.8867], grad_fn=<AddBackward0>)\n",
      "tensor([1.5190, 2.0043, 2.4896, 2.9749, 3.4602, 3.9455, 4.4308, 4.9161, 5.4014,\n",
      "        5.8867], grad_fn=<AddBackward0>)\n",
      "tensor([1.5190, 2.0043, 2.4896, 2.9749, 3.4602, 3.9455, 4.4308, 4.9161, 5.4014,\n",
      "        5.8867], grad_fn=<AddBackward0>)\n",
      "tensor([1.5191, 2.0044, 2.4896, 2.9749, 3.4602, 3.9455, 4.4308, 4.9161, 5.4014,\n",
      "        5.8866], grad_fn=<AddBackward0>)\n",
      "tensor([1.5191, 2.0044, 2.4897, 2.9750, 3.4602, 3.9455, 4.4308, 4.9161, 5.4013,\n",
      "        5.8866], grad_fn=<AddBackward0>)\n",
      "tensor([1.5192, 2.0044, 2.4897, 2.9750, 3.4602, 3.9455, 4.4308, 4.9160, 5.4013,\n",
      "        5.8866], grad_fn=<AddBackward0>)\n",
      "tensor([1.5192, 2.0045, 2.4897, 2.9750, 3.4603, 3.9455, 4.4308, 4.9160, 5.4013,\n",
      "        5.8866], grad_fn=<AddBackward0>)\n",
      "tensor([1.5193, 2.0045, 2.4898, 2.9750, 3.4603, 3.9455, 4.4308, 4.9160, 5.4013,\n",
      "        5.8865], grad_fn=<AddBackward0>)\n",
      "tensor([1.5193, 2.0046, 2.4898, 2.9751, 3.4603, 3.9455, 4.4308, 4.9160, 5.4013,\n",
      "        5.8865], grad_fn=<AddBackward0>)\n",
      "tensor([1.5194, 2.0046, 2.4899, 2.9751, 3.4603, 3.9455, 4.4308, 4.9160, 5.4012,\n",
      "        5.8865], grad_fn=<AddBackward0>)\n",
      "tensor([1.5194, 2.0047, 2.4899, 2.9751, 3.4603, 3.9456, 4.4308, 4.9160, 5.4012,\n",
      "        5.8864], grad_fn=<AddBackward0>)\n",
      "tensor([1.5195, 2.0047, 2.4899, 2.9751, 3.4604, 3.9456, 4.4308, 4.9160, 5.4012,\n",
      "        5.8864], grad_fn=<AddBackward0>)\n",
      "tensor([1.5196, 2.0048, 2.4900, 2.9752, 3.4604, 3.9456, 4.4308, 4.9160, 5.4012,\n",
      "        5.8864], grad_fn=<AddBackward0>)\n",
      "tensor([1.5196, 2.0048, 2.4900, 2.9752, 3.4604, 3.9456, 4.4308, 4.9160, 5.4012,\n",
      "        5.8864], grad_fn=<AddBackward0>)\n",
      "tensor([1.5197, 2.0048, 2.4900, 2.9752, 3.4604, 3.9456, 4.4308, 4.9160, 5.4011,\n",
      "        5.8863], grad_fn=<AddBackward0>)\n",
      "tensor([1.5197, 2.0049, 2.4901, 2.9752, 3.4604, 3.9456, 4.4308, 4.9160, 5.4011,\n",
      "        5.8863], grad_fn=<AddBackward0>)\n",
      "tensor([1.5198, 2.0049, 2.4901, 2.9753, 3.4604, 3.9456, 4.4308, 4.9159, 5.4011,\n",
      "        5.8863], grad_fn=<AddBackward0>)\n",
      "tensor([1.5198, 2.0050, 2.4901, 2.9753, 3.4605, 3.9456, 4.4308, 4.9159, 5.4011,\n",
      "        5.8863], grad_fn=<AddBackward0>)\n",
      "tensor([1.5199, 2.0050, 2.4902, 2.9753, 3.4605, 3.9456, 4.4308, 4.9159, 5.4011,\n",
      "        5.8862], grad_fn=<AddBackward0>)\n",
      "tensor([1.5199, 2.0051, 2.4902, 2.9753, 3.4605, 3.9456, 4.4308, 4.9159, 5.4011,\n",
      "        5.8862], grad_fn=<AddBackward0>)\n",
      "tensor([1.5200, 2.0051, 2.4902, 2.9754, 3.4605, 3.9456, 4.4308, 4.9159, 5.4010,\n",
      "        5.8862], grad_fn=<AddBackward0>)\n",
      "tensor([1.5200, 2.0052, 2.4903, 2.9754, 3.4605, 3.9456, 4.4308, 4.9159, 5.4010,\n",
      "        5.8861], grad_fn=<AddBackward0>)\n",
      "Epoch :  1000 cost :  0.04066196829080582\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-bd491f8bf7dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m    \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'02_Linear_Regression_Model_Data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "hypothesis = x_data * W + b\n",
    "\n",
    "cost = torch.mean((hypothesis-y_data) ** 2)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr = 0.01)\n",
    "\n",
    "np_epochs = 1000\n",
    "\n",
    "\n",
    "for i in range(np_epochs + 1 ) :\n",
    "    \n",
    "    # H(x)\n",
    "    hypothesis = x_data * W + b\n",
    "    print(hypothesis)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_data)**2)\n",
    "\n",
    "    # cost로 H 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    if i % 100 == 0 :\n",
    "        print('Epoch : ', i, 'cost : ', cost.item())\n",
    "        \n",
    "        \n",
    "plt.xlim(0, 11);    plt.ylim(0, 8)\n",
    "plt.title('02_Linear_Regression_Model_Data')\n",
    "plt.scatter(x_data, y_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-6057515ff210>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcal1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcal1\u001b[0m\u001b[1;31m# A, features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "cal1 = np.dot(np.dot(torch.LongTensor(adj.toarray()) , features.toarray()), w)# A, features\n",
    "\n",
    "# w는 입력할 때마다 변하는 값이기 때문에 np.dot이 계산을 안해줌.\n",
    "# 그래서 class를 사용하는건가 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "set_value not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-34ecfc2b3d7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSAGEConv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mSAGEConv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage_and_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py\u001b[0m in \u001b[0;36mmessage_and_aggregate\u001b[1;34m(self, adj_t, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m     def message_and_aggregate(self, adj_t: SparseTensor,\n\u001b[0;32m     77\u001b[0m                               x: OptPairTensor) -> Tensor:\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0madj_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madj_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: set_value not found"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "SAGEConv.message_and_aggregate(features, adj, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.0374e-08, 4.1657e-11, 4.1958e-08, 2.9574e-18],\n",
       "        [6.7333e+22, 1.7591e+22, 1.7184e+25, 4.3222e+27],\n",
       "        [6.1972e-04, 7.2443e+22, 1.7728e+28, 7.0367e+22],\n",
       "        [1.1970e+22, 8.4108e+20, 1.0245e-11, 2.7061e+23],\n",
       "        [5.4663e+22, 4.2882e-08, 2.5203e-09, 2.0544e+20],\n",
       "        [2.1037e+23, 2.4152e-18, 7.7052e+31, 1.9447e+31],\n",
       "        [4.7881e+22, 4.7427e+30, 9.9631e-39, 4.2246e-39]], requires_grad=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "w = Parameter(torch.Tensor(7, 4))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0350], requires_grad=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Parameter(torch.Tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0350e+00, 1.0350e+00, 1.0350e+00, 1.0350e+00],\n",
      "        [1.0350e+00, 1.7591e+22, 1.0350e+00, 1.0350e+00],\n",
      "        [1.0356e+00, 1.0350e+00, 1.0350e+00, 1.0350e+00],\n",
      "        [1.0350e+00, 8.4108e+20, 1.0350e+00, 1.0350e+00],\n",
      "        [1.0350e+00, 1.0350e+00, 1.0350e+00, 1.0350e+00],\n",
      "        [1.0350e+00, 1.0350e+00, 1.0350e+00, 1.9447e+31],\n",
      "        [1.0350e+00, 1.0350e+00, 1.0350e+00, 1.0350e+00]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-7a1c65c0f305>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "hypothesis = features * w + b\n",
    "print(hypothesis) # 4 x 4\n",
    "\n",
    "cost = torch.mean((hypothesis - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, features,  \n",
    "in_channel, out_channel # weight 생성\n",
    "\n",
    "# = A, features, weight + bias\n",
    "# labels와 비교하면서 _ loss계산 / update\n",
    "labels,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Got 6 and 2 in dimension 0 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c1bed2b4781f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-3b7cc638af9b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Step 1: Add self-loops to the adjacency matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Step 2: Linearly transform node feature matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cpu_env\\lib\\site-packages\\torch_geometric\\utils\\loop.py\u001b[0m in \u001b[0;36madd_self_loops\u001b[1;34m(edge_index, edge_weight, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0medge_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_weight\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0medge_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 6 and 2 in dimension 0 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "conv = GCNConv(16, 32)\n",
    "x = conv(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
